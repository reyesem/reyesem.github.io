<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>13 Prediction | Course Notes for Bayesian Data Analysis</title>
  <meta name="description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="13 Prediction | Course Notes for Bayesian Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="13 Prediction | Course Notes for Bayesian Data Analysis" />
  
  <meta name="twitter:description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  

<meta name="author" content="Eric M Reyes" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="interval-estimation.html"/>
<link rel="next" href="hypothesis-testing.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="essential-probability.html"><a href="essential-probability.html"><i class="fa fa-check"></i><b>1</b> Essential Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="essential-probability.html"><a href="essential-probability.html#density-functions-as-models"><i class="fa fa-check"></i><b>1.1</b> Density Functions as Models</a></li>
<li class="chapter" data-level="1.2" data-path="essential-probability.html"><a href="essential-probability.html#summarizing-distributions-parameters"><i class="fa fa-check"></i><b>1.2</b> Summarizing Distributions (Parameters)</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="essential-probability.html"><a href="essential-probability.html#kernels"><i class="fa fa-check"></i><b>1.2.1</b> Kernels</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="essential-probability.html"><a href="essential-probability.html#transformations-of-random-variables"><i class="fa fa-check"></i><b>1.3</b> Transformations of Random Variables</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="essential-probability.html"><a href="essential-probability.html#expectations-of-functions"><i class="fa fa-check"></i><b>1.3.1</b> Expectations of Functions</a></li>
<li class="chapter" data-level="1.3.2" data-path="essential-probability.html"><a href="essential-probability.html#moment-generating-functions"><i class="fa fa-check"></i><b>1.3.2</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="essential-probability.html"><a href="essential-probability.html#independent-random-variables"><i class="fa fa-check"></i><b>1.4</b> Independent Random Variables</a></li>
</ul></li>
<li class="part"><span><b>I Language and Logic of Inference</b></span></li>
<li class="chapter" data-level="2" data-path="CaseDeepwater.html"><a href="CaseDeepwater.html"><i class="fa fa-check"></i><b>2</b> Case Study: Health Effects of the Deepwater Horizon Oil Spill</a></li>
<li class="chapter" data-level="3" data-path="Basics.html"><a href="Basics.html"><i class="fa fa-check"></i><b>3</b> The Statistical Process</a>
<ul>
<li class="chapter" data-level="3.1" data-path="Basics.html"><a href="Basics.html#overview-of-drawing-inference"><i class="fa fa-check"></i><b>3.1</b> Overview of Drawing Inference</a></li>
<li class="chapter" data-level="3.2" data-path="Basics.html"><a href="Basics.html#anatomy-of-a-dataset"><i class="fa fa-check"></i><b>3.2</b> Anatomy of a Dataset</a></li>
<li class="chapter" data-level="3.3" data-path="Basics.html"><a href="Basics.html#a-note-on-codebooks"><i class="fa fa-check"></i><b>3.3</b> A Note on Codebooks</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Questions.html"><a href="Questions.html"><i class="fa fa-check"></i><b>4</b> Asking the Right Questions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="Questions.html"><a href="Questions.html#characterizing-a-variable"><i class="fa fa-check"></i><b>4.1</b> Characterizing a Variable</a></li>
<li class="chapter" data-level="4.2" data-path="Questions.html"><a href="Questions.html#framing-the-question"><i class="fa fa-check"></i><b>4.2</b> Framing the Question</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Data.html"><a href="Data.html"><i class="fa fa-check"></i><b>5</b> Gathering the Evidence (Data Collection)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="Data.html"><a href="Data.html#what-makes-a-sample-reliable"><i class="fa fa-check"></i><b>5.1</b> What Makes a Sample Reliable</a></li>
<li class="chapter" data-level="5.2" data-path="Data.html"><a href="Data.html#poor-methods-of-data-collection"><i class="fa fa-check"></i><b>5.2</b> Poor Methods of Data Collection</a></li>
<li class="chapter" data-level="5.3" data-path="Data.html"><a href="Data.html#preferred-methods-of-sampling"><i class="fa fa-check"></i><b>5.3</b> Preferred Methods of Sampling</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Summaries.html"><a href="Summaries.html"><i class="fa fa-check"></i><b>6</b> Presenting the Evidence (Summarizing Data)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="Summaries.html"><a href="Summaries.html#characteristics-of-a-distribution-summarizing-a-single-variable"><i class="fa fa-check"></i><b>6.1</b> Characteristics of a Distribution (Summarizing a Single Variable)</a></li>
<li class="chapter" data-level="6.2" data-path="Summaries.html"><a href="Summaries.html#summarizing-relationships"><i class="fa fa-check"></i><b>6.2</b> Summarizing Relationships</a></li>
</ul></li>
<li class="part"><span><b>II Fundamentals of Bayesian Inference</b></span></li>
<li class="chapter" data-level="7" data-path="bayes-rule.html"><a href="bayes-rule.html"><i class="fa fa-check"></i><b>7</b> Bayes Rule</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bayes-rule.html"><a href="bayes-rule.html#tenants-of-the-bayesian-approach-to-inference"><i class="fa fa-check"></i><b>7.1</b> Tenants of the Bayesian Approach to Inference</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="modeling-samples.html"><a href="modeling-samples.html"><i class="fa fa-check"></i><b>8</b> Modeling Samples</a>
<ul>
<li class="chapter" data-level="8.1" data-path="modeling-samples.html"><a href="modeling-samples.html#independence"><i class="fa fa-check"></i><b>8.1</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="quantifying-prior-information.html"><a href="quantifying-prior-information.html"><i class="fa fa-check"></i><b>9</b> Quantifying Prior Information</a></li>
<li class="chapter" data-level="10" data-path="posterior-distributions.html"><a href="posterior-distributions.html"><i class="fa fa-check"></i><b>10</b> Updating Prior Beliefs (Posterior Distributions)</a></li>
<li class="chapter" data-level="11" data-path="point-estimation.html"><a href="point-estimation.html"><i class="fa fa-check"></i><b>11</b> Point Estimation</a></li>
<li class="chapter" data-level="12" data-path="interval-estimation.html"><a href="interval-estimation.html"><i class="fa fa-check"></i><b>12</b> Interval Estimation</a></li>
<li class="chapter" data-level="13" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>13</b> Prediction</a>
<ul>
<li class="chapter" data-level="13.1" data-path="prediction.html"><a href="prediction.html#derivation-of-the-posterior-predictive"><i class="fa fa-check"></i><b>13.1</b> Derivation of the Posterior Predictive</a></li>
<li class="chapter" data-level="13.2" data-path="prediction.html"><a href="prediction.html#summary"><i class="fa fa-check"></i><b>13.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>14</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="14.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#point-null-hypotheses"><i class="fa fa-check"></i><b>14.1</b> Point-Null Hypotheses</a></li>
<li class="chapter" data-level="14.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#model-comparison"><i class="fa fa-check"></i><b>14.2</b> Model Comparison</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="constructing-priors.html"><a href="constructing-priors.html"><i class="fa fa-check"></i><b>15</b> Constructing Prior Distributions</a>
<ul>
<li class="chapter" data-level="15.1" data-path="constructing-priors.html"><a href="constructing-priors.html#elicitation-from-experts"><i class="fa fa-check"></i><b>15.1</b> Elicitation from Experts</a></li>
<li class="chapter" data-level="15.2" data-path="constructing-priors.html"><a href="constructing-priors.html#mixtures"><i class="fa fa-check"></i><b>15.2</b> Mixtures</a></li>
<li class="chapter" data-level="15.3" data-path="constructing-priors.html"><a href="constructing-priors.html#chains"><i class="fa fa-check"></i><b>15.3</b> Chains</a></li>
<li class="chapter" data-level="15.4" data-path="constructing-priors.html"><a href="constructing-priors.html#non-informative-priors"><i class="fa fa-check"></i><b>15.4</b> Non-Informative Priors</a></li>
</ul></li>
<li class="part"><span><b>III Numerical Approaches to Bayesian Computations</b></span></li>
<li class="chapter" data-level="16" data-path="mc-integration.html"><a href="mc-integration.html"><i class="fa fa-check"></i><b>16</b> Monte Carlo Integration</a></li>
<li class="chapter" data-level="17" data-path="mcmc.html"><a href="mcmc.html"><i class="fa fa-check"></i><b>17</b> Markov Chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="17.1" data-path="mcmc.html"><a href="mcmc.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>17.1</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="mcmc-assessment.html"><a href="mcmc-assessment.html"><i class="fa fa-check"></i><b>18</b> Assessing MCMC Samples</a></li>
<li class="chapter" data-level="19" data-path="additional-study-design.html"><a href="additional-study-design.html"><i class="fa fa-check"></i><b>19</b> Additional Study Design</a>
<ul>
<li class="chapter" data-level="19.1" data-path="additional-study-design.html"><a href="additional-study-design.html#two-types-of-studies"><i class="fa fa-check"></i><b>19.1</b> Two Types of Studies</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Course Notes for Bayesian Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="prediction" class="section level1" number="13">
<h1><span class="header-section-number">13</span> Prediction</h1>
<p>To this point, we have been focused on making a statement about the parameter given the data. However, researchers are often interested in using the data to predict what might occur in the future. Hopefully by now you realize that within the Bayesian framework, we are never interested <em>solely</em> in a point estimate. So, when we say we are interested in “prediction,” we mean we are interested in characterizing our uncertainty in a future value given the data we have observed. As this statement is not directly about the parameter, the posterior distribution does not contain the relevant information.</p>
<p>Imagine we had not yet collected data. Given only our prior beliefs, how might we characterize a future observation (one not yet observed)? We may not know what value a future observation will take, but we have some sense of the process that will generate it if we have posited a joint density to describe the data generating process.</p>
<p>In probability, we routinely used the distribution to characterize future values everytime you answered a question like “what is the probability you would observe a value between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>?” It is therefore intuitive that we might turn toward the likelihood <span class="math inline">\(f(\mathbf{y} \mid \theta)\)</span> to describe the variability in data that has not yet been observed. Of course, this highlights the difference between probability and statistics — in probability, we always knew the value of <span class="math inline">\(\theta\)</span>, but in statistics, we do not. Without a value of <span class="math inline">\(\theta\)</span> to plug into the density, we are unable to use it to compute probabilities about future observations (or, more precisely, any probabilities we computed would be a function of the unknown parameter).</p>
<p>Enter our prior beliefs. While we do not know the value of <span class="math inline">\(\theta\)</span>, we do have some prior beliefs about it, and these are captured in the prior distribution. The Bayesian framework proposes marginalizing out the parameter — essentially taking a weighted average over all possible values it could be. What results is not a single value, but a distribution of values known as the prior predictive distribution.</p>
<div class="definition">
<p><span id="def:defn-prior-predictive-distribution" class="definition"><strong>Definition 13.1  (Prior Predictive Distribution) </strong></span>The marginal distribution of the response(s) prior to observing any data:</p>
<p><span class="math display">\[m(\mathbf{y}) = \int f(\mathbf{y} \mid \theta) \pi(\theta) d\theta\]</span></p>
</div>
<p>Notice this distribution corresponds to the denominator in Bayes Theorem. While rarely used directly, this distribution provides a way of characterizing our uncertainty in future observations based solely on our beliefs about <span class="math inline">\(\theta\)</span> prior to observing any data.</p>
<p>The idea of marginalizing out the parameter is critical. Of course, we will eventually collect data, and we would like to take this knowledge into account. After observing the data, the parameter remains unknown; however, our beliefs about the parameter update (and are captured in the posterior distribution). We want to somehow marginalize out the parameter while accounting for these updated beliefs.</p>
<p>Intuitively, we may be led to swapping out the role of the posterior for the prior in Definition <a href="prediction.html#def:defn-prior-predictive-distribution">13.1</a>. This results in the posterior predictive distribution.</p>
<div class="definition">
<p><span id="def:defn-posterior-predictive-distribution" class="definition"><strong>Definition 13.2  (Posterior Predictive Distribution) </strong></span>Let <span class="math inline">\(\mathbf{Y}^*\)</span> represent a collection of <span class="math inline">\(m\)</span> <em>future</em> observations. The distribution of these future observations given the observed data <span class="math inline">\(\mathbf{Y}\)</span> (of length <span class="math inline">\(n\)</span>), is given by</p>
<p><span class="math display">\[\pi\left(\mathbf{y}^* \mid \mathbf{y}\right) = \int f\left(\mathbf{y}^* \mid \theta\right) \pi(\theta \mid \mathbf{y}) d\theta\]</span></p>
</div>
<p>While this definition is correct, its derivation requires some additional constraints on the data generating process. We present the derivation below primarily to combat any misconceptions about what is happening in the integration above.</p>
<div id="derivation-of-the-posterior-predictive" class="section level2" number="13.1">
<h2><span class="header-section-number">13.1</span> Derivation of the Posterior Predictive</h2>
<p>Let <span class="math inline">\(\mathbf{Y}^*\)</span> denote a collection of <span class="math inline">\(m\)</span> future (or new) observations not yet observed. This is distinguished from the collection of <span class="math inline">\(n\)</span> observations we have already made <span class="math inline">\(\mathbf{Y}\)</span>. We impose the following two conditions/assumptions on the data generating process:</p>
<ul>
<li>Given the value of the parameter, the likelihood of <span class="math inline">\(\mathbf{Y}^*\)</span> has the same form as the likelihood of of the observed data <span class="math inline">\(\mathbf{Y}\)</span>.</li>
<li>Given the value of the parameter, the observed data <span class="math inline">\(\mathbf{Y}\)</span> is <em>independent</em> of the new observations <span class="math inline">\(\mathbf{Y}^*\)</span>.</li>
</ul>
<p>The first condition above is intuitive. It does not make sense to use data that is generated under one process to predict data generated under a completely different process. Therefore, our future observations are always related in some way to the likelihood, as that models the data generating process of interest.</p>
<p>The second condition extends the concept of independence presented in a typical probability course. This is <em>conditional independence</em>.</p>
<div class="definition">
<p><span id="def:defn-conditional-independence" class="definition"><strong>Definition 13.3  (Conditional Independence) </strong></span>Two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are said to be independent, conditional on <span class="math inline">\(Z\)</span> if, and only if,</p>
<p><span class="math display">\[f_{(X,Y) \mid Z} (x, y \mid z) = f_{X \mid Z}(x \mid z) f_{Y \mid Z}(y \mid z)\]</span></p>
</div>
<p>Conditional independence is common in statistical theory. Two random quantities are somehow related, but given an additional piece of information become independent. Returning to the stated condition, we are saying that the only thing the new and old observations have in common is the data generating process; once we know the quantities that govern this process (the parameters), then we can gain no further knowledge from the old observations. That is, if someone told you what the parameters were, there would be no need to collect data — you would know everything possible for predicting a future observation. So, the data observed is only useful in that it informs our beliefs about the unknown parameters.</p>
<div class="rmdkeyidea">
<p>The data observed informs our beliefs about the parameters in the data generating process. It is only through what the data tells us about the parameters that the data is useful in predicting a future observation.</p>
</div>
<p>We are now prepared to derive the posterior predictive distribution. Recall that a marginal distribution can be constructed by integrating over the other elements of a joint distribution. This works even when we are carrying a conditional term through. Therefore, we know that</p>
<p><span class="math display">\[\pi\left(\mathbf{y}^* \mid \mathbf{y}\right) = \int f\left(\mathbf{y}^*, \theta \mid \mathbf{y}\right) d\theta\]</span></p>
<p>Here, we have considered the joint distribution of the new data <span class="math inline">\(\mathbf{Y}^*\)</span> and the parameter <span class="math inline">\(\theta\)</span>, then integrated over <span class="math inline">\(\theta\)</span>. This statement would have been true for any choice of random variable, but using the parameter allows us to make use of the information we have collected on it through the old data.</p>
<p>We now recall that any joint distribution can be written as the product of a conditional distribution and a marginal distribution; this is true even if we are already conditioning on another random variable:</p>
<p><span class="math display">\[f(\mathbf{y}^*, \theta \mid \mathbf{y}) = f(\mathbf{y}^* \mid \mathbf{y}, \theta) \pi(\theta \mid \mathbf{y})\]</span></p>
<p>Substituting this expression into the previous, we now have that the posterior predictive distribution must be</p>
<p><span class="math display">\[\pi\left(\mathbf{y}^* \mid \mathbf{y}\right) = \int f(\mathbf{y}^* \mid \mathbf{y}, \theta) \pi(\theta \mid \mathbf{y}) d\theta.\]</span></p>
<p>We now make use of the conditional independence. We take the first term inside the integral and apply the definition of a conditional density</p>
<p><span class="math display">\[f(\mathbf{y}^* \mid \mathbf{y}, \theta) = \frac{f(\mathbf{y}^*, \mathbf{y} \mid \theta)}{f(\mathbf{y} \mid \theta)}.\]</span></p>
<p>However, if we are willing to assume that <span class="math inline">\(\mathbf{Y}^*\)</span> is independent of <span class="math inline">\(\mathbf{Y}\)</span> given <span class="math inline">\(\theta\)</span>, then the numerator becomes the product of <span class="math inline">\(f(\mathbf{y}^* \mid \theta)\)</span> and <span class="math inline">\(f(\mathbf{y} \mid \theta)\)</span>, meaning we have that <span class="math inline">\(f(\mathbf{y}^* \mid \mathbf{y}, \theta) = f(\mathbf{y}^* \mid \theta)\)</span> under conditional independence. Substituting in this expression gives the resulting posterior predictive distribution.</p>
</div>
<div id="summary" class="section level2" number="13.2">
<h2><span class="header-section-number">13.2</span> Summary</h2>
<p>Once you have the posterior predictive distribution, you have everything there is to know about future observations given what we have observed. This distribution can be summarized just like any other. Summarizing the location (mean, median, mode) would result in point estimates for future observations. Or, we can construct interval estimates by defining a range for which the future observations would fall with some known probability.</p>
<div class="rmdwarning">
<p>Keep in mind that we have switched our focus. We are now focused on a possible data point, not a parameter. Therefore, the support of the posterior predictive may not be the same as the support of the posterior distribution.</p>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="interval-estimation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="hypothesis-testing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
