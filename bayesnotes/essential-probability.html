<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 Essential Probability | Course Notes for Bayesian Data Analysis</title>
  <meta name="description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="1 Essential Probability | Course Notes for Bayesian Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Essential Probability | Course Notes for Bayesian Data Analysis" />
  
  <meta name="twitter:description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  

<meta name="author" content="Eric M Reyes" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="CaseDeepwater.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="essential-probability.html"><a href="essential-probability.html"><i class="fa fa-check"></i><b>1</b> Essential Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="essential-probability.html"><a href="essential-probability.html#density-functions-as-models"><i class="fa fa-check"></i><b>1.1</b> Density Functions as Models</a></li>
<li class="chapter" data-level="1.2" data-path="essential-probability.html"><a href="essential-probability.html#summarizing-distributions-parameters"><i class="fa fa-check"></i><b>1.2</b> Summarizing Distributions (Parameters)</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="essential-probability.html"><a href="essential-probability.html#kernels"><i class="fa fa-check"></i><b>1.2.1</b> Kernels</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="essential-probability.html"><a href="essential-probability.html#transformations-of-random-variables"><i class="fa fa-check"></i><b>1.3</b> Transformations of Random Variables</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="essential-probability.html"><a href="essential-probability.html#expectations-of-functions"><i class="fa fa-check"></i><b>1.3.1</b> Expectations of Functions</a></li>
<li class="chapter" data-level="1.3.2" data-path="essential-probability.html"><a href="essential-probability.html#moment-generating-functions"><i class="fa fa-check"></i><b>1.3.2</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="essential-probability.html"><a href="essential-probability.html#independent-random-variables"><i class="fa fa-check"></i><b>1.4</b> Independent Random Variables</a></li>
</ul></li>
<li class="part"><span><b>I Language and Logic of Inference</b></span></li>
<li class="chapter" data-level="2" data-path="CaseDeepwater.html"><a href="CaseDeepwater.html"><i class="fa fa-check"></i><b>2</b> Case Study: Health Effects of the Deepwater Horizon Oil Spill</a></li>
<li class="chapter" data-level="3" data-path="Basics.html"><a href="Basics.html"><i class="fa fa-check"></i><b>3</b> The Statistical Process</a>
<ul>
<li class="chapter" data-level="3.1" data-path="Basics.html"><a href="Basics.html#overview-of-drawing-inference"><i class="fa fa-check"></i><b>3.1</b> Overview of Drawing Inference</a></li>
<li class="chapter" data-level="3.2" data-path="Basics.html"><a href="Basics.html#anatomy-of-a-dataset"><i class="fa fa-check"></i><b>3.2</b> Anatomy of a Dataset</a></li>
<li class="chapter" data-level="3.3" data-path="Basics.html"><a href="Basics.html#a-note-on-codebooks"><i class="fa fa-check"></i><b>3.3</b> A Note on Codebooks</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Questions.html"><a href="Questions.html"><i class="fa fa-check"></i><b>4</b> Asking the Right Questions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="Questions.html"><a href="Questions.html#characterizing-a-variable"><i class="fa fa-check"></i><b>4.1</b> Characterizing a Variable</a></li>
<li class="chapter" data-level="4.2" data-path="Questions.html"><a href="Questions.html#framing-the-question"><i class="fa fa-check"></i><b>4.2</b> Framing the Question</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Data.html"><a href="Data.html"><i class="fa fa-check"></i><b>5</b> Gathering the Evidence (Data Collection)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="Data.html"><a href="Data.html#what-makes-a-sample-reliable"><i class="fa fa-check"></i><b>5.1</b> What Makes a Sample Reliable</a></li>
<li class="chapter" data-level="5.2" data-path="Data.html"><a href="Data.html#poor-methods-of-data-collection"><i class="fa fa-check"></i><b>5.2</b> Poor Methods of Data Collection</a></li>
<li class="chapter" data-level="5.3" data-path="Data.html"><a href="Data.html#preferred-methods-of-sampling"><i class="fa fa-check"></i><b>5.3</b> Preferred Methods of Sampling</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Summaries.html"><a href="Summaries.html"><i class="fa fa-check"></i><b>6</b> Presenting the Evidence (Summarizing Data)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="Summaries.html"><a href="Summaries.html#characteristics-of-a-distribution-summarizing-a-single-variable"><i class="fa fa-check"></i><b>6.1</b> Characteristics of a Distribution (Summarizing a Single Variable)</a></li>
<li class="chapter" data-level="6.2" data-path="Summaries.html"><a href="Summaries.html#summarizing-relationships"><i class="fa fa-check"></i><b>6.2</b> Summarizing Relationships</a></li>
</ul></li>
<li class="part"><span><b>II Fundamentals of Bayesian Inference</b></span></li>
<li class="chapter" data-level="7" data-path="bayes-rule.html"><a href="bayes-rule.html"><i class="fa fa-check"></i><b>7</b> Bayes Rule</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bayes-rule.html"><a href="bayes-rule.html#tenants-of-the-bayesian-approach-to-inference"><i class="fa fa-check"></i><b>7.1</b> Tenants of the Bayesian Approach to Inference</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="modeling-samples.html"><a href="modeling-samples.html"><i class="fa fa-check"></i><b>8</b> Modeling Samples</a>
<ul>
<li class="chapter" data-level="8.1" data-path="modeling-samples.html"><a href="modeling-samples.html#independence"><i class="fa fa-check"></i><b>8.1</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="quantifying-prior-information.html"><a href="quantifying-prior-information.html"><i class="fa fa-check"></i><b>9</b> Quantifying Prior Information</a></li>
<li class="chapter" data-level="10" data-path="posterior-distributions.html"><a href="posterior-distributions.html"><i class="fa fa-check"></i><b>10</b> Updating Prior Beliefs (Posterior Distributions)</a></li>
<li class="chapter" data-level="11" data-path="point-estimation.html"><a href="point-estimation.html"><i class="fa fa-check"></i><b>11</b> Point Estimation</a></li>
<li class="chapter" data-level="12" data-path="interval-estimation.html"><a href="interval-estimation.html"><i class="fa fa-check"></i><b>12</b> Interval Estimation</a></li>
<li class="chapter" data-level="13" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>13</b> Prediction</a>
<ul>
<li class="chapter" data-level="13.1" data-path="prediction.html"><a href="prediction.html#derivation-of-the-posterior-predictive"><i class="fa fa-check"></i><b>13.1</b> Derivation of the Posterior Predictive</a></li>
<li class="chapter" data-level="13.2" data-path="prediction.html"><a href="prediction.html#summary"><i class="fa fa-check"></i><b>13.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>14</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="14.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#point-null-hypotheses"><i class="fa fa-check"></i><b>14.1</b> Point-Null Hypotheses</a></li>
<li class="chapter" data-level="14.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#model-comparison"><i class="fa fa-check"></i><b>14.2</b> Model Comparison</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="constructing-priors.html"><a href="constructing-priors.html"><i class="fa fa-check"></i><b>15</b> Constructing Prior Distributions</a>
<ul>
<li class="chapter" data-level="15.1" data-path="constructing-priors.html"><a href="constructing-priors.html#elicitation-from-experts"><i class="fa fa-check"></i><b>15.1</b> Elicitation from Experts</a></li>
<li class="chapter" data-level="15.2" data-path="constructing-priors.html"><a href="constructing-priors.html#mixtures"><i class="fa fa-check"></i><b>15.2</b> Mixtures</a></li>
<li class="chapter" data-level="15.3" data-path="constructing-priors.html"><a href="constructing-priors.html#chains"><i class="fa fa-check"></i><b>15.3</b> Chains</a></li>
<li class="chapter" data-level="15.4" data-path="constructing-priors.html"><a href="constructing-priors.html#non-informative-priors"><i class="fa fa-check"></i><b>15.4</b> Non-Informative Priors</a></li>
</ul></li>
<li class="part"><span><b>III Numerical Approaches to Bayesian Computations</b></span></li>
<li class="chapter" data-level="16" data-path="mc-integration.html"><a href="mc-integration.html"><i class="fa fa-check"></i><b>16</b> Monte Carlo Integration</a></li>
<li class="chapter" data-level="17" data-path="mcmc.html"><a href="mcmc.html"><i class="fa fa-check"></i><b>17</b> Markov Chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="17.1" data-path="mcmc.html"><a href="mcmc.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>17.1</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="mcmc-assessment.html"><a href="mcmc-assessment.html"><i class="fa fa-check"></i><b>18</b> Assessing MCMC Samples</a></li>
<li class="chapter" data-level="19" data-path="additional-study-design.html"><a href="additional-study-design.html"><i class="fa fa-check"></i><b>19</b> Additional Study Design</a>
<ul>
<li class="chapter" data-level="19.1" data-path="additional-study-design.html"><a href="additional-study-design.html#two-types-of-studies"><i class="fa fa-check"></i><b>19.1</b> Two Types of Studies</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Course Notes for Bayesian Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="essential-probability" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Essential Probability</h1>
<p>Statistics uses data to make inference on a population. In turn, statistical theory is built on probability — the discipline of mathematics which studies and models random processes. This is particularly true within the Bayesian paradigm. An introductory course in probability provides a foundation in model random processes. Throughout the text, we will extend these ideas to modeling the process which generates observed data and the uncertainty in parameters that govern these processes. This chapter, however, provides a brief review of the most relevant aspects of probability theory necessary which permeate the remainder of the text.</p>
<div id="density-functions-as-models" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Density Functions as Models</h2>
<p>Any process for which the outcome cannot be predicted with certainty is a random process. Typically, probability is taught from a mathematical perspective, with a goal of constructing a coherent and complete framework for characterizing such processes. Here, our goal is to introduce key probability concepts by relating them to their data-centric analogues. That is, we want to think of probability in light of how we will use it in statistical analysis.</p>
<p>Each time we collect data, we can think of each observation as the result of a random process. These observations are recorded as variables in our dataset. Technically, a <em>random variable</em> is a function which maps outcomes from a random process to the real line; however, it suffices to think of a random variable as representing a measurement that results from a random process. Just as we have both <em>quantitative</em> and <em>qualitative</em> variables, there are <em>continuous</em> and <em>discrete</em> random variables.</p>
<div class="definition">
<p><span id="def:defn-random-variable" class="definition"><strong>Definition 1.1  (Random Variable) </strong></span>Represents a measurement that will be collected and for which the value cannot be predicted with certainty. Generally represented with a capital letter. Continuous random variables represent quantitative measurements while discrete random variables represent qualitative measurements.</p>
</div>
<p>While the exact value the random variable will take is unknown (because it results from a random process), we often know the range of the random variable — the possible values it can assume. In statistics, we refer to this as the <em>support</em> of the random variable.</p>
<div class="definition">
<p><span id="def:defn-support" class="definition"><strong>Definition 1.2  (Support) </strong></span>The set of all possible values a random variable can assume. We typically denote the support of the random variable <span class="math inline">\(X\)</span> by <span class="math inline">\(\mathcal{S}_X\)</span>.</p>
</div>
<p>As a random variable is the result of a random process, each time we observe the process, the value of the random variable is subject to change. Further, it is often the case that not all values in the support are equally likely. A <em>distribution</em> is the way in which a random variable moves across its support. Probability is really about modeling and characterizing distributions. The most common way to represent a probability model is through its density function.</p>
<div class="definition">
<p><span id="def:defn-density" class="definition"><strong>Definition 1.3  (Density Function) </strong></span>A density function <span class="math inline">\(f\)</span> relates the potential values of a random variable <span class="math inline">\(X\)</span> with the probability those values occur. For a <em>continuous</em> random variable, the probability the random variable <span class="math inline">\(X\)</span> falls within an interval <span class="math inline">\((a, b)\)</span> is given by</p>
<p><span class="math display">\[Pr(a \leq X \leq b) = \int_{a}^{b} f(x) dx.\]</span></p>
<p>For a <em>discrete</em> random variable, the probability the random variable <span class="math inline">\(X\)</span> is equal to the value <span class="math inline">\(u\)</span> is given by</p>
<p><span class="math display">\[Pr(X = u) = f(u).\]</span></p>
</div>

<div class="rmdtip">
In a probability course, there is often a distinction made between probability density functions (continuous random variables) and probability mass functions (discrete random variables). We do not make this distinction and instead rely on the context to determine whether we are dealing with a continuous or discrete random variable.
</div>
<p>From the definition, it is clear that the density function for a discrete random variable provides a very nice interpretation — it is a probability. However, for a continuous random variable, the density function is a smooth function over some region, and the actual value of the function is not interpretable; instead, we get at a probability by considering the area under the curve.</p>
<p>Especially for visualization, the density function is the most common way of characterizing a probability model. However, computing the probability using the density is problematic due to the integration required. Many software programs address this by working with the cumulative distribution function (CDF).</p>
<div class="definition">
<p><span id="def:defn-cdf" class="definition"><strong>Definition 1.4  (Cumulative Distribution Function (CDF)) </strong></span>Let <span class="math inline">\(X\)</span> be a random variable; the cumulative distribution function (CDF) is defined as</p>
<p><span class="math display">\[F(u) = Pr(X \leq u).\]</span></p>
<p>For a continuous random variable, we have that</p>
<p><span class="math display">\[F(u) = \int_{-\infty}^{u} f(x) dx\]</span></p>
<p>implying that the density function is the derivative of the CDF. For a discrete random variable</p>
<p><span class="math display">\[F(u) = \sum_{x \leq u} f(x).\]</span></p>
</div>
<p>Working with the CDF improves computation because it avoids the need to integrate each time; instead, the integral is computed once (and stored internally in the computer), and we use the result to compute probabilities directly.</p>

<div class="rmdkeyidea">
Density functions are the mathematical models for distributions; they link values of the variable with the likelihood of occurence. However, for computational reasons, we often work with the cumulative distribution function which provides the probability of being less than or equal to a value.
</div>
</div>
<div id="summarizing-distributions-parameters" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Summarizing Distributions (Parameters)</h2>
<p>Most scientific questions are focused on the location or spread of a distribution. For example, we are interested in estimating the average yield of a crop, or the variance in the amount of sleep among college students. In probability, these are typically viewed simply as summaries of the entire distribution.</p>
<p>In particular, the mean of a random variable (denoted by <span class="math inline">\(E(X)\)</span>) and the variance of a random variable (denoted by <span class="math inline">\(Var(X)\)</span>) are measures of the location and spread, respectively, of the distribution represented by its corresponding density function. When the density function is a model for the population, these represent the parameters of the population — the same parameters we will later estimate and make inference on using data analysis.</p>
<div class="definition">
<p><span id="def:defn-rv-mean-variance" class="definition"><strong>Definition 1.5  (Mean and Variance of a Random Variable) </strong></span>Suppose <span class="math inline">\(X\)</span> is a random variable with density function <span class="math inline">\(f\)</span>. If <span class="math inline">\(X\)</span> is a continuous random variable, then the mean and variance are given by</p>
<p><span class="math display">\[
\begin{aligned}
  E(X) &amp;= \int x f(x) dx \\
  Var(X) &amp;= \int \left(x - E(X)\right)^2 f(x) dx
\end{aligned}
\]</span></p>
<p>where the integral is taken over the support of <span class="math inline">\(X\)</span>. If <span class="math inline">\(X\)</span> is a discrete random variable, then the mean and variance are given by</p>
<p><span class="math display">\[
\begin{aligned}
  E(X) &amp;= \sum x f(x) \\
  Var(X) &amp;= \sum \left(x - E(X)\right)^2 f(x)
\end{aligned}
\]</span></p>
<p>where the sum is taken over the support of <span class="math inline">\(X\)</span>.</p>
</div>
<p>In probability, often the distribution we are working with is completely determined. In reality, however, only the functional form of the distribution may be known, and therefore the distribution is known only up to some defining parameters that govern the shape. For example, a researcher might posit that within the population, the time until a medical device fails could be modeled using the density</p>
<p><span class="math display">\[f(x) = \frac{1}{\mu} e^{-\frac{x}{\mu}} \qquad x &gt; 0.\]</span></p>
<p>Here, the researcher has really posited a form of the model, but not the exact model. The value <span class="math inline">\(\mu\)</span> is the average (which could be confirmed using the formulas in the above definition). In such cases, making inference on the parameters allows us to really characterize the entire distribution of the population.</p>

<div class="rmdkeyidea">
When a probability model is specified for a population, it is generally specified up to some unknown parameter(s). Making inference on the unknown parameter(s) therefore characterizes the entire distribution.
</div>
<div id="kernels" class="section level3" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Kernels</h3>
<p>All density functions share some common properties. For example, a density function must be non-negative everywhere. We also know that if evaluated across the entire support, the density should sum to 1. That is, if <span class="math inline">\(X\)</span> is a discrete random variable, then</p>
<p><span class="math display">\[\sum_{\mathcal{S}_X} f(x) = 1;\]</span></p>
<p>and, if <span class="math inline">\(X\)</span> is a continuous random variable, then</p>
<p><span class="math display">\[\int_{\mathcal{S}_X} f(x) dx = 1.\]</span></p>
<p>Any density function <span class="math inline">\(f(x)\)</span> can be rewritten as <span class="math inline">\(f(x) = K h(x)\)</span> where <span class="math inline">\(K &gt; 0\)</span> is a constant. We call <span class="math inline">\(h\)</span> the <em>kernel</em> of the density function.</p>
<div class="definition">
<p><span id="def:defn-kernel" class="definition"><strong>Definition 1.6  (Kernel) </strong></span>A valid density function can be obtained by scaling the kernel such that integrating (or summing) over the support results in the value 1. Often, the kernel is useful for identifying the distributional family to which a random variable belongs.</p>
</div>
</div>
</div>
<div id="transformations-of-random-variables" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Transformations of Random Variables</h2>
<p>Occasionally, we have some intuition about how to model the distribution of a random variable <span class="math inline">\(X\)</span>, but we are actually interested in the distribution of some function <span class="math inline">\(g(X)\)</span> of that random variable. This is known as a variable transformation. For example, we may posit that the grade on an exam (when represented as a fraction) follow a Beta distribution; however, we are interested in characterizing the grade when it has been rescaled on the interval from 0 to 100.</p>
<p>The derivation of the density function for the transformed random variable, when <span class="math inline">\(X\)</span> is continuous, is often useful. While there are some well-known formulas for obtaining this density function, we prefer the following procedure:</p>
<ol style="list-style-type: decimal">
<li>Define <span class="math inline">\(Y = g(X)\)</span>.<br />
</li>
<li>Recognize that <span class="math inline">\(F_Y(y) = Pr(Y \leq y) = Pr(g(X) \leq y)\)</span>.<br />
</li>
<li>Rewrite the above probability statement in terms of <span class="math inline">\(X\)</span> and therefore the CDF <span class="math inline">\(F_X\)</span> of <span class="math inline">\(X\)</span>. For monotone functions, this is simple by applying the inverse function <span class="math inline">\(g^{-1}(y)\)</span> to both sides of the inequality within the probability statement; but, it may be more cumbersome in general.</li>
<li>Compute the derivative of the result, with respect to <span class="math inline">\(y\)</span> to obtain the density.</li>
</ol>
<p>Once you have the density function for <span class="math inline">\(Y\)</span>, which is a characterization of its distribution, you are able to summarize all aspects of its distribution as we would with any other random variable.</p>
<div id="expectations-of-functions" class="section level3" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> Expectations of Functions</h3>
<p>If all we are interested in is some summary of <span class="math inline">\(Y = g(X)\)</span>, determining the entire distribution is overkill. For example, suppose we are interested in <span class="math inline">\(E(Y) = E\left[g(X)\right]\)</span>; we could determine this by first performing the variable transformation to obtain the density of <span class="math inline">\(Y\)</span> and then taking the expectation as defined in the previous section. However, there is a nice result that allows us to skip the intermediate step (it unfortunately is sometimes referred to as the “Law of the Unconscious Statistician,” which we of course object to).</p>
<div class="definition">
<p><span id="def:defn-expectation-transformation" class="definition"><strong>Definition 1.7  (Expectation of a Function of a Random Variable) </strong></span>Suppose <span class="math inline">\(X\)</span> is a random variable with density function <span class="math inline">\(f\)</span>, and let <span class="math inline">\(Y = g(X)\)</span>. If <span class="math inline">\(X\)</span> is a continuous random variable, then we have that</p>
<p><span class="math display">\[E(Y) = E\left[g(X)\right] = \int g(x) f(x) dx\]</span></p>
<p>where the integral is taken over the support of <span class="math inline">\(X\)</span>. If <span class="math inline">\(X\)</span> is a discrete random variable, then</p>
<p><span class="math display">\[E(Y) = E\left[g(X)\right] = \sum g(x) f(x)\]</span></p>
<p>where the sum is taken over the support of <span class="math inline">\(X\)</span>.</p>
</div>
</div>
<div id="moment-generating-functions" class="section level3" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> Moment Generating Functions</h3>
<p>The mean and variance are known as the first two central moments of a distribution. For well-behaved distributions, any moment of a distribution can be computed from its moment generating function.</p>
<div class="definition">
<p><span id="def:defn-mgf" class="definition"><strong>Definition 1.8  (Moment Generating Function) </strong></span>Suppose <span class="math inline">\(X\)</span> is a random variable with density function <span class="math inline">\(f\)</span>. The moment generating function (MGF) of <span class="math inline">\(X\)</span>, evaluated at <span class="math inline">\(t\)</span>, is given by</p>
<p><span class="math display">\[M_X(t) = E\left[e^{tX}\]</span>]</p>
<p>The <span class="math inline">\(k\)</span>-th moment of <span class="math inline">\(X\)</span> is then given by evaluating <span class="math inline">\(M^{(k)}_X(t)\)</span>, the <span class="math inline">\(k\)</span>-th derivative of the MGF, at <span class="math inline">\(t = 0\)</span>.</p>
</div>
</div>
</div>
<div id="independent-random-variables" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Independent Random Variables</h2>
<p>As we will see as we progress, assuming random variables are independent greatly simplifies a model for the data generating process. In particular, when two random variables are independent, probability statements become easier to evaluate because “and” statements reduce to multiplication.</p>
<div class="lemma">
<p><span id="lem:thm-independence" class="lemma"><strong>Lemma 1.1  (Independence) </strong></span>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables, then</p>
<p><span class="math display">\[Pr(a \leq X \leq b, c \leq Y \leq d) = Pr(a \leq X \leq b) Pr(c \leq Y \leq d).\]</span></p>
<p>Note: stating <span class="math inline">\(Pr(A, B)\)</span> is equivalent to <span class="math inline">\(Pr(A \cap B)\)</span>.</p>
</div>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="CaseDeepwater.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
