<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>16 Monte Carlo Integration | Course Notes for Bayesian Data Analysis</title>
  <meta name="description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="16 Monte Carlo Integration | Course Notes for Bayesian Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="16 Monte Carlo Integration | Course Notes for Bayesian Data Analysis" />
  
  <meta name="twitter:description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  

<meta name="author" content="Eric M Reyes" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="constructing-priors.html"/>
<link rel="next" href="mcmc.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="essential-probability.html"><a href="essential-probability.html"><i class="fa fa-check"></i><b>1</b> Essential Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="essential-probability.html"><a href="essential-probability.html#density-functions-as-models"><i class="fa fa-check"></i><b>1.1</b> Density Functions as Models</a></li>
<li class="chapter" data-level="1.2" data-path="essential-probability.html"><a href="essential-probability.html#summarizing-distributions-parameters"><i class="fa fa-check"></i><b>1.2</b> Summarizing Distributions (Parameters)</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="essential-probability.html"><a href="essential-probability.html#kernels"><i class="fa fa-check"></i><b>1.2.1</b> Kernels</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="essential-probability.html"><a href="essential-probability.html#transformations-of-random-variables"><i class="fa fa-check"></i><b>1.3</b> Transformations of Random Variables</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="essential-probability.html"><a href="essential-probability.html#expectations-of-functions"><i class="fa fa-check"></i><b>1.3.1</b> Expectations of Functions</a></li>
<li class="chapter" data-level="1.3.2" data-path="essential-probability.html"><a href="essential-probability.html#moment-generating-functions"><i class="fa fa-check"></i><b>1.3.2</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="essential-probability.html"><a href="essential-probability.html#independent-random-variables"><i class="fa fa-check"></i><b>1.4</b> Independent Random Variables</a></li>
</ul></li>
<li class="part"><span><b>I Language and Logic of Inference</b></span></li>
<li class="chapter" data-level="2" data-path="CaseDeepwater.html"><a href="CaseDeepwater.html"><i class="fa fa-check"></i><b>2</b> Case Study: Health Effects of the Deepwater Horizon Oil Spill</a></li>
<li class="chapter" data-level="3" data-path="Basics.html"><a href="Basics.html"><i class="fa fa-check"></i><b>3</b> The Statistical Process</a>
<ul>
<li class="chapter" data-level="3.1" data-path="Basics.html"><a href="Basics.html#overview-of-drawing-inference"><i class="fa fa-check"></i><b>3.1</b> Overview of Drawing Inference</a></li>
<li class="chapter" data-level="3.2" data-path="Basics.html"><a href="Basics.html#anatomy-of-a-dataset"><i class="fa fa-check"></i><b>3.2</b> Anatomy of a Dataset</a></li>
<li class="chapter" data-level="3.3" data-path="Basics.html"><a href="Basics.html#a-note-on-codebooks"><i class="fa fa-check"></i><b>3.3</b> A Note on Codebooks</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Questions.html"><a href="Questions.html"><i class="fa fa-check"></i><b>4</b> Asking the Right Questions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="Questions.html"><a href="Questions.html#characterizing-a-variable"><i class="fa fa-check"></i><b>4.1</b> Characterizing a Variable</a></li>
<li class="chapter" data-level="4.2" data-path="Questions.html"><a href="Questions.html#framing-the-question"><i class="fa fa-check"></i><b>4.2</b> Framing the Question</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Data.html"><a href="Data.html"><i class="fa fa-check"></i><b>5</b> Gathering the Evidence (Data Collection)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="Data.html"><a href="Data.html#what-makes-a-sample-reliable"><i class="fa fa-check"></i><b>5.1</b> What Makes a Sample Reliable</a></li>
<li class="chapter" data-level="5.2" data-path="Data.html"><a href="Data.html#poor-methods-of-data-collection"><i class="fa fa-check"></i><b>5.2</b> Poor Methods of Data Collection</a></li>
<li class="chapter" data-level="5.3" data-path="Data.html"><a href="Data.html#preferred-methods-of-sampling"><i class="fa fa-check"></i><b>5.3</b> Preferred Methods of Sampling</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Summaries.html"><a href="Summaries.html"><i class="fa fa-check"></i><b>6</b> Presenting the Evidence (Summarizing Data)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="Summaries.html"><a href="Summaries.html#characteristics-of-a-distribution-summarizing-a-single-variable"><i class="fa fa-check"></i><b>6.1</b> Characteristics of a Distribution (Summarizing a Single Variable)</a></li>
<li class="chapter" data-level="6.2" data-path="Summaries.html"><a href="Summaries.html#summarizing-relationships"><i class="fa fa-check"></i><b>6.2</b> Summarizing Relationships</a></li>
</ul></li>
<li class="part"><span><b>II Fundamentals of Bayesian Inference</b></span></li>
<li class="chapter" data-level="7" data-path="bayes-rule.html"><a href="bayes-rule.html"><i class="fa fa-check"></i><b>7</b> Bayes Rule</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bayes-rule.html"><a href="bayes-rule.html#tenants-of-the-bayesian-approach-to-inference"><i class="fa fa-check"></i><b>7.1</b> Tenants of the Bayesian Approach to Inference</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="modeling-samples.html"><a href="modeling-samples.html"><i class="fa fa-check"></i><b>8</b> Modeling Samples</a>
<ul>
<li class="chapter" data-level="8.1" data-path="modeling-samples.html"><a href="modeling-samples.html#independence"><i class="fa fa-check"></i><b>8.1</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="quantifying-prior-information.html"><a href="quantifying-prior-information.html"><i class="fa fa-check"></i><b>9</b> Quantifying Prior Information</a></li>
<li class="chapter" data-level="10" data-path="posterior-distributions.html"><a href="posterior-distributions.html"><i class="fa fa-check"></i><b>10</b> Updating Prior Beliefs (Posterior Distributions)</a></li>
<li class="chapter" data-level="11" data-path="point-estimation.html"><a href="point-estimation.html"><i class="fa fa-check"></i><b>11</b> Point Estimation</a></li>
<li class="chapter" data-level="12" data-path="interval-estimation.html"><a href="interval-estimation.html"><i class="fa fa-check"></i><b>12</b> Interval Estimation</a></li>
<li class="chapter" data-level="13" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>13</b> Prediction</a>
<ul>
<li class="chapter" data-level="13.1" data-path="prediction.html"><a href="prediction.html#derivation-of-the-posterior-predictive"><i class="fa fa-check"></i><b>13.1</b> Derivation of the Posterior Predictive</a></li>
<li class="chapter" data-level="13.2" data-path="prediction.html"><a href="prediction.html#summary"><i class="fa fa-check"></i><b>13.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>14</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="14.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#point-null-hypotheses"><i class="fa fa-check"></i><b>14.1</b> Point-Null Hypotheses</a></li>
<li class="chapter" data-level="14.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#model-comparison"><i class="fa fa-check"></i><b>14.2</b> Model Comparison</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="constructing-priors.html"><a href="constructing-priors.html"><i class="fa fa-check"></i><b>15</b> Constructing Prior Distributions</a>
<ul>
<li class="chapter" data-level="15.1" data-path="constructing-priors.html"><a href="constructing-priors.html#elicitation-from-experts"><i class="fa fa-check"></i><b>15.1</b> Elicitation from Experts</a></li>
<li class="chapter" data-level="15.2" data-path="constructing-priors.html"><a href="constructing-priors.html#mixtures"><i class="fa fa-check"></i><b>15.2</b> Mixtures</a></li>
<li class="chapter" data-level="15.3" data-path="constructing-priors.html"><a href="constructing-priors.html#chains"><i class="fa fa-check"></i><b>15.3</b> Chains</a></li>
<li class="chapter" data-level="15.4" data-path="constructing-priors.html"><a href="constructing-priors.html#non-informative-priors"><i class="fa fa-check"></i><b>15.4</b> Non-Informative Priors</a></li>
</ul></li>
<li class="part"><span><b>III Numerical Approaches to Bayesian Computations</b></span></li>
<li class="chapter" data-level="16" data-path="mc-integration.html"><a href="mc-integration.html"><i class="fa fa-check"></i><b>16</b> Monte Carlo Integration</a></li>
<li class="chapter" data-level="17" data-path="mcmc.html"><a href="mcmc.html"><i class="fa fa-check"></i><b>17</b> Markov Chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="17.1" data-path="mcmc.html"><a href="mcmc.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>17.1</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="mcmc-assessment.html"><a href="mcmc-assessment.html"><i class="fa fa-check"></i><b>18</b> Assessing MCMC Samples</a></li>
<li class="chapter" data-level="19" data-path="study-design.html"><a href="study-design.html"><i class="fa fa-check"></i><b>19</b> Elements of Good Study Design</a></li>
<li class="chapter" data-level="20" data-path="independent-groups.html"><a href="independent-groups.html"><i class="fa fa-check"></i><b>20</b> Models for Comparing Independent Groups</a>
<ul>
<li class="chapter" data-level="20.1" data-path="independent-groups.html"><a href="independent-groups.html#bridge-sampling"><i class="fa fa-check"></i><b>20.1</b> Bridge Sampling</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="dependent-groups.html"><a href="dependent-groups.html"><i class="fa fa-check"></i><b>21</b> Comparing Related Groups</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Course Notes for Bayesian Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mc-integration" class="section level1" number="16">
<h1><span class="header-section-number">16</span> Monte Carlo Integration</h1>
<p>Integration plays a critical role in deriving the posterior distribution, which is central to Bayesian inference. This integration can often become analytically intractable. In this unit, we discuss numerical approaches to avoiding this integration by obtaining a sample from the posterior. In this chapter, we discuss a popular method of numerical integration in the statistical community that serves as the foundation for modern Bayesian computational approaches.</p>
<div class="example">
<p><span id="exm:ex-integration" class="example"><strong>Example 16.1  (Pharmacokinetics) </strong></span>When modeling the rate at which a drug is broken down by the body (known as pharmacokinetics), it is often of interest to know the logarithm of the ED50 value (the time at which 50% of the drug has been metabolized by the body). Suppose it is known that for a particular drug, the ED50 value <span class="math inline">\(T\)</span> for the patient population can be modeled using the following density:</p>
<p><span class="math display">\[f(t) = \frac{b^a}{\Gamma(a)} t^{a-1} e^{-bt} \qquad t &gt; 0\]</span></p>
<p>where <span class="math inline">\(a = 4\)</span> and <span class="math inline">\(b = 2\)</span>. We are interested in the average logarithm of the ED50 value, which is given by</p>
<p><span class="math display">\[\int \log(t) f(t) dt.\]</span></p>
</div>
<p>While this example is contrived, integrals like this arise in practice regularly. The integral of interest does not yield an analytical solution; a numerical procedure must be used. There are several numerical methods for integration which are developed and studied in mathematics. However, consider the following suggested procedure:</p>
<ul>
<li>Let <span class="math inline">\(T_1, T_2, \dots, T_n\)</span> be a random sample such that <span class="math inline">\(T_i \sim f(t)\)</span> for all <span class="math inline">\(i\)</span>.</li>
<li>Define <span class="math inline">\(Y_i = \log\left(T_i\right)\)</span> for all <span class="math inline">\(i\)</span>.</li>
<li>Compute <span class="math inline">\(\bar{Y}\)</span>.</li>
</ul>
<p>The value of <span class="math inline">\(\bar{Y}\)</span> will estimate the integral! As an initial justification, observe that on average, the estimate is correct. Observe that</p>
<p><span class="math display">\[
\begin{aligned}
  E\left(\bar{Y}\right) &amp;= E\left(n^{-1} \sum_{i=1}^{n} Y_i\right) \\
    &amp;= E\left(n^{-1} \sum_{i=1}^{n} \log\left(T_i\right)\right) \\
    &amp;= n^{-1} \sum_{i=1}^{n} E\left(\log\left(T_i\right)\right) \\
\end{aligned}
\]</span></p>
<p>where the last line is a result of the expectation being a linear operator. Since each <span class="math inline">\(T_i\)</span> is identically distributed (they are a random sample from the same population), the expectation is the same for each <span class="math inline">\(i\)</span>; specifically, we have (see Definition <a href="essential-probability.html#def:defn-expectation-transformation">1.7</a>)</p>
<p><span class="math display">\[E\left(\log\left(T_i\right)\right) = \int \log(t) f(t) dt.\]</span></p>
<p>Since this is not indexed by <span class="math inline">\(i\)</span>, we have that</p>
<p><span class="math display">\[E\left(\bar{Y}\right) = \int \log(t) f(t) dt\]</span></p>
<p>the desired integral. Of course, this just states that the distribution of <span class="math inline">\(\bar{Y}\)</span> (across many samples of size <span class="math inline">\(n\)</span>) will center on the true value of the integral. We can actually say a lot more because of the Law of Large Numbers.</p>
<div class="theorem">
<p><span id="thm:lln" class="theorem"><strong>Theorem 16.1  (Law of Large Numbers) </strong></span>Let <span class="math inline">\(X_1, X_2,\dotsc, X_n\)</span> be independent and identically distributed random variables with density <span class="math inline">\(f(x)\)</span>. Consider a real valued function <span class="math inline">\(g\)</span>. Then, for any <span class="math inline">\(\epsilon &gt; 0\)</span> we have that</p>
<p><span class="math display">\[Pr\left(\lvert\frac{1}{n}\sum_{i=1}^n g\left(X_i\right) - E\left[g(X)\right]\rvert &gt; \epsilon\right) \rightarrow 0\]</span></p>
<p>as <span class="math inline">\(n \rightarrow \infty\)</span>, assuming <span class="math inline">\(E\left[g(X)\right]\)</span> exists.</p>
</div>
<p>The Law of Large Numbers essentially says that the sample mean can be made arbitrarily close to the expectation it approximates given a large enough sample size. That is, as the sample size increases, the sample mean is really close to the true mean. That is,</p>
<p><span class="math display">\[\frac{1}{n} \sum_{i=1}^{n} g\left(X_i\right) \approx \int g(x) f(x) dx.\]</span></p>
<div class="rmdkeyidea">
<p>The Law of Large Numbers allows us to approximate integrals, in the form of expectations of random variables, using a corresponding sample mean.</p>
</div>
<p>The following pseudo-code illustrates the use of the Law of Large Numbers in order to compute the integral from the Pharmacokinetics example (<a href="mc-integration.html#exm:ex-integration">16.1</a>):</p>
<pre><code>let m = 10000;

for i from 1 to m do;
  x[i] = random_gamma(shape = 4, rate = 2);
  y[i] = log(x[i]);
end;
  
ybar = mean(y[1:m]);</code></pre>
<p>Since we have a large sample size, we know that the sample mean computed in the last step will be a good approximation to the integral of interest.</p>
<p>Upon first inspection, it may seem that the Law of Large Numbers is limited to estimating means. However, nearly every quantity of interest we may want to compute can be written in terms of an integral and therefore approximated using this technique. This includes probabilities, means, quantiles (and therefore credible intervals), variances, and even the evidence in favor of a model. That is, the Law of Large Numbers provides a technique for performing integration numerically. Because of its dependence on random processes, it is known as Monte Carlo Integration. This will allow us to compute summaries for the posterior distribution.</p>
<div class="rmdkeyidea">
<p>We can construct Bayesian estimators using only a random sample from the posterior distribution.</p>
</div>
<p>When using numerical simulation to estimate a quantity, we will typically be required to sample from a joint density. This is difficult in general; however, if we can create a set of chained expressions, this is much simpler. Suppose <span class="math inline">\((X, Y)\)</span> is a vector of two random variables with joint density <span class="math inline">\(f(x, y)\)</span>; we make use of the fact that</p>
<p><span class="math display">\[f(x, y) = f(x \mid y) f(y);\]</span></p>
<p>that is, the joint distribution is the product of a conditional distribution and a marginal distribution. This process works for generating random variables as well.</p>
<div class="block2">
<p><strong>Simulating from Joint Distributions</strong>
Let the random vector <span class="math inline">\((X, Y)\)</span> be distributed according to the joint density <span class="math inline">\(f(x,y)\)</span>. The following procedure can be used to simulate observations <span class="math inline">\(\left(X_1, Y_1\right), \left(X_2, Y_2\right), \dotsc, \left(X_m, Y_m\right)\)</span> from the joint density:</p>
<ol style="list-style-type: decimal">
<li>Generate <span class="math inline">\(m\)</span> variates <span class="math inline">\(Y_1, Y_2, \dotsc, Y_m\)</span> from <span class="math inline">\(f(y)\)</span>, the marginal distribution of <span class="math inline">\(Y\)</span>.</li>
<li>For each <span class="math inline">\(Y_i\)</span>, generate a single <span class="math inline">\(X_i\)</span> from <span class="math inline">\(f(x \mid y)\)</span>, the conditional distribution of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span>.</li>
</ol>
<p>The resulting pairs will have the desired joint distribution. Further, the column of <span class="math inline">\(X\)</span>’s will behave according to the density <span class="math inline">\(f(x)\)</span>, the marginal distribution of <span class="math inline">\(X\)</span>.</p>
</div>
<p>The Law of Large Numbers guarantees that we can obtain approximations of Bayesian estimators. Of course, given that these approximations are based on random processes, it is reasonable to ask how good the approximation is. Similarly, it is natural to ask how many random samples are needed for a decent approximation. This is addressed via a version of the Central Limit Theorem.</p>
<div class="theorem">
<p><span id="thm:clt" class="theorem"><strong>Theorem 16.2  (Central Limit Theorem) </strong></span>Let <span class="math inline">\(X_1, X_2, \dotsc, X_n\)</span> be independent and identically distributed random variables. Consider a real-valued function <span class="math inline">\(g\)</span> such that <span class="math inline">\(E\left[g(X)\right]\)</span> and <span class="math inline">\(Var\left[g(X)\right]\)</span> exist. Then, we have that the quantity</p>
<p><span class="math display">\[\frac{n^{-1} \sum_{i=1}^{n} g\left(X_i\right) - E\left[g(X)\right]}{\sqrt{\widehat{Var}\left[g(X)\right]/n}}\]</span></p>
<p>behaves like a standard normal random variable as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
</div>
<p>The Central Limit Theorem provides a way of quantifying the error in the Monte Carlo approximation.</p>
<div class="definition">
<p><span id="def:def-mc-error" class="definition"><strong>Definition 16.1  (Monte Carlo Error) </strong></span>Also called the standard error for an approximation of the form <span class="math inline">\(m^{-1} \sum\limits_{k=1}^{m} g\left(X_k\right)\)</span>, the MC error is given by</p>
<p><span class="math display">\[\sqrt{\frac{1}{m(m-1)} \sum_{k=1}^{m} \left[g\left(X_k\right) - \frac{1}{m} \sum_{j=1}^{m} g\left(X_j\right)\right]^2}\]</span></p>
<p>which is the sample standard deviation of the generated variates divided by the square root of the number of replications.</p>
</div>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="constructing-priors.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mcmc.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
