<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>21 Comparing Related Groups | Course Notes for Bayesian Data Analysis</title>
  <meta name="description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="21 Comparing Related Groups | Course Notes for Bayesian Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="21 Comparing Related Groups | Course Notes for Bayesian Data Analysis" />
  
  <meta name="twitter:description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  

<meta name="author" content="Eric M Reyes" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="independent-groups.html"/>
<link rel="next" href="references.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="essential-probability.html"><a href="essential-probability.html"><i class="fa fa-check"></i><b>1</b> Essential Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="essential-probability.html"><a href="essential-probability.html#density-functions-as-models"><i class="fa fa-check"></i><b>1.1</b> Density Functions as Models</a></li>
<li class="chapter" data-level="1.2" data-path="essential-probability.html"><a href="essential-probability.html#summarizing-distributions-parameters"><i class="fa fa-check"></i><b>1.2</b> Summarizing Distributions (Parameters)</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="essential-probability.html"><a href="essential-probability.html#kernels"><i class="fa fa-check"></i><b>1.2.1</b> Kernels</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="essential-probability.html"><a href="essential-probability.html#transformations-of-random-variables"><i class="fa fa-check"></i><b>1.3</b> Transformations of Random Variables</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="essential-probability.html"><a href="essential-probability.html#expectations-of-functions"><i class="fa fa-check"></i><b>1.3.1</b> Expectations of Functions</a></li>
<li class="chapter" data-level="1.3.2" data-path="essential-probability.html"><a href="essential-probability.html#moment-generating-functions"><i class="fa fa-check"></i><b>1.3.2</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="essential-probability.html"><a href="essential-probability.html#independent-random-variables"><i class="fa fa-check"></i><b>1.4</b> Independent Random Variables</a></li>
</ul></li>
<li class="part"><span><b>I Language and Logic of Inference</b></span></li>
<li class="chapter" data-level="2" data-path="CaseDeepwater.html"><a href="CaseDeepwater.html"><i class="fa fa-check"></i><b>2</b> Case Study: Health Effects of the Deepwater Horizon Oil Spill</a></li>
<li class="chapter" data-level="3" data-path="Basics.html"><a href="Basics.html"><i class="fa fa-check"></i><b>3</b> The Statistical Process</a>
<ul>
<li class="chapter" data-level="3.1" data-path="Basics.html"><a href="Basics.html#overview-of-drawing-inference"><i class="fa fa-check"></i><b>3.1</b> Overview of Drawing Inference</a></li>
<li class="chapter" data-level="3.2" data-path="Basics.html"><a href="Basics.html#anatomy-of-a-dataset"><i class="fa fa-check"></i><b>3.2</b> Anatomy of a Dataset</a></li>
<li class="chapter" data-level="3.3" data-path="Basics.html"><a href="Basics.html#a-note-on-codebooks"><i class="fa fa-check"></i><b>3.3</b> A Note on Codebooks</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Questions.html"><a href="Questions.html"><i class="fa fa-check"></i><b>4</b> Asking the Right Questions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="Questions.html"><a href="Questions.html#characterizing-a-variable"><i class="fa fa-check"></i><b>4.1</b> Characterizing a Variable</a></li>
<li class="chapter" data-level="4.2" data-path="Questions.html"><a href="Questions.html#framing-the-question"><i class="fa fa-check"></i><b>4.2</b> Framing the Question</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Data.html"><a href="Data.html"><i class="fa fa-check"></i><b>5</b> Gathering the Evidence (Data Collection)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="Data.html"><a href="Data.html#what-makes-a-sample-reliable"><i class="fa fa-check"></i><b>5.1</b> What Makes a Sample Reliable</a></li>
<li class="chapter" data-level="5.2" data-path="Data.html"><a href="Data.html#poor-methods-of-data-collection"><i class="fa fa-check"></i><b>5.2</b> Poor Methods of Data Collection</a></li>
<li class="chapter" data-level="5.3" data-path="Data.html"><a href="Data.html#preferred-methods-of-sampling"><i class="fa fa-check"></i><b>5.3</b> Preferred Methods of Sampling</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Summaries.html"><a href="Summaries.html"><i class="fa fa-check"></i><b>6</b> Presenting the Evidence (Summarizing Data)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="Summaries.html"><a href="Summaries.html#characteristics-of-a-distribution-summarizing-a-single-variable"><i class="fa fa-check"></i><b>6.1</b> Characteristics of a Distribution (Summarizing a Single Variable)</a></li>
<li class="chapter" data-level="6.2" data-path="Summaries.html"><a href="Summaries.html#summarizing-relationships"><i class="fa fa-check"></i><b>6.2</b> Summarizing Relationships</a></li>
</ul></li>
<li class="part"><span><b>II Fundamentals of Bayesian Inference</b></span></li>
<li class="chapter" data-level="7" data-path="bayes-rule.html"><a href="bayes-rule.html"><i class="fa fa-check"></i><b>7</b> Bayes Rule</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bayes-rule.html"><a href="bayes-rule.html#tenants-of-the-bayesian-approach-to-inference"><i class="fa fa-check"></i><b>7.1</b> Tenants of the Bayesian Approach to Inference</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="modeling-samples.html"><a href="modeling-samples.html"><i class="fa fa-check"></i><b>8</b> Modeling Samples</a>
<ul>
<li class="chapter" data-level="8.1" data-path="modeling-samples.html"><a href="modeling-samples.html#independence"><i class="fa fa-check"></i><b>8.1</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="quantifying-prior-information.html"><a href="quantifying-prior-information.html"><i class="fa fa-check"></i><b>9</b> Quantifying Prior Information</a></li>
<li class="chapter" data-level="10" data-path="posterior-distributions.html"><a href="posterior-distributions.html"><i class="fa fa-check"></i><b>10</b> Updating Prior Beliefs (Posterior Distributions)</a></li>
<li class="chapter" data-level="11" data-path="point-estimation.html"><a href="point-estimation.html"><i class="fa fa-check"></i><b>11</b> Point Estimation</a></li>
<li class="chapter" data-level="12" data-path="interval-estimation.html"><a href="interval-estimation.html"><i class="fa fa-check"></i><b>12</b> Interval Estimation</a></li>
<li class="chapter" data-level="13" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>13</b> Prediction</a>
<ul>
<li class="chapter" data-level="13.1" data-path="prediction.html"><a href="prediction.html#derivation-of-the-posterior-predictive"><i class="fa fa-check"></i><b>13.1</b> Derivation of the Posterior Predictive</a></li>
<li class="chapter" data-level="13.2" data-path="prediction.html"><a href="prediction.html#summary"><i class="fa fa-check"></i><b>13.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>14</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="14.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#point-null-hypotheses"><i class="fa fa-check"></i><b>14.1</b> Point-Null Hypotheses</a></li>
<li class="chapter" data-level="14.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#model-comparison"><i class="fa fa-check"></i><b>14.2</b> Model Comparison</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="constructing-priors.html"><a href="constructing-priors.html"><i class="fa fa-check"></i><b>15</b> Constructing Prior Distributions</a>
<ul>
<li class="chapter" data-level="15.1" data-path="constructing-priors.html"><a href="constructing-priors.html#elicitation-from-experts"><i class="fa fa-check"></i><b>15.1</b> Elicitation from Experts</a></li>
<li class="chapter" data-level="15.2" data-path="constructing-priors.html"><a href="constructing-priors.html#mixtures"><i class="fa fa-check"></i><b>15.2</b> Mixtures</a></li>
<li class="chapter" data-level="15.3" data-path="constructing-priors.html"><a href="constructing-priors.html#chains"><i class="fa fa-check"></i><b>15.3</b> Chains</a></li>
<li class="chapter" data-level="15.4" data-path="constructing-priors.html"><a href="constructing-priors.html#non-informative-priors"><i class="fa fa-check"></i><b>15.4</b> Non-Informative Priors</a></li>
</ul></li>
<li class="part"><span><b>III Numerical Approaches to Bayesian Computations</b></span></li>
<li class="chapter" data-level="16" data-path="mc-integration.html"><a href="mc-integration.html"><i class="fa fa-check"></i><b>16</b> Monte Carlo Integration</a></li>
<li class="chapter" data-level="17" data-path="mcmc.html"><a href="mcmc.html"><i class="fa fa-check"></i><b>17</b> Markov Chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="17.1" data-path="mcmc.html"><a href="mcmc.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>17.1</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="mcmc-assessment.html"><a href="mcmc-assessment.html"><i class="fa fa-check"></i><b>18</b> Assessing MCMC Samples</a></li>
<li class="chapter" data-level="19" data-path="study-design.html"><a href="study-design.html"><i class="fa fa-check"></i><b>19</b> Elements of Good Study Design</a></li>
<li class="chapter" data-level="20" data-path="independent-groups.html"><a href="independent-groups.html"><i class="fa fa-check"></i><b>20</b> Models for Comparing Independent Groups</a>
<ul>
<li class="chapter" data-level="20.1" data-path="independent-groups.html"><a href="independent-groups.html#bridge-sampling"><i class="fa fa-check"></i><b>20.1</b> Bridge Sampling</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="dependent-groups.html"><a href="dependent-groups.html"><i class="fa fa-check"></i><b>21</b> Comparing Related Groups</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Course Notes for Bayesian Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="dependent-groups" class="section level1" number="21">
<h1><span class="header-section-number">21</span> Comparing Related Groups</h1>
<p>The previous chapter was a natural extension of the framework we had developed in earlier portions of the text. Assuming each group is independent, we were essentially able to model each group separately; the likelihood was then the product of the likelihoods from each group, and the prior was the product of the priors from each group. This independence will actually carry through into the posterior distribution. That is, the independence allows us to model the groups separately and then combine afterwards. However, independence between the groups is not always reasonable.</p>
<p>Recall that one of the aspects of a good study design is comparative groups — treatment groups which are similar except for the treatment being applied. The benefit of this is that it reduces extraneous variability. One method of constructing such similar groups is known as blocking.</p>
<div class="definition">
<p><span id="def:defn-blocking" class="definition"><strong>Definition 21.1  (Blocking) </strong></span>One way of minimizing variability contributed by an intrinsic characteristic. All observations that are linked through the characteristic are grouped together and treatments are applied within the block.</p>
</div>
<p>A very common example of blocking is a pre/post test. In such settings, participants are given a baseline assessment. Then, each participant is exposed to some form of treatment; following this, participants take an another assessment. Interest is typically on quantifying the change from baseline. In this case, the two groups (pre-treatment and post-treatment) are not only similar, they are identical! Intuitively, this is a good design because we are allowing every individual to act as their own control. We have eliminated all other external sources of variability allowing us to focus on the treatment of interest. Here, the individual participants act as the blocks. In such cases, we believe the variability among observations between blocks is greater than the variability among observations within a block. It is therefore no longer reasonable to assume the observations within a block are independent.</p>
<div class="rmdkeyidea">
<p>When subjects can be blocked (or pooled) into similar groups, independence is not reasonable.</p>
</div>
<p>In such cases, we lean on the hierarchical nature of the data generating process.</p>
<div class="example">
<p><span id="exm:ex-exams" class="example"><strong>Example 21.1  (Final Exams) </strong></span>Common final exams are typical for multiple sections of the same course. For example, there may be four instructors, each teaching two sections of Calculus; all eight sections will receive the same final exam. Suppose each exam is graded out of 100 points and we are interested in modeling the exam score for students taking the exam.</p>
</div>
<p>We might start off by assuming a common distribution for all students. That is,</p>
<p><span class="math display">\[
\begin{aligned}
  Y_i \mid \boldsymbol{\theta} &amp;\stackrel{IID}{\sim} f(y \mid \boldsymbol{\theta}) \\
  \theta &amp;\sim \pi(\theta)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(Y_i\)</span> is the exam score for the <span class="math inline">\(i\)</span>-th subject. However, this model is imposing a particular assumptions — there are no differences among instructors that would impact the scores of the students. Essentially, every instructor delivers the same content in the same way; the students might has well have been taught by the same instructor. While this is a nice ideal, it is typically not the case. Autonomy in the classroom means that different instructors approach the material differently — emphasizing different topics and presenting the material in different ways. As a result, it is possible that students who share an instructor are more likely to perform well on the same types of problems and also more likely to make the same types of mistakes (compared with students who do not share the same instructor). The additional variability introduced by the differences across instructors is being ignored.</p>
<p>We might try to correct for this by assuming the instructors are completely independent of one another, following the approach of the previous chapter. This would say</p>
<p><span class="math display">\[
\begin{aligned}
  Y_{j, i} \mid \boldsymbol{\theta}_j &amp;\stackrel{IID}{\sim} f(y \mid \boldsymbol{\theta}_j) \\
  Y_{r, s} &amp;\perp\negthickspace\negmedspace\perp Y_{t, u} \quad \forall r, s, t, u \\
  \boldsymbol{\theta}_j &amp;\stackrel{Ind}{\sim} \pi_j(\boldsymbol{\theta}_j)
\end{aligned}
\]</span></p>
<p>However, this model has very limited utility in practice. If we wanted to predict the exam score of a student taking calculus, our model would only be applicable if they were taking it with one of <em>these</em> four instructors! That is, we do not actually care about comparing Instructor 1 and Instructor 2; and, the next time the course is offered, there are likely to be four completely new instructors. Further, we do not actually think the instructors are completely unrelated (they are teaching the same major content after all…at least, we would hope so). So, this extreme perspective also seems to ignore the structure in the problem.</p>
<div class="rmdkeyidea">
<p>How we model the relationship between groups, if one exists at all, must be based on the context of the problem.</p>
</div>
<p>It seems natural to think of these four instructors as a representative sample from a population of all instructors. That is, we are adding a layer to the data generating process. Further, the way in which these instructors impact the data-generating process is through the formation of the parameters <span class="math inline">\(\boldsymbol{\theta}_j\)</span>. Therefore, we incorporate this into the model:</p>
<p><span class="math display">\[
\begin{aligned}
  Y_{j, i} \mid \boldsymbol{\theta}_j &amp;\stackrel{IID}{\sim} f(y \mid \boldsymbol{\theta}_j) \\
  \boldsymbol{\theta}_j \mid \boldsymbol{\eta} &amp;\stackrel{IID}{\sim} \pi(\boldsymbol{\theta} \mid \boldsymbol{\eta}) \\
  \boldsymbol{\eta} &amp;\sim \pi(\boldsymbol{\eta})
\end{aligned}
\]</span></p>
<p>That is, our model has three layers:</p>
<ul>
<li>Layer 1: Describes how students <em>within</em> an instructor perform; the parameters are unique to each instructor but shared across students with the same instructor.</li>
<li>Layer 2: Describes the variability <em>across</em> instructors; treating the instructors as a random sample of all instructors, we allow the parameters to move across instructors according to some overall model.</li>
<li>Layer 3: Describes our prior beliefs about the shared parameters for the instructor-level model.</li>
</ul>
<p>This hierarchical model bridges the gap between ignoring the blocks and treating the blocks as independent groups. This allows us to pool information which is similar across blocks while allowing unique properties to exist for each block as well.</p>
<div class="rmdkeyidea">
<p>When the dependence in data is the result of a hierarchical data generating process, we can incorporate that additional variability by modeling the hierarchical structure directly. At a minimum, this has the following three layers:</p>
<ul>
<li>Layer 1: Describes how observations <em>within</em> a block behave; with parameters unique to each block.</li>
<li>Layer 2: Describes how the parameters move <em>across</em> blocks; this views the blocks as a random sample.</li>
<li>Layer 3: Describes our prior beliefs on the common parameters for the model in Layer 2.</li>
</ul>
</div>
<p>It is natural to ask the difference between a block and a factor (or group that we considered in the previous chapter). The difference is primarily in how we address them in the modeling. If we are interested in making group-to-group comparisons, and if we were to repeat the study the groups would remain the same, this should be treated as a factor of interest. If the groups primarily capture an additional source of variability and can be viewed as a random sample from some larger population, this should be treated as a block.</p>
<p>Of course, we can combine the two ideas. Suppose further that we were interested in comparing students majoring in mathematics and those not majoring in mathematics. The first layer of the model must still describe what happens within a block (or within an instructor in our example). The assumptions you are willing to make determine how complex this becomes. For example, assuming that the difference between mathematics majors and non-mathematics majors is similar regardless of the instructor implies that mathematics majors share some common parameter across instructors. However, allowing the difference to vary across instructors implies that there is no common parameter. This is addressed within the first two layers of the model. If the effect is held constant across the blocks, a single set of “global” parameters enters into Layer 1 for which priors are described in Layer 3. If the effect is allowed to vary, then the way in which they vary is defined in Layer 2. To broaden our model before, we might have something of the form</p>
<p><span class="math display">\[
\begin{aligned}
  Y_{j, i} \mid \boldsymbol{\theta}_j, \boldsymbol{\phi} &amp;\stackrel{IID}{\sim} f(y \mid \boldsymbol{\theta}_j, \boldsymbol{\phi}) \\
  \boldsymbol{\theta}_j \mid \boldsymbol{\eta} &amp;\stackrel{IID}{\sim} \pi(\boldsymbol{\theta} \mid \boldsymbol{\eta}) \\
  \boldsymbol{\eta} &amp;\sim \pi(\boldsymbol{\eta}) \\
  \boldsymbol{\phi} &amp;\sim \pi(\boldsymbol{\phi})
\end{aligned}
\]</span></p>
<p>In this formulation, <span class="math inline">\(\boldsymbol{\phi}\)</span> represents parameters which are constant across the blocks; so, they are separated out from <span class="math inline">\(\boldsymbol{\theta}_j\)</span> as they do not vary across blocks. We place a prior directly on these parameters in Layer 3.</p>
<p>Note that we have been intentionally vague about the modeling structure by leaving everything in terms of <span class="math inline">\(f\)</span> instead of defining a specific model. The form of the model will change depending on the data generating process. Further, which parameters in that family may be altered. For example, suppose we considered a Normal distribution for the data generating process; we could allow the mean response to vary across groups, the variability to vary across groups, both or neither. This is true for both the blocks as well as the treatment groups! That is, our modeling should be specific to the question at hand and the data generating process being considered.</p>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="independent-groups.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
