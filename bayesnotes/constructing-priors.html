<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>15 Constructing Prior Distributions | Course Notes for Bayesian Data Analysis</title>
  <meta name="description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="15 Constructing Prior Distributions | Course Notes for Bayesian Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="15 Constructing Prior Distributions | Course Notes for Bayesian Data Analysis" />
  
  <meta name="twitter:description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  

<meta name="author" content="Eric M Reyes" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="hypothesis-testing.html"/>
<link rel="next" href="mc-integration.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="essential-probability.html"><a href="essential-probability.html"><i class="fa fa-check"></i><b>1</b> Essential Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="essential-probability.html"><a href="essential-probability.html#density-functions-as-models"><i class="fa fa-check"></i><b>1.1</b> Density Functions as Models</a></li>
<li class="chapter" data-level="1.2" data-path="essential-probability.html"><a href="essential-probability.html#summarizing-distributions-parameters"><i class="fa fa-check"></i><b>1.2</b> Summarizing Distributions (Parameters)</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="essential-probability.html"><a href="essential-probability.html#kernels"><i class="fa fa-check"></i><b>1.2.1</b> Kernels</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="essential-probability.html"><a href="essential-probability.html#transformations-of-random-variables"><i class="fa fa-check"></i><b>1.3</b> Transformations of Random Variables</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="essential-probability.html"><a href="essential-probability.html#expectations-of-functions"><i class="fa fa-check"></i><b>1.3.1</b> Expectations of Functions</a></li>
<li class="chapter" data-level="1.3.2" data-path="essential-probability.html"><a href="essential-probability.html#moment-generating-functions"><i class="fa fa-check"></i><b>1.3.2</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="essential-probability.html"><a href="essential-probability.html#independent-random-variables"><i class="fa fa-check"></i><b>1.4</b> Independent Random Variables</a></li>
</ul></li>
<li class="part"><span><b>I Language and Logic of Inference</b></span></li>
<li class="chapter" data-level="2" data-path="CaseDeepwater.html"><a href="CaseDeepwater.html"><i class="fa fa-check"></i><b>2</b> Case Study: Health Effects of the Deepwater Horizon Oil Spill</a></li>
<li class="chapter" data-level="3" data-path="Basics.html"><a href="Basics.html"><i class="fa fa-check"></i><b>3</b> The Statistical Process</a>
<ul>
<li class="chapter" data-level="3.1" data-path="Basics.html"><a href="Basics.html#overview-of-drawing-inference"><i class="fa fa-check"></i><b>3.1</b> Overview of Drawing Inference</a></li>
<li class="chapter" data-level="3.2" data-path="Basics.html"><a href="Basics.html#anatomy-of-a-dataset"><i class="fa fa-check"></i><b>3.2</b> Anatomy of a Dataset</a></li>
<li class="chapter" data-level="3.3" data-path="Basics.html"><a href="Basics.html#a-note-on-codebooks"><i class="fa fa-check"></i><b>3.3</b> A Note on Codebooks</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Questions.html"><a href="Questions.html"><i class="fa fa-check"></i><b>4</b> Asking the Right Questions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="Questions.html"><a href="Questions.html#characterizing-a-variable"><i class="fa fa-check"></i><b>4.1</b> Characterizing a Variable</a></li>
<li class="chapter" data-level="4.2" data-path="Questions.html"><a href="Questions.html#framing-the-question"><i class="fa fa-check"></i><b>4.2</b> Framing the Question</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Data.html"><a href="Data.html"><i class="fa fa-check"></i><b>5</b> Gathering the Evidence (Data Collection)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="Data.html"><a href="Data.html#what-makes-a-sample-reliable"><i class="fa fa-check"></i><b>5.1</b> What Makes a Sample Reliable</a></li>
<li class="chapter" data-level="5.2" data-path="Data.html"><a href="Data.html#poor-methods-of-data-collection"><i class="fa fa-check"></i><b>5.2</b> Poor Methods of Data Collection</a></li>
<li class="chapter" data-level="5.3" data-path="Data.html"><a href="Data.html#preferred-methods-of-sampling"><i class="fa fa-check"></i><b>5.3</b> Preferred Methods of Sampling</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Summaries.html"><a href="Summaries.html"><i class="fa fa-check"></i><b>6</b> Presenting the Evidence (Summarizing Data)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="Summaries.html"><a href="Summaries.html#characteristics-of-a-distribution-summarizing-a-single-variable"><i class="fa fa-check"></i><b>6.1</b> Characteristics of a Distribution (Summarizing a Single Variable)</a></li>
<li class="chapter" data-level="6.2" data-path="Summaries.html"><a href="Summaries.html#summarizing-relationships"><i class="fa fa-check"></i><b>6.2</b> Summarizing Relationships</a></li>
</ul></li>
<li class="part"><span><b>II Fundamentals of Bayesian Inference</b></span></li>
<li class="chapter" data-level="7" data-path="bayes-rule.html"><a href="bayes-rule.html"><i class="fa fa-check"></i><b>7</b> Bayes Rule</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bayes-rule.html"><a href="bayes-rule.html#tenants-of-the-bayesian-approach-to-inference"><i class="fa fa-check"></i><b>7.1</b> Tenants of the Bayesian Approach to Inference</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="modeling-samples.html"><a href="modeling-samples.html"><i class="fa fa-check"></i><b>8</b> Modeling Samples</a>
<ul>
<li class="chapter" data-level="8.1" data-path="modeling-samples.html"><a href="modeling-samples.html#independence"><i class="fa fa-check"></i><b>8.1</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="quantifying-prior-information.html"><a href="quantifying-prior-information.html"><i class="fa fa-check"></i><b>9</b> Quantifying Prior Information</a></li>
<li class="chapter" data-level="10" data-path="posterior-distributions.html"><a href="posterior-distributions.html"><i class="fa fa-check"></i><b>10</b> Updating Prior Beliefs (Posterior Distributions)</a></li>
<li class="chapter" data-level="11" data-path="point-estimation.html"><a href="point-estimation.html"><i class="fa fa-check"></i><b>11</b> Point Estimation</a></li>
<li class="chapter" data-level="12" data-path="interval-estimation.html"><a href="interval-estimation.html"><i class="fa fa-check"></i><b>12</b> Interval Estimation</a></li>
<li class="chapter" data-level="13" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>13</b> Prediction</a>
<ul>
<li class="chapter" data-level="13.1" data-path="prediction.html"><a href="prediction.html#derivation-of-the-posterior-predictive"><i class="fa fa-check"></i><b>13.1</b> Derivation of the Posterior Predictive</a></li>
<li class="chapter" data-level="13.2" data-path="prediction.html"><a href="prediction.html#summary"><i class="fa fa-check"></i><b>13.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>14</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="14.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#point-null-hypotheses"><i class="fa fa-check"></i><b>14.1</b> Point-Null Hypotheses</a></li>
<li class="chapter" data-level="14.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#model-comparison"><i class="fa fa-check"></i><b>14.2</b> Model Comparison</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="constructing-priors.html"><a href="constructing-priors.html"><i class="fa fa-check"></i><b>15</b> Constructing Prior Distributions</a>
<ul>
<li class="chapter" data-level="15.1" data-path="constructing-priors.html"><a href="constructing-priors.html#elicitation-from-experts"><i class="fa fa-check"></i><b>15.1</b> Elicitation from Experts</a></li>
<li class="chapter" data-level="15.2" data-path="constructing-priors.html"><a href="constructing-priors.html#mixtures"><i class="fa fa-check"></i><b>15.2</b> Mixtures</a></li>
<li class="chapter" data-level="15.3" data-path="constructing-priors.html"><a href="constructing-priors.html#chains"><i class="fa fa-check"></i><b>15.3</b> Chains</a></li>
<li class="chapter" data-level="15.4" data-path="constructing-priors.html"><a href="constructing-priors.html#non-informative-priors"><i class="fa fa-check"></i><b>15.4</b> Non-Informative Priors</a></li>
</ul></li>
<li class="part"><span><b>III Numerical Approaches to Bayesian Computations</b></span></li>
<li class="chapter" data-level="16" data-path="mc-integration.html"><a href="mc-integration.html"><i class="fa fa-check"></i><b>16</b> Monte Carlo Integration</a></li>
<li class="chapter" data-level="17" data-path="mcmc.html"><a href="mcmc.html"><i class="fa fa-check"></i><b>17</b> Markov Chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="17.1" data-path="mcmc.html"><a href="mcmc.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>17.1</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="mcmc-assessment.html"><a href="mcmc-assessment.html"><i class="fa fa-check"></i><b>18</b> Assessing MCMC Samples</a></li>
<li class="chapter" data-level="19" data-path="study-design.html"><a href="study-design.html"><i class="fa fa-check"></i><b>19</b> Elements of Good Study Design</a></li>
<li class="chapter" data-level="20" data-path="independent-groups.html"><a href="independent-groups.html"><i class="fa fa-check"></i><b>20</b> Models for Comparing Independent Groups</a>
<ul>
<li class="chapter" data-level="20.1" data-path="independent-groups.html"><a href="independent-groups.html#bridge-sampling"><i class="fa fa-check"></i><b>20.1</b> Bridge Sampling</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="dependent-groups.html"><a href="dependent-groups.html"><i class="fa fa-check"></i><b>21</b> Comparing Related Groups</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Course Notes for Bayesian Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="constructing-priors" class="section level1" number="15">
<h1><span class="header-section-number">15</span> Constructing Prior Distributions</h1>
<p>The selection of a prior distribution is critical to the Bayesian framework; it is also the most criticized component. There is rarely sufficient prior information to determine an exact prior; that is, rarely do we know for certain the family which represents the distribution as well as the exact parameters. Instead, we make some modeling assumptions, as with any analysis. In this chapter, we examine some common paths when constructing prior distributions and the implications of allowing the prior distribution to vary across analysts.</p>
<div id="elicitation-from-experts" class="section level2" number="15.1">
<h2><span class="header-section-number">15.1</span> Elicitation from Experts</h2>
<p>Ideally, the prior distribution would not be arbitrary but guided by experts. Up to this point, we have focused on using statements by experts to form a parametric approximation to the prior information. We have elicited information about the uncertainty in order to determine values for the hyperparameters — those values that determine the specific shape of the prior distribution. Often, these priors have been conjugate priors.</p>
<div class="definition">
<p><span id="def:conjugate-prior" class="definition"><strong>Definition 15.1  (Conjugate Prior) </strong></span>A prior chosen such that the posterior distribution belongs to the same family as the prior, with only updated parameters.</p>
</div>
<p>That is, if the prior is a Beta distribution, the posterior is also a Beta distribution. This was done historically in order to simplify computation in an era where computing power was limited. In the era of higher-speed computing, this is no longer a requirement.</p>
<p>One argument for the use of conjugate priors is that the form is invariant to the data; that is, the data is restricted in what it can say about the parameter. It can update our beliefs, but it cannot update the family which encodes those beliefs.</p>
<p>While we will not go into details here, it is <em>almost</em> always possible to construct a conjugate prior. And, if chosen carefully, that prior can approximate nearly any prior information you want.</p>
<div class="rmdtip">
<p>The posterior distribution is always a combination of the prior distribution and the likelihood. Conjugate priors make that very clear. As the sample gets large, the prior distribution is swamped by the data; and, as the sample size increases, Bayesian inference agrees with Frequentist inference.</p>
</div>
<p>When you are unable to determine a suitable parametric approximation, a histogram approach is possible.</p>
<div class="definition">
<p><span id="def:defn-histogram-prior" class="definition"><strong>Definition 15.2  (Histogram Approach to Prior Construction) </strong></span>Using expert information, attach probability to various intervals for the parameter:</p>
<ul>
<li>Define <span class="math inline">\(m+1\)</span> intervals over the parameter space.</li>
<li>Assign probability to each interval: <span class="math inline">\(\pi_j = Pr(\theta_{j-1} &lt; \theta &lt; \theta_j)\)</span> for each <span class="math inline">\(j=1,2,\dotsc,m\)</span> where <span class="math inline">\(\theta_0\)</span> represents the lower boundary of the support and <span class="math inline">\(\theta_{m+1}\)</span> represents the upper boundary of the support.</li>
<li>Set the prior <span class="math inline">\(\pi(\theta)\)</span> to be the piecewise distribution over this interval where <span class="math inline">\(\sum_{j=1}^{m+1} \pi_j = 1\)</span>.</li>
</ul>
</div>
<p>There have been critiques of eliciting information from experts. Estimates given may be biased, due to the current availability of data on which the experts are making their informed decisions. We tend to be overconfident in our opinions or go with our initial reaction instead of allowing our beliefs to be updated. We also tend to want to create the prior only after observing the data. The prior should always be constructed prior to observing data.</p>
<p>The experts may not actually represent a reasonable sample to capture widespread prior belief. How does one determine who is expertly qualified to speak on a particular topic? How do you rank levels of expertise?</p>
<p>We mention these critiques because more important than the choices you make is that those choices are clearly documented. It is okay to construct work that others critique; that is how science develops. No study is perfect, and being able to identify and own the limitations of our study and analysis is critical to the development of knowledge.</p>
</div>
<div id="mixtures" class="section level2" number="15.2">
<h2><span class="header-section-number">15.2</span> Mixtures</h2>
<p>Suppose we would like to work with a parametric approximation, but we cannot find a parametric family which captures the structure suggested by the prior information. In these cases, combining multiple distributions may be appropriate. As an example, consider the prior in Figure <a href="constructing-priors.html#fig:mixture-prior">15.1</a> for a parameter which has support <span class="math inline">\((0, 1)\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mixture-prior"></span>
<img src="images/mixture-prior-1.png" alt="Illustration of a mixture prior for a parameter on the interval (0, 1)." width="672" />
<p class="caption">
Figure 15.1: Illustration of a mixture prior for a parameter on the interval (0, 1).
</p>
</div>
<p>For a parameter with support <span class="math inline">\((0, 1)\)</span>, we naturally think of the Beta distribution. However, it is impossible to choose a Beta distribution which has the shape illustrated in Figure <a href="constructing-priors.html#fig:mixture-prior">15.1</a>.</p>
<div class="definition">
<p><span id="def:defn-mixture-distribution" class="definition"><strong>Definition 15.3  (Mixture Distribution) </strong></span>A distribution formed by taking a weighted sum of other distributions.</p>
<p><span class="math display">\[\pi(\theta) = \sum_{k=1}^{K} w_k \pi_k(\theta)\]</span></p>
<p>where <span class="math inline">\(\sum_{k=1}^{K} w_k = 1\)</span>.</p>
</div>
<p>We actually suggested the use of a mixture prior in dealing with point-null hypotheses. It turns out that if each component of the mixture distribution <span class="math inline">\(\pi_k(\theta)\)</span> is a member of the conjugate family, the entire prior will be conjugate (a weighted average of distributions). However, nothing requires that the individual components be of the same family (we could mix a Normal distribution with a t-distribution, for example).</p>
<div class="rmdtip">
<p>While we illustrate the use of a mixture distribution for the prior, nothing stops us from using a mixture distribution for the likelihood.</p>
</div>
<div class="rmdtip">
<p>It has been shown that a mixture distribution can approximate any distribution. That is, if we choose <span class="math inline">\(K\)</span> to be large enough, we can approximate any distributional shape with a mixture distribution.</p>
</div>
</div>
<div id="chains" class="section level2" number="15.3">
<h2><span class="header-section-number">15.3</span> Chains</h2>
<p>In this collection of chapters, we have developed the fundamental concepts of Bayesian inference. We have largely avoided specific examples within this text, but we may have had a single parameter in mind whenever we discussed <span class="math inline">\(\theta\)</span>. Many interesting questions, however, involve models for the data that are depend upon multiple parameters. These types of problems often necessitate the need for numerical solutions, which we will address in the next unit. Here, we simply discuss that a common tool for addressing priors over multiple parameters.</p>
<p>When <span class="math inline">\(\theta\)</span> is a parameter <em>vector</em>, then <span class="math inline">\(\pi(\theta)\)</span> is actually a <em>joint</em> density across all parameters. Therefore, one key decision that must be made is whether, a priori, we believe these parameters to be independent of one another.</p>
<div class="example">
<p><span id="exm:ex-heights" class="example"><strong>Example 15.1  (Heights) </strong></span>During early development, children are regularly benchmarked against national growth charts. One such chart traces a child’s height as they grow. However, these charts were developed using the entire population of “healthy” children. Suppose I am interested in developing such a chart for children with Hispanic parents, as I believe they tend to be a bit shorter, on average. It is typical to model heights as Normally distributed. We need to develop a prior for the unknown parameters.</p>
</div>
<p>The likelihood for the above example is readily available if we are willing to assume a random sample of <span class="math inline">\(n\)</span> children (of the same age) born to Hispanic parents:</p>
<p><span class="math display">\[
\begin{aligned}
  f(\mathbf{y} \mid \mu, \tau) 
    &amp;= \prod_{i=1}^{n} \frac{\tau^{1/2}}{\sqrt{2\pi}} e^{-\tau/2 (y_i - \mu)^2} \\
    &amp;= \frac{\tau^{n/2}}{(2\pi)^{n/2}} e^{-(\tau/2)\sum_{i=1}^{n}(y_i - \mu)^2}
\end{aligned}
\]</span></p>
<p>where we have defined the likelihood in terms of the mean <span class="math inline">\(\mu\)</span> and the <em>precision</em> <span class="math inline">\(\tau = 1/\sigma^2\)</span>, instead of the variance. Suppose we believe the parameters are independent of one another, then it is reasonable to propose the prior distributions independently; this leads to</p>
<p><span class="math display">\[
\begin{aligned}
  \pi(\mu) &amp;= \frac{\sqrt{b}}{\sqrt{2\pi}} e^{-b/2 (\mu - a)^2} \\
  \pi(\tau) &amp;= \frac{s^r}{\Gamma(r)} \tau^{r - 1} e^{-s\tau} \\
  \pi(\mu, \tau) &amp;= \pi(\mu) \pi(\tau)
\end{aligned}
\]</span></p>
<p>Of course, this is a modeling assumption. A different set of beliefs would lead to a different structure for the prior. For example, consider</p>
<p><span class="math display">\[
\begin{aligned}
  \pi(\tau) &amp;= \frac{s^2}{\Gamma(r)} \tau^{r - 1} e^{-s\tau} \\
  \pi(\mu \mid \tau) &amp;= \frac{\sqrt{\tau}}{\sqrt{2\pi}} e^{-\tau/2 (\mu - a)^2} \\
  \pi(\mu, \tau) &amp;= \pi(\mu \mid \tau) \pi(\tau)
\end{aligned}
\]</span></p>
<p>This has a hierarchical structure in which the distribution of the mean <span class="math inline">\(\mu\)</span> depends on the precision <span class="math inline">\(\tau\)</span>. The joint distribution of the parameters (prior to seeing the data) is then the product of the marginal distribution of <span class="math inline">\(\tau\)</span> and the conditional distribution of <span class="math inline">\(\mu \mid \tau\)</span>.</p>
<p>As an aside, consider the formation of the posterior; Bayes Theorem will have a denominator of the form</p>
<p><span class="math display">\[\int \int f(\mathbf{y} \mid \mu, \tau) \pi(\mu, \tau) d\mu d\tau\]</span></p>
<p>regardless of which prior formation we develop. The more parameters we have, the more complex the integration in the denominator; this is what motivates the next unit.</p>
<p>This process of defining a prior in stages by conditioning on other parameters is known as a “chaining.”</p>
</div>
<div id="non-informative-priors" class="section level2" number="15.4">
<h2><span class="header-section-number">15.4</span> Non-Informative Priors</h2>
<p>The discussion above assumes that our goal is to encode a set of prior information into a distribution. What if we have very little prior information? How do we encode ignorance?</p>
<div class="definition">
<p><span id="def:defn-laplaces-prior" class="definition"><strong>Definition 15.4  (Laplac's Prior) </strong></span>Also known as a “flat” prior, it considers the form</p>
<p><span class="math display">\[\pi(\theta) = 1 \qquad \forall \theta \in \Theta\]</span></p>
<p>This prior is often <em>improper</em>; that is, <span class="math inline">\(\int \pi(\theta) d\theta = \infty\)</span>.</p>
</div>
<p>Since we are taking the prior to be a constant over the entire support, not just an interval, whenever that support is not bounded, the prior will not be a true density. This seems like it is breaking all the rules, and to some degree it is, but it is still commonly used. This is often used as a default prior when no (or little) prior information is available.</p>
<div class="rmdtip">
<p>A flat prior cannot be expected to represent total ignorance. It is essentially saying that no matter how large of a value you imagine, there is still infinite probability that the parameter takes a value larger than that. The goal is simply to choose a prior that is easily overwhelmed by the likelihood.</p>
</div>
<p>The benefit of a flat prior is that the posterior distribution is proportional to the likelihood. The idea here is to make the Bayesian framework dependent solely on the data, similar to a Frequentist approach (though the two are still not guaranteed to give the same results).</p>
<p>Flat priors are a subset of a larger class of priors that is an active area of research in which the prior is determined solely by the form of the likelihood.</p>
<div class="definition">
<p><span id="def:defn-noninformative-prior" class="definition"><strong>Definition 15.5  (Noninformative Prior) </strong></span>A prior which is derived solely on the basis of the likelihood.</p>
</div>
<p>This seems like a happy middle ground between Bayesians and those who dislike the subjective nature of a prior distribution. So, why is this not the standard? On the one hand, true Bayesians argue that we should make use of prior information; we should not seek to make use of only the data available in that single study. This allows the information from one study to become the prior information for a follow-up study instead of beginning from scratch. Second, there is a potential pitfall when using noninformative priors when they are improper — it is possible for the posterior distribution to be improper (which is a nice way of saying it is not a distribution at all)! If the posterior distribution is improper, it cannot yield any valid inference on the parameters.</p>
<div class="rmdwarning">
<p>The Bayes Factor should not be computed when you have an improper prior as the prior odds are not defined since there is no valid probability of each hypothesis a priori.</p>
</div>
<p>An improper prior <em>can</em> lead to an improper posterior; however, a proper prior will <em>always</em> lead to a proper posterior. The danger is that software which automates Bayesian analyses currently have no way of checking if a posterior is proper; so, this must be done manually. As computing the posterior can be difficult (the entire reason for the next unit), this often involves bounding the integral in some way — a job for true mathematicians.</p>
<p>Fear around improper priors often leads to what are known as vague priors. This is taking a parametric family and choosing the hyperparameters to result in massive variances so that the prior, while proper, appears flat over the parameter space. The idea here is to allow the data to easily overwhelm any prior information.</p>
<div class="rmdkeyidea">
<p>Noninformative priors try to make it easy for the likelihood to dominate the prior in the computation of the posterior distribution.</p>
</div>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="hypothesis-testing.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mc-integration.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
