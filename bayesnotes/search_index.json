[["index.html", "Course Notes for Bayesian Data Analysis Preface", " Course Notes for Bayesian Data Analysis Eric M Reyes Last Updated: 2021-11-18 Preface Statistical analyses are used to make statements about a population using only a sample from the population, a process known as statistical inference. This resource introduces the Bayesian framework for statistical inference. Building from Bayes Rule for probability computations, we develop a framework of estimation and hypothesis testing. We will examine inference in several scenarios, including regression analysis. We discuss the construction of prior distributions given prior information about a parameter and give an introduction to computational tools for Bayesian inference, including Markov Chain Monte Carlo (MCMC) methods. While we do work through derivations when introducing the fundamental elements of Bayesian inference, we quickly move to computational approaches and emphasize application. Knowing the approach to modeling in the Bayesian framework and how to interpret the results is of primary importance. As with any text in statistics, we seek to develop your statistical literacy and statistical reasoning. "],["essential-probability.html", "1 Essential Probability 1.1 Density Functions as Models 1.2 Summarizing Distributions (Parameters) 1.3 Transformations of Random Variables 1.4 Independent Random Variables", " 1 Essential Probability Statistics uses data to make inference on a population. In turn, statistical theory is built on probability  the discipline of mathematics which studies and models random processes. This is particularly true within the Bayesian paradigm. An introductory course in probability provides a foundation in model random processes. Throughout the text, we will extend these ideas to modeling the process which generates observed data and the uncertainty in parameters that govern these processes. This chapter, however, provides a brief review of the most relevant aspects of probability theory necessary which permeate the remainder of the text. 1.1 Density Functions as Models Any process for which the outcome cannot be predicted with certainty is a random process. Typically, probability is taught from a mathematical perspective, with a goal of constructing a coherent and complete framework for characterizing such processes. Here, our goal is to introduce key probability concepts by relating them to their data-centric analogues. That is, we want to think of probability in light of how we will use it in statistical analysis. Each time we collect data, we can think of each observation as the result of a random process. These observations are recorded as variables in our dataset. Technically, a random variable is a function which maps outcomes from a random process to the real line; however, it suffices to think of a random variable as representing a measurement that results from a random process. Just as we have both quantitative and qualitative variables, there are continuous and discrete random variables. Definition 1.1 (Random Variable) Represents a measurement that will be collected and for which the value cannot be predicted with certainty. Generally represented with a capital letter. Continuous random variables represent quantitative measurements while discrete random variables represent qualitative measurements. While the exact value the random variable will take is unknown (because it results from a random process), we often know the range of the random variable  the possible values it can assume. In statistics, we refer to this as the support of the random variable. Definition 1.2 (Support) The set of all possible values a random variable can assume. We typically denote the support of the random variable \\(X\\) by \\(\\mathcal{S}_X\\). As a random variable is the result of a random process, each time we observe the process, the value of the random variable is subject to change. Further, it is often the case that not all values in the support are equally likely. A distribution is the way in which a random variable moves across its support. Probability is really about modeling and characterizing distributions. The most common way to represent a probability model is through its density function. Definition 1.3 (Density Function) A density function \\(f\\) relates the potential values of a random variable \\(X\\) with the probability those values occur. For a continuous random variable, the probability the random variable \\(X\\) falls within an interval \\((a, b)\\) is given by \\[Pr(a \\leq X \\leq b) = \\int_{a}^{b} f(x) dx.\\] For a discrete random variable, the probability the random variable \\(X\\) is equal to the value \\(u\\) is given by \\[Pr(X = u) = f(u).\\] In a probability course, there is often a distinction made between probability density functions (continuous random variables) and probability mass functions (discrete random variables). We do not make this distinction and instead rely on the context to determine whether we are dealing with a continuous or discrete random variable. From the definition, it is clear that the density function for a discrete random variable provides a very nice interpretation  it is a probability. However, for a continuous random variable, the density function is a smooth function over some region, and the actual value of the function is not interpretable; instead, we get at a probability by considering the area under the curve. Especially for visualization, the density function is the most common way of characterizing a probability model. However, computing the probability using the density is problematic due to the integration required. Many software programs address this by working with the cumulative distribution function (CDF). Definition 1.4 (Cumulative Distribution Function (CDF)) Let \\(X\\) be a random variable; the cumulative distribution function (CDF) is defined as \\[F(u) = Pr(X \\leq u).\\] For a continuous random variable, we have that \\[F(u) = \\int_{-\\infty}^{u} f(x) dx\\] implying that the density function is the derivative of the CDF. For a discrete random variable \\[F(u) = \\sum_{x \\leq u} f(x).\\] Working with the CDF improves computation because it avoids the need to integrate each time; instead, the integral is computed once (and stored internally in the computer), and we use the result to compute probabilities directly. Density functions are the mathematical models for distributions; they link values of the variable with the likelihood of occurence. However, for computational reasons, we often work with the cumulative distribution function which provides the probability of being less than or equal to a value. 1.2 Summarizing Distributions (Parameters) Most scientific questions are focused on the location or spread of a distribution. For example, we are interested in estimating the average yield of a crop, or the variance in the amount of sleep among college students. In probability, these are typically viewed simply as summaries of the entire distribution. In particular, the mean of a random variable (denoted by \\(E(X)\\)) and the variance of a random variable (denoted by \\(Var(X)\\)) are measures of the location and spread, respectively, of the distribution represented by its corresponding density function. When the density function is a model for the population, these represent the parameters of the population  the same parameters we will later estimate and make inference on using data analysis. Definition 1.5 (Mean and Variance of a Random Variable) Suppose \\(X\\) is a random variable with density function \\(f\\). If \\(X\\) is a continuous random variable, then the mean and variance are given by \\[ \\begin{aligned} E(X) &amp;= \\int x f(x) dx \\\\ Var(X) &amp;= \\int \\left(x - E(X)\\right)^2 f(x) dx \\end{aligned} \\] where the integral is taken over the support of \\(X\\). If \\(X\\) is a discrete random variable, then the mean and variance are given by \\[ \\begin{aligned} E(X) &amp;= \\sum x f(x) \\\\ Var(X) &amp;= \\sum \\left(x - E(X)\\right)^2 f(x) \\end{aligned} \\] where the sum is taken over the support of \\(X\\). In probability, often the distribution we are working with is completely determined. In reality, however, only the functional form of the distribution may be known, and therefore the distribution is known only up to some defining parameters that govern the shape. For example, a researcher might posit that within the population, the time until a medical device fails could be modeled using the density \\[f(x) = \\frac{1}{\\mu} e^{-\\frac{x}{\\mu}} \\qquad x &gt; 0.\\] Here, the researcher has really posited a form of the model, but not the exact model. The value \\(\\mu\\) is the average (which could be confirmed using the formulas in the above definition). In such cases, making inference on the parameters allows us to really characterize the entire distribution of the population. When a probability model is specified for a population, it is generally specified up to some unknown parameter(s). Making inference on the unknown parameter(s) therefore characterizes the entire distribution. 1.2.1 Kernels All density functions share some common properties. For example, a density function must be non-negative everywhere. We also know that if evaluated across the entire support, the density should sum to 1. That is, if \\(X\\) is a discrete random variable, then \\[\\sum_{\\mathcal{S}_X} f(x) = 1;\\] and, if \\(X\\) is a continuous random variable, then \\[\\int_{\\mathcal{S}_X} f(x) dx = 1.\\] Any density function \\(f(x)\\) can be rewritten as \\(f(x) = K h(x)\\) where \\(K &gt; 0\\) is a constant. We call \\(h\\) the kernel of the density function. Definition 1.6 (Kernel) A valid density function can be obtained by scaling the kernel such that integrating (or summing) over the support results in the value 1. Often, the kernel is useful for identifying the distributional family to which a random variable belongs. 1.3 Transformations of Random Variables Occasionally, we have some intuition about how to model the distribution of a random variable \\(X\\), but we are actually interested in the distribution of some function \\(g(X)\\) of that random variable. This is known as a variable transformation. For example, we may posit that the grade on an exam (when represented as a fraction) follow a Beta distribution; however, we are interested in characterizing the grade when it has been rescaled on the interval from 0 to 100. The derivation of the density function for the transformed random variable, when \\(X\\) is continuous, is often useful. While there are some well-known formulas for obtaining this density function, we prefer the following procedure: Define \\(Y = g(X)\\). Recognize that \\(F_Y(y) = Pr(Y \\leq y) = Pr(g(X) \\leq y)\\). Rewrite the above probability statement in terms of \\(X\\) and therefore the CDF \\(F_X\\) of \\(X\\). For monotone functions, this is simple by applying the inverse function \\(g^{-1}(y)\\) to both sides of the inequality within the probability statement; but, it may be more cumbersome in general. Compute the derivative of the result, with respect to \\(y\\) to obtain the density. Once you have the density function for \\(Y\\), which is a characterization of its distribution, you are able to summarize all aspects of its distribution as we would with any other random variable. 1.3.1 Expectations of Functions If all we are interested in is some summary of \\(Y = g(X)\\), determining the entire distribution is overkill. For example, suppose we are interested in \\(E(Y) = E\\left[g(X)\\right]\\); we could determine this by first performing the variable transformation to obtain the density of \\(Y\\) and then taking the expectation as defined in the previous section. However, there is a nice result that allows us to skip the intermediate step (it unfortunately is sometimes referred to as the Law of the Unconscious Statistician, which we of course object to). Definition 1.7 (Expectation of a Function of a Random Variable) Suppose \\(X\\) is a random variable with density function \\(f\\), and let \\(Y = g(X)\\). If \\(X\\) is a continuous random variable, then we have that \\[E(Y) = E\\left[g(X)\\right] = \\int g(x) f(x) dx\\] where the integral is taken over the support of \\(X\\). If \\(X\\) is a discrete random variable, then \\[E(Y) = E\\left[g(X)\\right] = \\sum g(x) f(x)\\] where the sum is taken over the support of \\(X\\). 1.3.2 Moment Generating Functions The mean and variance are known as the first two central moments of a distribution. For well-behaved distributions, any moment of a distribution can be computed from its moment generating function. Definition 1.8 (Moment Generating Function) Suppose \\(X\\) is a random variable with density function \\(f\\). The moment generating function (MGF) of \\(X\\), evaluated at \\(t\\), is given by \\[M_X(t) = E\\left[e^{tX}\\]] The \\(k\\)-th moment of \\(X\\) is then given by evaluating \\(M^{(k)}_X(t)\\), the \\(k\\)-th derivative of the MGF, at \\(t = 0\\). 1.4 Independent Random Variables As we will see as we progress, assuming random variables are independent greatly simplifies a model for the data generating process. In particular, when two random variables are independent, probability statements become easier to evaluate because and statements reduce to multiplication. Lemma 1.1 (Independence) If \\(X\\) and \\(Y\\) are independent random variables, then \\[Pr(a \\leq X \\leq b, c \\leq Y \\leq d) = Pr(a \\leq X \\leq b) Pr(c \\leq Y \\leq d).\\] Note: stating \\(Pr(A, B)\\) is equivalent to \\(Pr(A \\cap B)\\). "],["CaseDeepwater.html", "2 Case Study: Health Effects of the Deepwater Horizon Oil Spill", " 2 Case Study: Health Effects of the Deepwater Horizon Oil Spill On the evening of April 20, 2010, the Deepwater Horizon, an oil drilling platform positioned off the coast of Louisiana, was engulfed in flames as the result of an explosion. The drilling rig, leased and operated by BP, had been tasked with drilling an oil well in water nearly 5000 feet deep. Eleven personnel were killed in the explosion. The following screenshot is from the initial coverage by the New York Times1: Figure 2.1: New York Times coverage of the Deepwater Horizon oil spill. The incident is considered the worst oil spill in US history, creating an environmental disaster along the Gulf Coast. In addition to studying the effects on the local environment, researchers have undertaken studies to examine the short and long-term health effects caused by the incident. As an example, it is reasonable to ask whether volunteers who were directly exposed to oil, such as when cleaning wildlife, are at higher risk of respiratory irritation compared to those volunteers who were helping with administrative tasks and therefore were not directly exposed to the oil. An article appearing in The New England Journal of Medicine (Goldstein, Osofsky, and Lichtveld 2011) reported the results from a health symptom survey performed in the Spring and Summer of 2010 by the National Institute for Occupational Safety and Health. Of 54 volunteers assigned to wildlife cleaning and rehabilitation, 15 reported experiencing nose irritation, sinus problems, or sore throat. Of 103 volunteers who had no exposure to oil, dispersants, cleaners, or other chemicals, 16 reported experiencing nose irritation, sinus problems, or sore throat. While a larger fraction of volunteers cleaning wildlife in the study reported respiratory symptoms compared to those who were not directly exposed to irritants, would we expect similar results if we were able to interview all volunteers? What about during a future oil spill? Is there evidence that more than 1 in 5 volunteers who clean wildlife will develop respiratory symptoms? What is a reasonable value for the increased risk of respiratory symptoms for those volunteers with direct exposure compared to those without? In the first part of this text, we use this motivating example as the context for discussing how research questions should be framed, methods for data collection, and summarizing and presenting data clearly. References "],["Basics.html", "3 The Statistical Process 3.1 Overview of Drawing Inference 3.2 Anatomy of a Dataset 3.3 A Note on Codebooks", " 3 The Statistical Process Is driving while texting as dangerous as driving while intoxicated? Is there evidence that my measurement device is calibrated inappropriately? How much force, on average, can our concrete blocks withstand before failing? Regardless of your future career path, you will eventually need to answer a question. The discipline of statistics is about using data to address questions by converting that data into valuable information. Statistics is the discipline of converting data into information. It might be natural at this point to ask do I really need an entire class about answering questions with data? Isnt this simple? Sometimes, it is simple; other times, it can be far from it. Lets illustrate with the following example from Tintle et al. (2015). Example 3.1 (Organ Donation) Even though organ donations save lives, recruiting organ donors is difficult. Interestingly, surveys show that about 85% of Americans approve of organ donation in principle and many states offer a simple organ donor registration process when people apply for a drivers license. However, only about 38% of licensed drivers in the United States are registered to be organ donors. Some people prefer not to make an active decision about organ donation because the topic can be unpleasant to think about. But perhaps phrasing the question differently could affect a persons willingness to become a donor. Johnson and Goldstein (2003) recruited 161 participants for a study, published in the journal Science, to address the question of organ donor recruitment. The participants were asked to imagine they had moved to a new state and were applying for a drivers license. As part of this application, the participants were to decide whether or not to become an organ donor. Participants were presented with one of three different default choices: Some of the participants were forced to make a choice of becoming a donor or not, without being given a default option (the neutral group). Other participants were told that the default option was not to be a donor but that they could choose to become a donor if they wished (the opt-in group). The remaining participants were told that the default option was to be a donor but that they could choose not to become a donor if they wished (the opt-out group). The study found that 79% of those in the neutral group, 42% of those in the opt-in group, and 82.0% of those in the opt-out group agreed to become donors. The results of the study are presented in Figure 3.1. It seems obvious that using the opt-in strategy results in fewer people agreeing to organ donation. However, does the opt-out strategy, in which people are by default declared organ donors, result in more people agreeing to organ donation compared to the neutral strategy? On the one hand, a higher percentage did agree to organ donation under the opt-out (82% compared to 79%). However, since this study involved only a subset of Americans, is this enough evidence to claim the opt-out strategy is really superior compared to the neutral strategy in the broader population? The discipline of statistics provides a framework for addressing such ambiguity. Figure 3.1: Summary of the responses for the Organ Donation Study described in Example 3.1. 3.1 Overview of Drawing Inference Lets begin by taking a step back and considering the big picture of how data is turned into information. Every research question we pose, at its heart, is trying to characterize a population, the group of subjects of ultimate interest. Definition 3.1 (Population) The collection of subjects we would like to say something about. In the Organ Donation study, the researchers would like to say something about Americans who are of the age to consent to organ donation; in particular, they would like to quantify how likely it is that someone from this group agrees to organ donation. Therefore, the population is all Americans who are of the age to consent to organ donation. In general, the subjects in a population need not be people; in some studies, the population could be a collection of screws, cell phones, sheet metalwhatever characterizes the objects from which we would like to obtain measurements. We use the phrase like to because in reality it is often impossible (or impractical) to observe the entire population. Instead, we make observations on a subset of the population; this smaller group is known as the sample. Definition 3.2 (Sample) The collection of subjects for which we actually obtain measurements (data). For each subject within the sample, we obtain a collection of measurements forming our set of data. The goal of statistical modeling is to use the sample (the group we actually observe) to say something about the population of interest (the group we wish we had observed); this process is known as statistical inference. This process is illustrated in Figure 3.2. Definition 3.3 (Statistical Inference) The process of using a sample to characterize some aspect of the underlying population. Figure 3.2: Illustration of the statistical process. 3.2 Anatomy of a Dataset Once we have our sample, we take measurements on each of the subjects within this sample. These measurements form the data. When we hear the word data, most of us envision a large spreadsheet. In reality, data can take on many forms  spreadsheets, images, text files, unstructured text from a social media feed, etc. Regardless of the form, all datasets contain information for each subject in the sample; this information, the various measurements, are called variables. Definition 3.4 (Variable) A measurement, or category, describing some aspect of the subject. Variables come in one of two flavors. Categorical variables are those which denote a grouping to which the subject belongs. Examples include marital status, brand, and experimental treatment group. Numeric variables are those which take on values for which ordinary arithmetic (e.g., addition and multiplication) makes sense. Examples include height, age of a product, and diameter. Note that sometimes numeric values are used to represent the levels of a categorical variable in a dataset; for example, 0 may indicate No and 1 may indicate Yes for a variable capturing whether a person is a registered organ donor. Therefore, just because a variable has a numeric value does not make it a numeric variable; the key here is that numeric variables are those for which arithmetic makes sense. Definition 3.5 (Categorical Variable) Also called a qualitative variable, a measurement on a subject which denotes a grouping or categorization. Definition 3.6 (Numeric Variable) Also called a quantitative variable, a measurement on a subject which takes on a numeric value and for which ordinary arithmetic makes sense. While it may be natural to think of a dataset as a spreadsheet, not all spreadsheets are created equal. Here, we consider datasets which have the following characteristics: Each column contains a unique variable. Each record (row in the dataset) corresponds to a different observation of the variables. If you have multiple datasets, they should include a column in the table that allows them to be linked (subject identifier). These are characteristics of tidy data. Even unstructured data such as images or text files must be processed, often converted to tidy data, prior to performing a statistical analysis. The above description eliminates a common method of storing data in engineering and scientific disciplines  storing each sample in a different column. To illustrate, suppose we conduct a study comparing the lifetime (in hours) of two brands of batteries. We measure the lifetime of five batteries of Brand A and six of Brand B. It is common to see a dataset like that in Table 3.1; the problem here is that the first record of the dataset contains information on two different observations. We have the lifetime from a battery of Brand A in the same row as the lifetime from a battery of Brand B. This violates the second condition of tidy data described above. Table 3.1: Example of a common data structure which does not represent tidy data. Data is from a hypothetical study comparing battery lifetimes (hours). Brand A Brand B 8.3 8.4 5.1 8.6 3.3 3.8 5.3 4.1 5.7 4.5 4.0 In order to adhere to the tidy structure, we can reformat this dataset as illustrated in Table 3.2. Here, each record represents a unique observation and each column is a different variable. We have also added a unique identifier. Table 3.2: Example of a tidy dataset, a good way of storing data. Data is from a hypothetical study comparing battery lifetimes (hours). Battery Brand Lifetime 1 A 8.3 2 A 5.1 3 A 3.3 4 A 5.3 5 A 5.7 6 B 8.4 7 B 8.6 8 B 3.8 9 B 4.1 10 B 4.5 11 B 4.0 It may take some time to get used to storing data in this format, but it makes analysis easier and avoids time spent managing the data later. 3.3 A Note on Codebooks A dataset on its own is meaningless if you cannot understand what the values represent. Before you access a dataset, you should always review any available codebooks. Definition 3.7 (Codebook) Also called a data dictionary, these provide complete information regarding the variables contained within a dataset. Some codebooks are excellent, with detailed descriptions of how the variables were collected and appropriate units. Other codebooks give only an indication of what each variable represents. Whenever you are working with previously collected data, reviewing a codebook is the first step; and, you should be prepared to revisit the codebook often throughout an analysis. When you are collecting your own dataset, constructing a codebook is essential for others to make use of your data. References "],["Questions.html", "4 Asking the Right Questions 4.1 Characterizing a Variable 4.2 Framing the Question", " 4 Asking the Right Questions The discipline of statistics is about turning data into information in order to address some question. While there may be no such thing as a stupid question, there are ill-posed questions  those which cannot be answered as stated. Consider the Deepwater Horizon Case Study. It might seem natural to ask if a volunteer cleans wildlife, will she develop adverse respiratory symptoms? Lets consider the data. Of the 54 volunteers assigned to wildlife cleaning and rehabilitation, 15 reported experiencing adverse respiratory symptoms (nose irritation, sinus problems, or sore throat); while some volunteers developed symptoms, others did not. It seems the answer to our question is then it depends or maybe. This is an example of an ill-posed question. Such questions exist because of variability, the fact that every subject in the population does not behave in exactly the same way. In our example not every volunteer had the same reaction when directly exposed to oil. It is variability that creates a need for statistics; in fact, you could think of statistics as the study and characterization of variability. We must therefore learn to ask the right questions  those which can be answered in the presence of variability. Definition 4.1 (Variability) The notion that measurements differ from one observation to another. The presence of variability makes some questions ill-posed; statistics concerns itself with how to address questions in the presence of variability. 4.1 Characterizing a Variable Recall that the goal of statistical inference is to say something about the population; as a result, any question we ask should then be centered on this larger group. The first step to constructing a well-posed question is then to identify the population of interest for the study. For the Deepwater Horizon Case Study, it is unlikely that we are only interested in these 54 observed volunteers assigned to wildlife cleaning. In reality, we probably want to say something about volunteers for any oil spill. The 54 volunteers in our dataset form the sample, a subset from all volunteers who clean wildlife following an oil spill. Our population of interest is comprised of all volunteers who clean wildlife following an oil spill. When identifying the population of interest, be specific! Suppose you are trying to estimate the average height of trees. Are you really interested in all trees? Or, are you interested in Maple trees within the city limits of Terre Haute, Indiana? Since we expect that the reaction to oil exposure  the primary variable of interest for this study, sometimes called the response  to vary from one individual to another, we cannot ask a question about the value of the reaction (whether they experienced symptoms or not). Instead, we want to characterize the distribution of the response. Definition 4.2 (Response) The primary variable of interest within a study. This is the variable you would either like to explain or estimate. We will use random variables to represent the observations we will observe when we collect data; for example, \\(X_3\\) would refer to the value we expect to see for the third observation in our sample. Definition 4.3 (Distribution) The pattern of variability corresponding to a set of values. We will use probability theory to model the distribution of a response across the population. Notice that in this case, the response is a categorical variable; describing the distribution of such a variable is equivalent to describing how individuals are divided among the possible groups. With a finite number of observations, we could present the number of observations (frequeny) within each group. For example, of the 54 volunteers, 15 experienced adverse symptoms and 39 did not. This works well within the sample; however, as our population is infinitely large (all volunteers cleaning wildlife following an oil spill), reporting the frequencies is not appropriate. In this case, we report the fraction of observations (relative frequency) falling within each group; this helps convey information about the distribution of this variable. Definition 4.4 (Frequency) The number of observations falling into a particular level of a categorical variable. Definition 4.5 (Relative Frequency) Also called the proportion, the fraction of observations falling into a particular level of a categorical variable. Numeric quantities, like the proportion, which summarize the distribution of a variable within the population are known as parameters. Definition 4.6 (Parameter) Numeric quantity which summarizes the distribution of a variable within the population of interest. Generally denoted by Greek letters in statistical formulas. While the value of a variable may vary across the population, the parameter is a single fixed constant which summarizes the variable for that population. For example, the grade received on an exam varies from one student to another in a class; but, the average exam grade is a fixed number which summarizes the class as a whole. Well-posed questions can be constructed if we limit ourselves to questions about the parameter. The second step in constructing well-posed questions is then to identify the parameter of interest. The questions we ask generally fall into one of three categories: Estimation: what proportion of volunteers who clean wildlife following an oil spill will experience adverse respiratory symptoms? Hypothesis Testing: is it reasonable to expect that no more than 1 in 5 volunteers who clean wildlife following an oil spill will experience adverse respiratory symptoms? Prediction: how likely is a volunteer who cleans wildlife following an oil spill to experience adverse respiratory symptoms? Definition 4.7 (Estimation) Using the sample to approximate the value of a parameter from the underlying population. Definition 4.8 (Hypothesis Testing) Using a sample to determine if the data is consistent with a working theory or if there is evidence to suggest the data is not consistent with the theory. Definition 4.9 (Prediction) Using a sample to predict the value for an individual future observation. Since we do not get to observe the population (we only see the sample), we cannot observe the value of the parameter. That is, we will never know the true proportion of volunteers who experience symptoms. However, we can determine what the data suggests about the population (that is what inference is all about). Parameters are unknown values and can, in general, never be known. Parameters within the population correspond to the unknown parameters which define the mathematical probability model which describes the distribution of the response. It turns out, the vast majority of research questions can be framed in terms of a parameter. This is a fundamental ideas of inference. A research question can often be framed in terms of a parameter which characterizes the population. Framing the question should then guide our analysis. We now have a way of describing a well-posed question, a question which can be addressed using data. Well posed questions are about the population and can be framed in terms of a parameter which summarizes that population. We now describe how these questions are typically framed. 4.2 Framing the Question In engineering and scientific applications, many questions fall under the second category of model consistency. Examining such questions is known as hypothesis testing, which is a form of model comparison in which data is collected to help the researcher choose between two competing theories for the parameter of interest. In this section, we consider the terminology surrounding specifying such questions. For the Deepwater Horizon Case Study suppose we are interested in addressing the following question: Is there evidence that more than 1 in 5 volunteers who clean wildlife following an oil spill will develop adverse respiratory symptoms? The question itself is about the population (all volunteers assigned to clean wildlife following an oil spill) and is centered on a parameter (the proportion who develop adverse respiratory symptoms). That is, this is a well-posed question that can be answered with appropriate data. The overall process for addressing these types of questions is to form opposing statements defining reality, and then quantifying how likely each of those statements are based on the data we have observed. Consider the above question for the Deepwater Horizon Case Study. We are interested in finding evidence that the proportion experiencing adverse symptoms exceeds 0.20 (1 in 5). Therefore, we are interested in determining how likely this is to be true given the data we have observed. More, we might want to compare it against the likelihood that the proportion experiencing adverse symptoms does not exceed 0.20 (the opposite statement) given the data we have observed. There are therefore two opposing statements which could each define reality: the proportion experiencing adverse symptoms exceeds 0.20, or the proportion experiencing adverse symptoms does not exceed 0.20. Notice that both statements cannot be simultaneously true; each is a different statement about the parameter. We call these hypotheses. Definition 4.10 (Hypothesis) A statement (or theory) regarding a region for the parameter(s) of interest. Hypotheses are generally mutually exclusive. If there are only two competing hypotheses, the one we are more interested in establishing is typically called the alternative hypothesis (written \\(H_1\\) and read H-one), and its opposite is known as the null hypothesis (written \\(H_0\\) and read H-naught). For the Deepwater Horizon Case Study, we write: \\(H_0:\\) The proportion of volunteers assigned to clean wildlife following an oil spill who experience adverse respiratory symptoms is no more than 0.20. \\(H_1:\\) The proportion of volunteers assigned to clean wildlife following an oil spill who experience adverse respiratory symptoms exceeds 0.20. Each hypothesis is a well-posed statement (about a parameter characterizing the entire population), and the two statements are exactly opposite of one another meaning only one can be a true statement. For those who have prior exposure to statistical inference, notice that this framework vastly differs from the classical frequentist perspective. We are actually interested in the probability the hypothesis is true given the data observed! After writing down our hypotheses, we can collect data to determine which hypothesis is more likely given what we observe. Often these statements are written in a bit more of a mathematical structure in which a Greek letter is used to represent the parameter of interest. For example, we might write Let \\(\\theta\\) be the proportion of volunteers (assigned to clean wildlife following an oil spill) who experience adverse respiratory symptoms. \\(H_0: \\theta \\leq 0.20\\) \\(H_1: \\theta &gt; 0.20\\) In the above statements, \\(\\theta\\) represents the parameter of interest; the value 0.20 is known as the null value. Writing the hypotheses in this way connects them to probabilistic models underlying the distribution. Definition 4.11 (Null Value) The value associated with the equality component of the null hypothesis; it forms the threshold or boundary between the two hypothesis. Note: not all questions of interest require a null value be specified. This section has focused on developing the null and alternative hypothesis when our question of interest is best characterized as one of comparing models or evaluating a particular statement. If our goal is estimation, a null and alternative hypothesis are not applicable. For example, we might have the following goal: Estimate the proportion of volunteers (assigned to clean wildlife following an oil spill) who experience adverse respiratory symptoms. In this version of our research question there is no statement which needs to be evaluated. We are interested in estimation, not hypothesis testing and thus there is no corresponding null and alternative hypothesis. Process for Framing a Question In order to frame a research question, consider the following steps: Identify the population of interest. Identify the parameter(s) of interest. Determine if you are interested in estimating the parameter(s) or quantifying the evidence against some working theory. If you are interested in testing a working theory, make the null hypothesis the working theory and the alternative the exact opposite statement (what you want to provide evidence for). "],["Data.html", "5 Gathering the Evidence (Data Collection) 5.1 What Makes a Sample Reliable 5.2 Poor Methods of Data Collection 5.3 Preferred Methods of Sampling", " 5 Gathering the Evidence (Data Collection) Consider again the goal of statistical inference  to use a sample as a snapshot to say something about the underlying population (Figure 5.1). This generally provokes unease in people, leading to a distrust of statistical results. In this section we attack that distrust head on. Figure 5.1: Illustration of the statistical process (reprinted from Chapter 1). 5.1 What Makes a Sample Reliable If we are going to have some amount of faith in the statistical results we produce, we must have data in which we can place our trust. The Treachery of Images (Figure 5.2) is a canvas painting depicting a pipe, below which the artist wrote the French phrase This is not a pipe. Regarding the painting, the artist said The famous pipe. How people reproached me for it! And yet, could you stuff my pipe? No, its just a representation, is it not? So if I had written on my picture This is a pipe, Id have been lying! Figure 5.2: The Treachery of Images by Ren√© Magritte. Just as a painting is a representation of the object it depicts, so a sample should be a representation of the population under study. This is the primary requirement if we are to rely on the resulting data. In order for a statistical analysis to be reliable, the sample must be representative of the population under study. We need to be careful to not get carried away in our expectations. What constitutes representative really depends on the question, just as an artist chooses his depiction based on how he wants to represent the object. Lets consider the following example. Example 5.1 (School Debt) In addition to a degree, college graduates also tend to leave with a large amount of debt due to college loans. In 2012, a graduate with a student loan had an average debt of $29,400; for graduates from private non-profit institutions, the average debt was $32,3002. Suppose we are interested in determining the average amount of debt in student loans carried by a graduating senior from Rose-Hulman Institute of Technology, a small private non-profit engineering school. There are many faculty at Rose-Hulman who choose to send their children to the institute. Since I am also on the faculty, I know many of these individuals. Suppose I were to ask each to report the amount of student loans their children carried upon graduation from Rose-Hulman. I compile the 25 responses and compute the average amount of debt. Further, I report that based on this study, there is significant evidence that the average debt carried by a graduate of Rose-Hulman is far below the $32,300 reported above (great news for this years graduating class)! Why might we be hesitant to trust these results? Many objections to statistical results stem from a distrust of whether the data (the sample) is really representative of the population of interest. Rose-Hulman, like many other universities, has a policy that the children of faculty may attend their university (assuming admittance) tuition-free. We would therefore expect their children to carry much less debt than the typical graduating senior. There is a mismatch between the group we would like to study and the data we have collected. This example provides a nice backdrop for discussing what it means to be representative. First, lets define our population; in this case, we are interested in graduating seniors from Rose-Hulman. The variable of interest is the amount of debt carried in student loans; the parameter of interest is then the average amount of debt in student loans carried by graduating seniors of Rose-Hulman. However, the sample consists of only graduating seniors of Rose-Hulman who have a parent employeed by the university. With regard to the grade point average of the students in our sample, it is probably similar to all graduating seniors. The starting salary of the students in our sample is probably similar to all graduating seniors; the fraction of mechanical engineering majors versus math majors is probably similar. So, in many regards the sample is representative of the population; however, it fails to be representative with regard to the variable of interest. This is our concern. The amount of debt carried by students in our sample is not representative of that debt carried by all graduating seniors from the university. When thinking about whether a sample is representative, focus your attention to the characteristics specific to your research question. Does that mean the sample is useless? Yes and no. The sample collected cannot be used to answer our initial question of interest. No statistical method can fix bad data; statistics adheres to the garbage-in, garbage-out phenomena. If the data is bad, no analysis will undo that. However, while the sample cannot be used to answer our initial question, it could be used to address a different question: What is the average amount of debt in student loans carried by graduating seniors from Rose-Hulman whose parent is a faculty member at the university? For this revised question, the sample may indeed be representative. If we are working with previously collected data, we must consider the population to which our results will generalize. That is, for what population is the given sample representative? If we are collecting our data, we need to be sure we collect data in such a way that the data is representative of our target population. Lets first look at what not to do. 5.2 Poor Methods of Data Collection Example 5.1 is an example of a convenience sample, when the subjects in the sample are chosen simply due to ease of collection. Examples include surveying students only in your sorority when you are interested in all females who are part of a sorority on campus; taking soil samples from only your city when you are interested in the soil for the entire state; and, obtaining measurements from only one brand of phone, because it was the only one you could afford on your budget, when you are interested in studying all cell phones on the market. A convenience sample is unlikely to be representative if there is a relationship between the ease of collection and the variable under study. This was true in the School Debt example; the relationship of a student to a faculty member was directly related to the amount of debt they carried. As a result, the resulting sample was not representative of the population. When conducting a survey with human subjects, it is common to only illicit responses from volunteers. Such volunteer samples tend to draw in those with extreme opinions. Consider product ratings on Amazon. Individual ratings tend to cluster around 5s and 1s. This is because those customers who take time to submit a review (which is voluntary) tend to be those who are really thrilled with their product (and want to encourage others to purchase it) and those who are really disappointed with their purchase (and want to encourage others to avoid it). Such surveys often fail to capture those individuals in the population who have middle of the road opinions. We could not possibly name all the poor methods for collecting a sample; but, poor methods all share something in common  it is much more likely the resulting sample is not representative. Failing to be representative results in biased estimates of the parameter. Definition 5.1 (Bias) A set of measurements is said to be biased if they are consistently too high (or too low). Similarly, an estimate of a parameter is said to be biased if it is consistently too high (or too low). To illustrate the concept of bias, consider shooting at a target as in Figure 5.3. We can consider the center of our target to be the parameter we would like to estimate within the population. The values in our sample (the strikes on the target) will vary around the parameter; while we do not expect any one value to hit the target precisely, a representative sample is one in which the values tend to be clustered about the parameter (unbiased). When the sample is not representative, the values in the sample tend to cluster off the mark (biased). Notice that to be unbiased, it may be that not a single value in the sample is perfect, but aggregated together, they point in the right direction. So, bias is not about an individual measurement being an outlier, (more on those in a later chapter) but about repeatedly shooting in the wrong direction. Figure 5.3: Illustration of bias and variability. There is a difference between accuracy and precision. Generally, accuracy refers to location (and therefore bias); we say a process is accurate when it is unbiased. Precision refers to the variability. Biased results are typically due to poor sampling methods that result in a sample which is not representative of the target population. The catch (there is always a catch) is that we will never know if a sample is actually representative or not. We can, however, employ methods that help to minimize the chance that the sample is biased. 5.3 Preferred Methods of Sampling No method guarantees a perfectly representative sample; but, we can take measures to reduce or eliminate bias. A useful strategy is to employ randomization. If data is to be useful for making conclusions about the population, a process referred to as drawing inference, proper data collection is crucial. Randomization can play an important role ensuring a sample is representative and that inferential conclusions are appropriate. Consider the School Debt example again. Suppose instead of the strategy described there, we had done the following: We constructed a list of all graduating seniors from the university. We placed the name of each student on an index card; then, we thoroughly shuffled the cards and chose the top 25 cards. For these 25 individuals, we recorded the amount of debt in student loans each carried. This essentially describes using a lottery to select the sample. This popular method is known as taking a simple random sample. By conducting a lottery, we make it very unlikely that our sample consists of only students with a very small amount of student debt (as occurred when we used a convenience sample). Definition 5.2 (Simple Random Sample) Often abbreviated SRS, this is a sample of size \\(n\\) such that every collection of size \\(n\\) is equally likely to be the resulting sample. This is equivalent to a lottery. There are situations in which a simple random sample does not suffice. Again, consider our School Debt example. The Rose-Hulman student body is predominantly domestic, with only about 3% of the student body being international students. But, suppose we are interested in comparing the average debt carried between international and domestic students. It is very likely, by chance alone, that in a simple random sample of 25 students none will be international. Instead of a simple random sample, we might consider taking a sample of 13 domestic students and a sample of 12 international students; this is an example of a stratified random sample. This approach is useful when there is a natural grouping of interest within the population. Definition 5.3 (Stratified Random Sample) A sample in which the population is first divided into groups, or strata, based on a characteristic of interest; a simple random sample is then taken within each group. There are countless sampling techniques used in practice. The two described above can be very useful starting points for developing a custom method suitable for a particular application. Their benefit stems from their use of randomization. This section is entitled Preferred Methods because while these methods are ideal, they are not always practical. Consider the Deepwater Horizon Case Study; conceptually, we can take a simple random sample of the volunteers for our study. However, as with any study involving human subjects, researchers would be required to obtain consent from each subject in the study. That is, a volunteer has the right to refuse to participate in the study. Therefore, it is unlikely that a simple random sample as described above could be obtained. Again, the key is to obtain a representative sample; while random selection may be a nice tool for accomplishing this, we may need to appeal to the composition of the sample itself to justify its use. Based on the characteristics of those willing to participate in the study, do we feel the study participants form a representative group of all volunteers? That is the essential question. This is often why studies report a table summarizing subject demographics such as age, gender, etc. It is also why it is extremely important for researchers to describe how subjects were selected so that readers may make the judgement for themselves whether the sample is representative. http://ticas.org/sites/default/files/pub_files/Debt_Facts_and_Sources.pdf "],["Summaries.html", "6 Presenting the Evidence (Summarizing Data) 6.1 Characteristics of a Distribution (Summarizing a Single Variable) 6.2 Summarizing Relationships", " 6 Presenting the Evidence (Summarizing Data) If you open any search engine and look up data visualization, you will quickly be overwhelmed by a host of pages, texts, and software filled with tools for summarizing your data. Here is the bottom line: a good visualization is one that helps you answer your question of interest. It is both that simple and that complicated. The use of data for decision making requires that the data be summarized and presented in ways that address the question of interest. Whether simple or complex, all graphical and numerical summaries should help turn the data into usable information. Pretty pictures for the sake of pretty pictures are not helpful. In this section, we will consider various simple graphical and numerical summaries to help build a case for addressing the question of interest. 6.1 Characteristics of a Distribution (Summarizing a Single Variable) Remember that because of variability, the key to asking good questions is to not ask questions about individual values but to characterize the underlying distribution (see Definition 4.3). Therefore, characterizing the underlying distribution is also the key to a good visualization or numeric summary. For the Deepwater Horizon Case Study, the response (whether a volunteer experienced adverse respiratory symptoms) is categorical. As we stated previously, summarizing the distribution of a categorical variable reduces to showing how individual subjects fall into the various groups. Figure 6.1 displays a bar chart summarizing the rate of respiratory symptoms for volunteers cleaning wildlife. Figure 6.1: Frequency of adverse respiratory symptoms for volunteers cleaning wildlife following the Deepwater Horizon oil spill. In general, it does not matter whether the frequency or the relative frequencies are reported; however, if the relative frequencies are plotted, some indication of the sample size should be provided with the figure either as an annotation or within the caption. From the above graphic, we see that nearly 28% of volunteers assigned to wildlife experienced adverse respiratory symptoms; the graphic helps address our question, even if not definitively. When you are summarizing only categorical variables, a bar chart is sufficient. Statisticians tend to agree that bar charts are preferable to pie charts (see this whitepaper and this blog for further explanation). While a single type of graphic (bar charts) are helpful for looking at categorical data, summarizing the distribution of a numeric variable requires a bit more thought. Consider the following example. Example 6.1 (Paper Strength) While electronic records have become the predominant means of storing information, we do not yet live in a paperless society. Paper products are still used in a variety of applications ranging from printing reports and photography to packaging and bathroom tissue. In manufacturing paper for a particular application, the strength of the resulting paper product is a key characteristic. There are several metrics for the strength of paper. A conventional metric for assessing the inherent (not dependent upon the physical characteristics, such as the weight of the paper, which might have an effect) strength of paper is the breaking length. This is the length of a paper strip, if suspended vertically from one end, that would break under its own weight. Typically reported in kilometers, the breaking length is computed from other common measurements. For more information on paper strength measurements and standards, see the following website: http://www.paperonweb.com A study was conducted at the University of Toronto to investigate the relationship between pulp fiber properties and the resulting paper properties (Lee 1992). The breaking length was obtained for each of the 62 paper specimens, the first 5 measurements of which are shown in Table 6.1. The complete dataset is available online at the following website: https://vincentarelbundock.github.io/Rdatasets/doc/robustbase/pulpfiber.html While there are several questions one might ask with the available data, here we are primarily interested in characterizing the breaking length of these paper specimens. Table 6.1: Breaking length (km) for first 5 specimens in the Paper Strength study. Specimen Breaking Length 1 21.312 2 21.206 3 20.709 4 19.542 5 20.449 Figure 6.2 presents the breaking length for all 62 paper specimens in the sample through a dot plot in which the breaking length for each observed specimen is represented on a number line using a single dot. Figure 6.2: Breaking Length (km) for 62 paper specimens. With any graphic, we tend to be drawn to three components: where the values tend to be, how tightly the values tend to be clustered there, and the way the values tend to cluster. Notice that about half of the paper specimens in the sample had a breaking length longer than 21.26 km. Only about 25% of paper specimens had a breaking length less than 19.33 km. These are measures of location. In particular, these are known as percentiles, of which the median, first quartile and third quartile are commonly used examples. Definition 6.1 (Percentile) The \\(k\\)-th percentile is the value \\(q\\) such that \\(k\\)% of the values in the distribution are less than or equal to \\(q\\). For example, 25% of values in a distribution are less than or equal to the 25-th percentile (known as the first quartile and denoted \\(Q_1\\)). 50% of values in a distribution are less than or equal to the 50-th percentile (known as the median). 75% of values in a distribution are less than or equal to the 75-th percentile (known as the third quartile and denoted \\(Q_3\\)). The average is also a common measure of location. The breaking length of a paper specimen is 21.72 km, on average. In this case, the average breaking length and median breaking length are very close; this need not be the case. The average is not describing the center of the data in the same way as the median; they capture different properties. Definition 6.2 (Average) Also known as the mean, this measure of location represents the balance point for the distribution. It is denoted by \\(\\bar{x}\\). For a sample of size \\(n\\), it is computed by \\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\] where \\(x_i\\) rerpesents the \\(i\\)-th value in the sample. When referencing the average for a population, the mean is also called the Expected Value, and is often denoted by \\(\\mu\\). Clearly, the breaking length is not equivalent for all paper specimens; that is, there is variability in the measurements. Measures of spread quantify the variability of values within a distribution. Common examples include the standard deviation (related to variance) and interquartile range. For the Paper Strength example, the breaking length varies with a standard deviation of 2.88 km; the interquartile range for the breaking length is 5.2 km. The standard deviation is often reported more often than the variance since it is on the same scale as the original data; however, as we will see later, the variance is useful from a mathematical perspective for derivations. Neither of these values has a natural interpretation; instead, larger values of these measures simply indicate a higher degree of variability in the data. Definition 6.3 (Variance) A measure of spread, this roughly captures the average distance values in the distribution are from the mean. For a sample of size \\(n\\), it is computed by \\[s^2 = \\frac{1}{n-1}\\sum_{i=1}^{n} \\left(x_i - \\bar{x}\\right)^2\\] where \\(\\bar{x}\\) is the sample mean and \\(x_i\\) is the \\(i\\)-th value in the sample. The division by \\(n-1\\) instead of \\(n\\) reduces the bias in the statistic. The symbol \\(\\sigma^2\\) is often used to denote the variance in the population. Definition 6.4 (Standard Deviation) A measure of spread, this is the square root of the variance. Definition 6.5 (Interquartile Range) The distance between the first and third quartiles. This measure of spread indicates the range over which the middle 50% of the data is spread. The measures we have discussed so far are illustrated in Figure 6.3. While some authors suggest the summaries you choose to report depends on the shape of the distribution, we argue that it is best to report the values that align with the question of interest. It is the question that should be shaped by the beliefs about the underlying distribution. Figure 6.3: Illustration of measures of location and spread for a distribution of values. Finally, consider the shape of the distribution of breaking length we have observed. The breaking length tends to be clustered in two locations; we call this bimodal (each mode is a hump in the distribution). Other terms used to describe the shape of a distribution are symmetric and skewed. Symmetry refers to cutting a distribution in half (at the median) and the lower half being a mirror image of the upper half; skewed distributions are those which are not symmetric. Observe then that the dot plot above gives us some idea of the location, spread, and shape of the distribution, in a way that the table of values could not. This makes it a useful graphic as it is characterizing the distribution of the sample we have observed. This is one of the four components of what we call the Distributional Quartet. Definition 6.6 (Distribution of the Sample) The pattern of variability in the observed values of a variable. When the sample is not large, a dot plot is reasonable. Other common visualizations for a single variable include: jitter plot: similar to a dot plot, each value observed is represented by a dot; the dots are jittered (shifted randomly) in order to avoid overplotting when many subjects share the same value of the response. box plot: a visual depiction of five key percentiles; the plot includes the minimum, first quartile, median, third quartile, and maximum value observed. The quartiles are connected with a box, the median cuts the box into two components. histogram: can be thought of as a grouped dot plot in which subjects are binned into groups of similar values. The height of each bin represents the number of subjects falling into that bin. density plot: a smoothed histogram in which the y-axis has been standardized so that the area under the curve has value 1. The y-axis is not interpretable directly, but higher values simply mean more likely to occur. To illustrate these graphics, the breaking length for the Paper Strength example is summarized using various methods in Figure 6.4. The latter three visualizations are more helpful when the dataset is very large and plotting the raw values actually hides the distribution. There is no right or wrong graphic; it is about choosing the graphic which addresses the question and adequately portrays the distribution. Figure 6.4: Four graphical summaries of the breaking length for the Paper Strength example. The numeric summaries of a distribution are known as statistics. While parameters characterize a variable at the population level, statistics characterize a variable at the sample level. Definition 6.7 (Statistic) Numeric quantity which summarizes the distribution of a variable within a sample. Why would we compute numerical summaries in the sample if we are interested in the population? Remember the goal of this discipline is to use the sample to say something about the underlying population. As long as the sample is representative, the distribution of the sample should reflect the distribution of the population; therefore, summaries of the sample should be close to the analogous summaries of the population (statistics estimate their corresponding parameters). Now we see the real importance of having a representative sample; it allows us to say that what we observe in the sample is a good proxy for what is happening in the population. Definition 6.8 (Distribution of the Population) The pattern of variability in values of a variable at the population level. Generally, this is impossible to know, but we might model it. That is, the mean in the sample should approximate (estimate) the mean in the population; the standard deviation of the sample should estimate the standard deviation in the population; and, the shape of the sample should approximate the shape of the population, etc. The sample is acting as a representation in all possible ways of the population. A representative sample reflects the population; therefore, we can use statistics as estimates of the population parameters. We would never use \\(\\bar{x}\\) to represent a parameter like the mean of the population. The symbol \\(\\bar{x}\\) (or \\(\\bar{y}\\), etc.) represents observed values being averaged together. Since the values are observed, we must be talking about the sample, and therefore \\(\\bar{x}\\) represents a statistic. A similar statement could be made for \\(s^2\\) (sample variance) compared to \\(\\sigma^2\\) (often used to represent population variance, though it can represent other things). In reality, the symbols themselves are not important. The importance is on their representation. Statistics are observed while parameters are not. 6.2 Summarizing Relationships The summaries discussed above are nice for examining a single variable. In general, research questions of interest typically involve the relationship between two or more variables. Most graphics are two-dimensional (though 3-dimensional graphics and even virtual reality are being utilized now); therefore, summarizing a rich set of relationships may require the use of both axes as well as color, shape, size, and even multiple plots in order to tell the right story. We will explore these various features in upcoming units of the text. Here, we focus on the need to tell a story that answers the question of interest instead of getting lost in making a graphic. Consider the following question from the Deepwater Horizon Case Study: What is the increased risk of developing adverse respiratory symptoms for volunteers cleaning wildlife compared to those volunteers who do not have direct exposure to oil? Consider the graphic in Figure 6.5; this is not a useful graphic. While it compares the number of volunteers with symptoms in each group, we cannot adequately address the question because the research question involves comparing the rates for the two groups. Figure 6.5: Illustration of a poor graphic; the graphic does not give us a sense of the rate within each group in order to make a comparison. Instead, Figure 6.6 compares the rates within each group. Notice that since we are reporting relative frequencies, we also report the sample size for each group. Figure 6.6: Comparison of the rate of adverse respiratory symptoms among volunteers assigned to different tasks. From the graphic, it becomes clear that within the sample a higher fraction of volunteers cleaning wildlife experienced adverse symptoms compared with those without oil exposure. In fact, volunteers cleaning wildlife were 1.79 times more likely to experience adverse respiratory symptoms. The key to a good summary is understanding the question of interest and addressing this question through a useful characterization of the variability. References "],["additional-study-design.html", "7 Additional Study Design 7.1 Two Types of Studies", " 7 Additional Study Design 7.1 Two Types of Studies Thinking about how the data was collected helps us determine how the results generalize beyond the sample itself (to what population the results apply). When our question of interest is about the relationship between two variables (as most questions are), we must also carefully consider the study design. Too often separated from the statistical analysis that follows, keeping the study design in mind should guide the analysis as well as inform us about the conclusions we can draw. In order to illustrate how study design can impact the results, consider the following example. Example 7.1 (Kangaroo Care) At birth, infants have low levels of Vitamin K, a vitamin needed in order to form blood clots. Though rare, without the ability for her blood to clot, an infant could develop a serious bleed. In order to prevent this, the American Academy of Pediatrics recommends that all infants be given a Vitamin K shot shortly after birth in order to raise Vitamin K levels. As with any shot, there is typically discomfort to the infant, which can be very discomforting to new parents. Kangaroo Care is a method of holding a baby which emphasizes skin-to-skin contact. The child, who is dressed only in a diaper, is placed upright on the parents bare chest; a light blanket is draped over the child. Suppose we are interested in determining if utilizing the method while giving the child a Vitamin K shot reduces the discomfort in the infant, as measured by the total amount of time the child cries following the shot. Contrast the following two potential study designs: We allow the attending nurse to determine whether Kangaroo Care is initiated prior to giving the Vitamin K shot. Following the shot, we record the total time (in seconds) the child cries. We flip a coin. If it comes up heads, the nurse should have the parents implement Kangaroo Care prior to giving the Vitamin K shot; if it comes up tails, the nurse should give the Vitamin K shot without implementing Kagaroo Care. Following the shot, we record the total time (in seconds) the child cries. Note, in both study designs (A) and (B), we only consider term births which have no complications to avoid situations that might alter the timing of the Vitamin K shot or the ability to implement Kangaroo Care. Note that there are some similarities in the two study designs: The underlying population is the same for both designs  infants born at term with no complications. There are two groups being compared in both designs  the Kangaroo Care group and the no Kangaroo Care group. The response (variable of interest) is the same in both designs  the time (in seconds) the infant cries. There is action taken by the researcher in both designs  a Vitamin K shot is given to the child. There is one prominent difference between the two study designs: For design (A), the choice of Kangaroo Care is left up to the nurse (self-selected); for design (B), the choice of Kangaroo is assigned to the nurse by the researcher, and this selection is made at random. Design (A) is an example of an observational study; design (B) is a controlled experiment. Definition 7.1 (Observational Study) A study in which each subject self-selects into one of groups being compared in the study. The phrase self-selects is used very loosely here and can include studies in which the groups are defined by an inherent characteristic or the groups are chosen haphazardly. Definition 7.2 (Controlled Experiment) A study in which each subject is randomly assigned to one of the groups being compared in the study. It is common to think that anytime the environment is controlled by the researcher that a controlled experiment is taking place, but the defining characteristic is the random assignment to groups (sometimes referred to as the factor under study or treatment groups). In the example above, both study designs involved a controlled setting (the delivery room of a hospital) in which trained staff (the nurse) deliver the shot. However, only design (B) is a controlled experiment because the researchers randomly determined into which group the infant would be placed. To understand the impact of random allocation, suppose that we had conducted a study using design (A); further, the results suggest that those infants who were given a shot while using Kangaroo Care cried for a shorter time period, on average. Can we conclude that it was the Kangaroo Care that led to the shorter crying time? Maybe. Consider the following two potential explanations for the resulting data: Kangaroo Care is very effective; as a result, those children who were given Kangaroo Care had reduced crying time, on average, following the Vitamin K shot. It turns out that those nurses who chose to implement Kangaroo Care (remember, they have a choice under design (A) whether they implement the method) were also the nurses with a gentler bedside manner. Therefore, these nurses tended to be very gentle when giving the Vitamin K shot whereas the nurses who chose not to implement Kangaroo Care tended to just jab the needle in when giving the shot. As a result, the reduced crying time is not a result of the Kangaroo Care but the manner in which the shot was given. The problem is that we are unable to determine which of the explanations is correct. Given the data we have collected, we are unable to tease out the effect of the Kangaroo Care from that of the nurses bedside manner. As a result, we are able to say we observed an association between the use of Kangaroo Care and reduced crying time, but we are unable to conclude that Kangaroo Care caused a reduction in the crying time (that is, there may not be a relationship between the two variables). In this hypothetical scenario, the nurses bedside manner is called a confounder. Definition 7.3 (Confounding) When the effect of a variable on the response is mis-represented due to the presence of a third, potentially unobserved, variable known as a confounder. Confounders can mask the relationship between the factor under study and the response. There is a documented association between ice cream sales and the risk of shark attacks. As ice cream sales increase, the risk of a shark attack also tends to increase. This does not mean that if a small city in the Midwest increases its ice cream sales that the citizens are at higher risk of being attacked by a shark. As Figure 7.1 illustrates, there is a confounder  temperature. As the temperatures increase, people tend to buy more ice cream; as the temperature increases, people tend to go to the beach increasing the risk of a shark attack. Two variables can appear to be related as a result of a confounder. Figure 7.1: Illustration of a confounding variable. The confounder, related to both the factor and the treatment can make it appear as though there is a causal relationship when none exists. Confounders are variables that influence both the factor of interest and the response. Observational studies are subject to confounding; thus, controlled experiments are often considered the gold standard in research because they allow us to infer cause-and-effect relationships from the data. Why does the random allocation make such an impact? Because it removes the impact of confounders. Lets return to the hypothetical Vitamin-K study. Suppose there are nurses with a gentle bedside manner and those who are a little less gentle. If the infants are randomly assigned to one of the two treatment groups, then for every gentle nurse who is told to implement Kangaroo Care while giving the shot, there tends to be a gentle nurse who is told to not implement Kangaroo Care. Similarly, for every mean nurse who is told to implement Kangaroo Care while giving a shot, there tends to be a mean nurse who is told to not implement Kangaroo Care. This is illustrated in Figure 7.2. For an observational study, the treatment groups can be unbalanced; for example, the figure illustrates a case in which there is a higher fraction (11/12 compared to 1/4) of friendly nurses in the Kangaroo Care group compared to the No Kangaroo Care group. For the controlled experiment however, the treatment groups tend to be balanced; there is approximately the same fraction of friendly nurses in both groups. Random assignment is the great equalizer. It tends to result in groups which are similar in all respects; therefore, any differences we observe between the groups must be due to the grouping and not an underlying confounding variable. Figure 7.2: Illustration of the impact of random assignment in study design. For the observational study, the treatment groups are unbalanced. For the controlled experiment, the treatment groups are balanced. Randomly assigning subjects to groups balances the groups with respect to any confounders; that is, the groups being compared are similar. Therefore, any differences between the two groups can be attributed to the grouping factor itself, leading to cause-and-effect conclusions. While controlled experiments are a fantastic study design, we should not discount the use of observational studies. Consider the Deepwater Horizon Case Study; suppose we are interested in the following question: Is there evidence that volunteers who are directly exposed to oil have an increased risk of developing adverse respiratory symptoms compared to those who are not directly exposed to oil? The response is whether a volunteer develops adverse respiratory symptoms; the factor of interest is whether the volunteer has direct exposure to oil. We could conduct a controlled experiment by randomly determining which volunteers are assigned to wildlife clean up and which are assigned to administrative tasks, for example. However, it may be that volunteer tasks need to be determined by skillset or by greatest need at the time the person volunteers. It may not be feasible to randomly assign volunteers to specific positions. Or, it could be that the data was obtained after the fact; that is, the data is not the result of a planned study in which case random assignment is not possible because volunteers self-selected into positions in the past. If random assignment is not possible, it does not mean the data is useless. But, it does mean we will need to be sure we address the potential confounding when performing the analysis and discussing the results. The big idea is that in order to make causal conclusions, we must be able to state that the groups being compared are balanced with respect to any potential confounders; random assignment is one technique for accomplishing this. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
