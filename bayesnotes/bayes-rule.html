<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Bayes Rule | Course Notes for Bayesian Data Analysis</title>
  <meta name="description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Bayes Rule | Course Notes for Bayesian Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Bayes Rule | Course Notes for Bayesian Data Analysis" />
  
  <meta name="twitter:description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  

<meta name="author" content="Eric M Reyes" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="Summaries.html"/>
<link rel="next" href="modeling-samples.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="essential-probability.html"><a href="essential-probability.html"><i class="fa fa-check"></i><b>1</b> Essential Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="essential-probability.html"><a href="essential-probability.html#density-functions-as-models"><i class="fa fa-check"></i><b>1.1</b> Density Functions as Models</a></li>
<li class="chapter" data-level="1.2" data-path="essential-probability.html"><a href="essential-probability.html#summarizing-distributions-parameters"><i class="fa fa-check"></i><b>1.2</b> Summarizing Distributions (Parameters)</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="essential-probability.html"><a href="essential-probability.html#kernels"><i class="fa fa-check"></i><b>1.2.1</b> Kernels</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="essential-probability.html"><a href="essential-probability.html#transformations-of-random-variables"><i class="fa fa-check"></i><b>1.3</b> Transformations of Random Variables</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="essential-probability.html"><a href="essential-probability.html#expectations-of-functions"><i class="fa fa-check"></i><b>1.3.1</b> Expectations of Functions</a></li>
<li class="chapter" data-level="1.3.2" data-path="essential-probability.html"><a href="essential-probability.html#moment-generating-functions"><i class="fa fa-check"></i><b>1.3.2</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="essential-probability.html"><a href="essential-probability.html#independent-random-variables"><i class="fa fa-check"></i><b>1.4</b> Independent Random Variables</a></li>
</ul></li>
<li class="part"><span><b>I Language and Logic of Inference</b></span></li>
<li class="chapter" data-level="2" data-path="CaseDeepwater.html"><a href="CaseDeepwater.html"><i class="fa fa-check"></i><b>2</b> Case Study: Health Effects of the Deepwater Horizon Oil Spill</a></li>
<li class="chapter" data-level="3" data-path="Basics.html"><a href="Basics.html"><i class="fa fa-check"></i><b>3</b> The Statistical Process</a>
<ul>
<li class="chapter" data-level="3.1" data-path="Basics.html"><a href="Basics.html#overview-of-drawing-inference"><i class="fa fa-check"></i><b>3.1</b> Overview of Drawing Inference</a></li>
<li class="chapter" data-level="3.2" data-path="Basics.html"><a href="Basics.html#anatomy-of-a-dataset"><i class="fa fa-check"></i><b>3.2</b> Anatomy of a Dataset</a></li>
<li class="chapter" data-level="3.3" data-path="Basics.html"><a href="Basics.html#a-note-on-codebooks"><i class="fa fa-check"></i><b>3.3</b> A Note on Codebooks</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Questions.html"><a href="Questions.html"><i class="fa fa-check"></i><b>4</b> Asking the Right Questions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="Questions.html"><a href="Questions.html#characterizing-a-variable"><i class="fa fa-check"></i><b>4.1</b> Characterizing a Variable</a></li>
<li class="chapter" data-level="4.2" data-path="Questions.html"><a href="Questions.html#framing-the-question"><i class="fa fa-check"></i><b>4.2</b> Framing the Question</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Data.html"><a href="Data.html"><i class="fa fa-check"></i><b>5</b> Gathering the Evidence (Data Collection)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="Data.html"><a href="Data.html#what-makes-a-sample-reliable"><i class="fa fa-check"></i><b>5.1</b> What Makes a Sample Reliable</a></li>
<li class="chapter" data-level="5.2" data-path="Data.html"><a href="Data.html#poor-methods-of-data-collection"><i class="fa fa-check"></i><b>5.2</b> Poor Methods of Data Collection</a></li>
<li class="chapter" data-level="5.3" data-path="Data.html"><a href="Data.html#preferred-methods-of-sampling"><i class="fa fa-check"></i><b>5.3</b> Preferred Methods of Sampling</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Summaries.html"><a href="Summaries.html"><i class="fa fa-check"></i><b>6</b> Presenting the Evidence (Summarizing Data)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="Summaries.html"><a href="Summaries.html#characteristics-of-a-distribution-summarizing-a-single-variable"><i class="fa fa-check"></i><b>6.1</b> Characteristics of a Distribution (Summarizing a Single Variable)</a></li>
<li class="chapter" data-level="6.2" data-path="Summaries.html"><a href="Summaries.html#summarizing-relationships"><i class="fa fa-check"></i><b>6.2</b> Summarizing Relationships</a></li>
</ul></li>
<li class="part"><span><b>II Fundamentals of Bayesian Inference</b></span></li>
<li class="chapter" data-level="7" data-path="bayes-rule.html"><a href="bayes-rule.html"><i class="fa fa-check"></i><b>7</b> Bayes Rule</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bayes-rule.html"><a href="bayes-rule.html#tenants-of-the-bayesian-approach-to-inference"><i class="fa fa-check"></i><b>7.1</b> Tenants of the Bayesian Approach to Inference</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="modeling-samples.html"><a href="modeling-samples.html"><i class="fa fa-check"></i><b>8</b> Modeling Samples</a>
<ul>
<li class="chapter" data-level="8.1" data-path="modeling-samples.html"><a href="modeling-samples.html#independence"><i class="fa fa-check"></i><b>8.1</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="quantifying-prior-information.html"><a href="quantifying-prior-information.html"><i class="fa fa-check"></i><b>9</b> Quantifying Prior Information</a></li>
<li class="chapter" data-level="10" data-path="posterior-distributions.html"><a href="posterior-distributions.html"><i class="fa fa-check"></i><b>10</b> Updating Prior Beliefs (Posterior Distributions)</a></li>
<li class="chapter" data-level="11" data-path="point-estimation.html"><a href="point-estimation.html"><i class="fa fa-check"></i><b>11</b> Point Estimation</a></li>
<li class="chapter" data-level="12" data-path="interval-estimation.html"><a href="interval-estimation.html"><i class="fa fa-check"></i><b>12</b> Interval Estimation</a></li>
<li class="chapter" data-level="13" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>13</b> Prediction</a>
<ul>
<li class="chapter" data-level="13.1" data-path="prediction.html"><a href="prediction.html#derivation-of-the-posterior-predictive"><i class="fa fa-check"></i><b>13.1</b> Derivation of the Posterior Predictive</a></li>
<li class="chapter" data-level="13.2" data-path="prediction.html"><a href="prediction.html#summary"><i class="fa fa-check"></i><b>13.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>14</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="14.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#point-null-hypotheses"><i class="fa fa-check"></i><b>14.1</b> Point-Null Hypotheses</a></li>
<li class="chapter" data-level="14.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#model-comparison"><i class="fa fa-check"></i><b>14.2</b> Model Comparison</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="constructing-priors.html"><a href="constructing-priors.html"><i class="fa fa-check"></i><b>15</b> Constructing Prior Distributions</a>
<ul>
<li class="chapter" data-level="15.1" data-path="constructing-priors.html"><a href="constructing-priors.html#elicitation-from-experts"><i class="fa fa-check"></i><b>15.1</b> Elicitation from Experts</a></li>
<li class="chapter" data-level="15.2" data-path="constructing-priors.html"><a href="constructing-priors.html#mixtures"><i class="fa fa-check"></i><b>15.2</b> Mixtures</a></li>
<li class="chapter" data-level="15.3" data-path="constructing-priors.html"><a href="constructing-priors.html#chains"><i class="fa fa-check"></i><b>15.3</b> Chains</a></li>
<li class="chapter" data-level="15.4" data-path="constructing-priors.html"><a href="constructing-priors.html#non-informative-priors"><i class="fa fa-check"></i><b>15.4</b> Non-Informative Priors</a></li>
</ul></li>
<li class="part"><span><b>III Numerical Approaches to Bayesian Computations</b></span></li>
<li class="chapter" data-level="16" data-path="mc-integration.html"><a href="mc-integration.html"><i class="fa fa-check"></i><b>16</b> Monte Carlo Integration</a></li>
<li class="chapter" data-level="17" data-path="mcmc.html"><a href="mcmc.html"><i class="fa fa-check"></i><b>17</b> Markov Chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="17.1" data-path="mcmc.html"><a href="mcmc.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>17.1</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="mcmc-assessment.html"><a href="mcmc-assessment.html"><i class="fa fa-check"></i><b>18</b> Assessing MCMC Samples</a></li>
<li class="part"><span><b>IV Hierarchical Models Comparing Groups</b></span></li>
<li class="chapter" data-level="19" data-path="study-design.html"><a href="study-design.html"><i class="fa fa-check"></i><b>19</b> Elements of Good Study Design</a></li>
<li class="chapter" data-level="20" data-path="independent-groups.html"><a href="independent-groups.html"><i class="fa fa-check"></i><b>20</b> Models for Comparing Independent Groups</a>
<ul>
<li class="chapter" data-level="20.1" data-path="independent-groups.html"><a href="independent-groups.html#bridge-sampling"><i class="fa fa-check"></i><b>20.1</b> Bridge Sampling</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="dependent-groups.html"><a href="dependent-groups.html"><i class="fa fa-check"></i><b>21</b> Comparing Related Groups</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Course Notes for Bayesian Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayes-rule" class="section level1" number="7">
<h1><span class="header-section-number">7</span> Bayes Rule</h1>
<p>Statistical inference is the process of using a sample to make a statement about the underlying population or data generating process. There are two large paradigms in the statistical community for defining the framework under which inference occurs. This unit introduces the fundamental components of the Bayesian paradigm. This builds on a simple result, Bayes’ Rule, which has much greater potential than we might see at first glance.</p>
<p>In a probability course, Bayes’ Rule is often presented as a neat trick for solving a particular type of problem involving two events. While this simple class of problems understates its true potential, it does serve as a way of highlighting the key idea behind the method.</p>
<div class="example">
<p><span id="exm:ex-disease" class="example"><strong>Example 7.1  (Disease Testing) </strong></span>An enzyme-linked immunosorent assay (ELISA) test is performed to determine if the human immunodeficiency virus (HIV) is present in the blood of individuals. Suppose that the ELISA test correctly indicates HIV 99% of the time, and it correctly indicates being HIV-free in 99.5% of cases. Finally, suppose that the prevalence of HIV among blood donors is known to be 1/10000. What is the probability an individual who testes positive is actually infected with HIV?</p>
</div>
<p>In this example, we are interested in the probability of an individual being infected with HIV <em>given</em> their test is positive. Recalling the definition of conditional probability, we consider</p>
<p><span class="math display">\[Pr(\text{Infected with HIV} \mid \text{Tests Positive}) = \frac{Pr(\text{Infected with HIV} \cap \text{Tests Positive})}{Pr(\text{Tests Positive})};\]</span></p>
<p>however, these probabilities are not provided in the problem. What we actually have are the probability of testing positive given the patient is infected with HIV (0.99), the probability of testing negative given the patient is not infected with HIV (0.95), and the probability of having HIV (1/10000). This is the power of Bayes’ Rule — it allows you to address problems by reversing the conditioning. That is, we can make a statement about the likelihood of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span> using information about the likelihood of <span class="math inline">\(B\)</span> given <span class="math inline">\(A\)</span>! As we state Bayes Rule, we keep in mind that it is not the rule itself which is innovative but (a) what the rule implies and (b) how we apply it to solve problems in statistical inference, that make it valuable.</p>
<div class="theorem">
<p><span id="thm:thm-simple-bayes-rule" class="theorem"><strong>Theorem 7.1  (Bayes Theorem, Two Events) </strong></span>Given events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> such that <span class="math inline">\(Pr(A), Pr(B) \neq 0\)</span>, then we have that</p>
<p><span class="math display">\[Pr(A \mid B) = \frac{Pr(B \mid A)Pr(A)}{Pr(B \mid A)Pr(A) + Pr(B \mid A^\mathsf{c}) Pr(A^\mathsf{c})}\]</span></p>
</div>
<p>Bayes’ Rule is actually a convenient wrapper for an application of several other basic probability results combined:</p>
<ul>
<li>The numerator is an application of the definition of conditional probability; we can always write a joint probability (“and statement”) as the product of a conditional probability and a marginal probability.</li>
<li>The denominator is the result of the total probability rule; a marginal probability is computed by summing over mutually exclusive joint probabilities, which again are rewritten similarly to the numerator.</li>
</ul>
<div class="rmdkeyidea">
<p>Bayes’ Rule says that our information about <span class="math inline">\(A\)</span>, <em>after</em> observing <span class="math inline">\(B\)</span>, can be stated in terms of what we know about <span class="math inline">\(B\)</span> after observing <span class="math inline">\(A\)</span> and our belief about <span class="math inline">\(A\)</span> <em>prior</em> to seeing <span class="math inline">\(B\)</span>.</p>
</div>
<p>The above is useful when talking about two events, but the majority of applications address random variables, not specific events. That is, we are interested in characterizing entire distributions, not just probabilities for specific events. Following the same logic as above, we are able to extend (from a total probability rule and from the definition of conditional probability) the above result to two random variables.</p>
<div class="theorem">
<p><span id="thm:thm-rv-bayes-rule" class="theorem"><strong>Theorem 7.2  (Bayes Theorem, Two Random Variables) </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables; then, we have that</p>
<p><span class="math display">\[f_{Y \mid X}(y \mid x) = \frac{f_{X \mid Y}(x \mid y) f_Y(y)}{\int_{\mathcal{S}_Y} f_{X \mid Y}(x \mid y) f_Y(y) dy},\]</span></p>
<p>where the integration is replaced by summation when necessary to account for a discrete random variable.</p>
</div>
<div class="rmdtip">
<p>We will not distinguish between continuous and discrete random variables. For compactness, all results are presented assuming continuous random variables. When necessary, replace integration with summation (as summation is really just integration with respect to a specific measure).</p>
</div>
<p>As stated above, this result is really the application of basic definitions covered in a probability course. But, they are worth revisiting.</p>
<div class="definition">
<p><span id="def:defn-conditional-density" class="definition"><strong>Definition 7.1  (Conditional Density) </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables; the conditional density of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span> is given by</p>
<p><span class="math display">\[f_{X \mid Y}(y \mid x) = \frac{f_{X, Y}(x, y)}{f_Y(y)}\]</span></p>
</div>
<div class="rmdtip">
<p>Subscripts are to denote which random variable is being discussed; so, <span class="math inline">\(f_X(x)\)</span> refers to the density function of the random variable <span class="math inline">\(X\)</span> evaluated at <span class="math inline">\(x\)</span>. We suppress the subscripts when they are unnecessary.</p>
</div>
<p>Rearranging the terms in Definition <a href="bayes-rule.html#def:defn-conditional-density">7.1</a>, we are able to see that any joint density function <span class="math inline">\(f_{X, Y}(x, y)\)</span> can be written as the product of a conditional density and a marginal density. Similarly, we can write any marginal density by integrating over a joint density.</p>
<div class="lemma">
<p><span id="lem:thm-total-probability-rule" class="lemma"><strong>Lemma 7.1  (Total Probability Rule for Random Variables) </strong></span>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables with joint density <span class="math inline">\(f_{X,Y}(x,y)\)</span>; then, the marginal density of <span class="math inline">\(X\)</span> is given by</p>
<p><span class="math display">\[f_X(x) = \int_{\mathcal{S}_Y} f_{X,Y}(x, y) dy,\]</span></p>
<p>where <span class="math inline">\(\mathcal{S}_Y\)</span> is the support of <span class="math inline">\(Y\)</span>.</p>
</div>
<p>To add a little more intuition to this result, let <span class="math inline">\(X\)</span> represent the grade in this course, and suppose we are interested in the event that <span class="math inline">\(X\)</span> takes the value “A.” Well, this course is most certainly impacted by your other courses; so, let <span class="math inline">\(Y\)</span> take the value of the grade in the hardest class remaining on your schedule. Then, there are only a certain number of options:</p>
<ul>
<li><span class="math inline">\(X\)</span> takes the value “A” while <span class="math inline">\(Y\)</span> takes the value “A” (whoo hoo!);</li>
<li><span class="math inline">\(X\)</span> takes the value “A” while <span class="math inline">\(Y\)</span> takes the value “B”;</li>
<li><span class="math inline">\(X\)</span> takes the value “A” while <span class="math inline">\(Y\)</span> takes the value “C”;</li>
<li><span class="math inline">\(X\)</span> takes the value “A” while <span class="math inline">\(Y\)</span> takes the value “D”; and,</li>
<li><span class="math inline">\(X\)</span> takes the value “A” while <span class="math inline">\(Y\)</span> takes the value “F” (let’s hope not).</li>
</ul>
<p>Each of these has some probability of occurring. Since this exhausts all possibilities for <span class="math inline">\(Y\)</span>, then we can determine the probability of <span class="math inline">\(X\)</span> by adding up probability of each of these mutually exclusive events. The above lemma captures that we can do this for all values in the support of <span class="math inline">\(X\)</span> simultaneously.</p>
<p>The above definition is sufficient for several applications. However, it is worth stating the theorem from the most general of perspectives. To do so, we need to define the concept of a random vector.</p>
<div class="definition">
<p><span id="def:defn-random-vector" class="definition"><strong>Definition 7.2  (Random Vector) </strong></span>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be <span class="math inline">\(n\)</span> random variables. Then, the vector <span class="math inline">\(\mathbf{X} = \left(X_1, X_2, \dots, X_n\right)^\top\)</span> is a random vector of length <span class="math inline">\(n\)</span>.</p>
</div>
<p>A random vector is essentially a vector comprised of random components. This will be necessary moving forward because we typically have samples of size <span class="math inline">\(n &gt; 1\)</span>.</p>
<div class="theorem">
<p><span id="thm:thm-bayes-rule" class="theorem"><strong>Theorem 7.3  (Bayes Theorem) </strong></span>Let <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span> be two random <em>vectors</em>. Then, we have that</p>
<p><span class="math display">\[f_{\mathbf{Y} \mid \mathbf{X}}(\mathbf{y} \mid \mathbf{x}) = \frac{f_{\mathbf{X} \mid \mathbf{Y}}(\mathbf{x} \mid \mathbf{y}) f_{\mathbf{Y}}(\mathbf{y})}{\int_{\mathcal{S}_\mathbf{Y}} f_{\mathbf{X} \mid \mathbf{Y}}(\mathbf{x} \mid \mathbf{y}) f_{\mathbf{Y}}(\mathbf{y}) d\mathbf{y}}\]</span></p>
<p>where the integral is now a multi-dimensional integral. Again, integration for any component is replaced by summation when needed.</p>
</div>
<div id="tenants-of-the-bayesian-approach-to-inference" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Tenants of the Bayesian Approach to Inference</h2>
<p>The above results on their own may be interesting in a probability course. However, we are interested primarily in their application when we have observed a sample from a population which is not fully known. Before we delve into the mechanics, let’s pause to reflect on how we intend to apply these results.</p>
<p>Recall statistics is about using a sample to make inference on the population. Specifically, we will posit a model for the distribution of a response within the population; however, that model will be specified only up to some unknown parameters. There are two general statistical paradigms for performing inference, and these stem from two different questions we might ask:</p>
<ul>
<li>Given a hypothesis is true, how likely is the observed data?</li>
<li>Given the observed data, how likely is a particular hypothesis?</li>
</ul>
<p>The first question results in the classical Frequentist perspective (most statistical courses). The second results in the Bayesian perspective.</p>
<p>Prior to collecting data, we might have some belief about the those parameters. Then, we collect a sample from the population; since this data is representative of the population, it must contain information about those parameters. Therefore, we want to update our belief about those parameters in light of this data. That is the Bayesian process in a nutshell.</p>
<div class="keyidea" name="Tenants of the Bayesian Approach to Inference">
<p>The following three tenants are things that are at the heart of all our analyses in this course.</p>
<ul>
<li>The Bayesian approach takes into account <em>prior</em> knowledge when making inference.</li>
<li>The Bayesian approach uses probability to <em>quantify uncertainty</em> in parameters are models.</li>
<li>The Bayesian approach conditions on the observed data to <em>update</em> our prior knowledge.</li>
</ul>
</div>
<p>Throughout, we will rely on a subjective view of probability. That is, probability characterizes how sure you are of something. So, it does not make sense to say “how likely is it to rain tomorrow?” There is no one probability that answers this question. Instead, we will always have (even if not explicitly stated) a “how likely <em>do you believe</em>…’’ element to our question. That is, we are always bringing in our personal (subjective) opinion. This can be very uncomfortable for some of us — the idea of there not being a single”right” answer. We will save this discussion for a future chapter; right now, it suffices to say that in a probability course, we generally do not consider the <em>interpretation</em> of probability (it is simply a value to be computed). However, it is this interpretation that separates Bayesians and Frequentists.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="Summaries.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="modeling-samples.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
