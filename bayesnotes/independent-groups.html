<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>20 Models for Comparing Independent Groups | Course Notes for Bayesian Data Analysis</title>
  <meta name="description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="20 Models for Comparing Independent Groups | Course Notes for Bayesian Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="20 Models for Comparing Independent Groups | Course Notes for Bayesian Data Analysis" />
  
  <meta name="twitter:description" content="Course notes for MA483 (Bayesian Data Analysis) at Rose-Hulman Institute of Technology." />
  

<meta name="author" content="Eric M Reyes" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="study-design.html"/>
<link rel="next" href="dependent-groups.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="essential-probability.html"><a href="essential-probability.html"><i class="fa fa-check"></i><b>1</b> Essential Probability</a>
<ul>
<li class="chapter" data-level="1.1" data-path="essential-probability.html"><a href="essential-probability.html#density-functions-as-models"><i class="fa fa-check"></i><b>1.1</b> Density Functions as Models</a></li>
<li class="chapter" data-level="1.2" data-path="essential-probability.html"><a href="essential-probability.html#summarizing-distributions-parameters"><i class="fa fa-check"></i><b>1.2</b> Summarizing Distributions (Parameters)</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="essential-probability.html"><a href="essential-probability.html#kernels"><i class="fa fa-check"></i><b>1.2.1</b> Kernels</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="essential-probability.html"><a href="essential-probability.html#transformations-of-random-variables"><i class="fa fa-check"></i><b>1.3</b> Transformations of Random Variables</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="essential-probability.html"><a href="essential-probability.html#expectations-of-functions"><i class="fa fa-check"></i><b>1.3.1</b> Expectations of Functions</a></li>
<li class="chapter" data-level="1.3.2" data-path="essential-probability.html"><a href="essential-probability.html#moment-generating-functions"><i class="fa fa-check"></i><b>1.3.2</b> Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="essential-probability.html"><a href="essential-probability.html#independent-random-variables"><i class="fa fa-check"></i><b>1.4</b> Independent Random Variables</a></li>
</ul></li>
<li class="part"><span><b>I Language and Logic of Inference</b></span></li>
<li class="chapter" data-level="2" data-path="CaseDeepwater.html"><a href="CaseDeepwater.html"><i class="fa fa-check"></i><b>2</b> Case Study: Health Effects of the Deepwater Horizon Oil Spill</a></li>
<li class="chapter" data-level="3" data-path="Basics.html"><a href="Basics.html"><i class="fa fa-check"></i><b>3</b> The Statistical Process</a>
<ul>
<li class="chapter" data-level="3.1" data-path="Basics.html"><a href="Basics.html#overview-of-drawing-inference"><i class="fa fa-check"></i><b>3.1</b> Overview of Drawing Inference</a></li>
<li class="chapter" data-level="3.2" data-path="Basics.html"><a href="Basics.html#anatomy-of-a-dataset"><i class="fa fa-check"></i><b>3.2</b> Anatomy of a Dataset</a></li>
<li class="chapter" data-level="3.3" data-path="Basics.html"><a href="Basics.html#a-note-on-codebooks"><i class="fa fa-check"></i><b>3.3</b> A Note on Codebooks</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Questions.html"><a href="Questions.html"><i class="fa fa-check"></i><b>4</b> Asking the Right Questions</a>
<ul>
<li class="chapter" data-level="4.1" data-path="Questions.html"><a href="Questions.html#characterizing-a-variable"><i class="fa fa-check"></i><b>4.1</b> Characterizing a Variable</a></li>
<li class="chapter" data-level="4.2" data-path="Questions.html"><a href="Questions.html#framing-the-question"><i class="fa fa-check"></i><b>4.2</b> Framing the Question</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Data.html"><a href="Data.html"><i class="fa fa-check"></i><b>5</b> Gathering the Evidence (Data Collection)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="Data.html"><a href="Data.html#what-makes-a-sample-reliable"><i class="fa fa-check"></i><b>5.1</b> What Makes a Sample Reliable</a></li>
<li class="chapter" data-level="5.2" data-path="Data.html"><a href="Data.html#poor-methods-of-data-collection"><i class="fa fa-check"></i><b>5.2</b> Poor Methods of Data Collection</a></li>
<li class="chapter" data-level="5.3" data-path="Data.html"><a href="Data.html#preferred-methods-of-sampling"><i class="fa fa-check"></i><b>5.3</b> Preferred Methods of Sampling</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="Summaries.html"><a href="Summaries.html"><i class="fa fa-check"></i><b>6</b> Presenting the Evidence (Summarizing Data)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="Summaries.html"><a href="Summaries.html#characteristics-of-a-distribution-summarizing-a-single-variable"><i class="fa fa-check"></i><b>6.1</b> Characteristics of a Distribution (Summarizing a Single Variable)</a></li>
<li class="chapter" data-level="6.2" data-path="Summaries.html"><a href="Summaries.html#summarizing-relationships"><i class="fa fa-check"></i><b>6.2</b> Summarizing Relationships</a></li>
</ul></li>
<li class="part"><span><b>II Fundamentals of Bayesian Inference</b></span></li>
<li class="chapter" data-level="7" data-path="bayes-rule.html"><a href="bayes-rule.html"><i class="fa fa-check"></i><b>7</b> Bayes Rule</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bayes-rule.html"><a href="bayes-rule.html#tenants-of-the-bayesian-approach-to-inference"><i class="fa fa-check"></i><b>7.1</b> Tenants of the Bayesian Approach to Inference</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="modeling-samples.html"><a href="modeling-samples.html"><i class="fa fa-check"></i><b>8</b> Modeling Samples</a>
<ul>
<li class="chapter" data-level="8.1" data-path="modeling-samples.html"><a href="modeling-samples.html#independence"><i class="fa fa-check"></i><b>8.1</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="quantifying-prior-information.html"><a href="quantifying-prior-information.html"><i class="fa fa-check"></i><b>9</b> Quantifying Prior Information</a></li>
<li class="chapter" data-level="10" data-path="posterior-distributions.html"><a href="posterior-distributions.html"><i class="fa fa-check"></i><b>10</b> Updating Prior Beliefs (Posterior Distributions)</a></li>
<li class="chapter" data-level="11" data-path="point-estimation.html"><a href="point-estimation.html"><i class="fa fa-check"></i><b>11</b> Point Estimation</a></li>
<li class="chapter" data-level="12" data-path="interval-estimation.html"><a href="interval-estimation.html"><i class="fa fa-check"></i><b>12</b> Interval Estimation</a></li>
<li class="chapter" data-level="13" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>13</b> Prediction</a>
<ul>
<li class="chapter" data-level="13.1" data-path="prediction.html"><a href="prediction.html#derivation-of-the-posterior-predictive"><i class="fa fa-check"></i><b>13.1</b> Derivation of the Posterior Predictive</a></li>
<li class="chapter" data-level="13.2" data-path="prediction.html"><a href="prediction.html#summary"><i class="fa fa-check"></i><b>13.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>14</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="14.1" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#point-null-hypotheses"><i class="fa fa-check"></i><b>14.1</b> Point-Null Hypotheses</a></li>
<li class="chapter" data-level="14.2" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html#model-comparison"><i class="fa fa-check"></i><b>14.2</b> Model Comparison</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="constructing-priors.html"><a href="constructing-priors.html"><i class="fa fa-check"></i><b>15</b> Constructing Prior Distributions</a>
<ul>
<li class="chapter" data-level="15.1" data-path="constructing-priors.html"><a href="constructing-priors.html#elicitation-from-experts"><i class="fa fa-check"></i><b>15.1</b> Elicitation from Experts</a></li>
<li class="chapter" data-level="15.2" data-path="constructing-priors.html"><a href="constructing-priors.html#mixtures"><i class="fa fa-check"></i><b>15.2</b> Mixtures</a></li>
<li class="chapter" data-level="15.3" data-path="constructing-priors.html"><a href="constructing-priors.html#chains"><i class="fa fa-check"></i><b>15.3</b> Chains</a></li>
<li class="chapter" data-level="15.4" data-path="constructing-priors.html"><a href="constructing-priors.html#non-informative-priors"><i class="fa fa-check"></i><b>15.4</b> Non-Informative Priors</a></li>
</ul></li>
<li class="part"><span><b>III Numerical Approaches to Bayesian Computations</b></span></li>
<li class="chapter" data-level="16" data-path="mc-integration.html"><a href="mc-integration.html"><i class="fa fa-check"></i><b>16</b> Monte Carlo Integration</a></li>
<li class="chapter" data-level="17" data-path="mcmc.html"><a href="mcmc.html"><i class="fa fa-check"></i><b>17</b> Markov Chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="17.1" data-path="mcmc.html"><a href="mcmc.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>17.1</b> Hamiltonian Monte Carlo</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="mcmc-assessment.html"><a href="mcmc-assessment.html"><i class="fa fa-check"></i><b>18</b> Assessing MCMC Samples</a></li>
<li class="part"><span><b>IV Hierarchical Models Comparing Groups</b></span></li>
<li class="chapter" data-level="19" data-path="study-design.html"><a href="study-design.html"><i class="fa fa-check"></i><b>19</b> Elements of Good Study Design</a></li>
<li class="chapter" data-level="20" data-path="independent-groups.html"><a href="independent-groups.html"><i class="fa fa-check"></i><b>20</b> Models for Comparing Independent Groups</a>
<ul>
<li class="chapter" data-level="20.1" data-path="independent-groups.html"><a href="independent-groups.html#bridge-sampling"><i class="fa fa-check"></i><b>20.1</b> Bridge Sampling</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="dependent-groups.html"><a href="dependent-groups.html"><i class="fa fa-check"></i><b>21</b> Comparing Related Groups</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Course Notes for Bayesian Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="independent-groups" class="section level1" number="20">
<h1><span class="header-section-number">20</span> Models for Comparing Independent Groups</h1>
<p>While the approaches we have described in the text have been general, we have generally assumed we were modeling a single variable from a single population. That is, we were interested in characterizing the data generating process for a response <span class="math inline">\(Y\)</span>; specifically, we assumed the density <span class="math inline">\(f(y \mid \boldsymbol{\theta})\)</span> of <span class="math inline">\(Y\)</span> depended on unknown parameters <span class="math inline">\(\boldsymbol{\theta}\)</span>. We would use a random sample <span class="math inline">\(Y_1, Y_2, \dots, Y_n\)</span> from this population to make inference on those parameters. Many questions do not fit into that mold.</p>
<div class="example">
<p><span id="exm:ex-distracted-driving" class="example"><strong>Example 20.1  (Distracted Driving) </strong></span>Nearly every state in the US has a restriction on cell phone use while driving. Some states prohibit texting while driving, while states like Indiana only permit hands-free use of a phone. The goal of these regulations is to reduce distractions while driving and thereby improve reaction time.</p>
</div>
<p>Does cell phone use reduce reaction time? Does it increase the likelihood of a collision? These questions seek to compare a response (the reaction time) across groups formed by the predictor (cell phone use or not). There are two ways to conceptually think about this comparison. We can think of there being two independent populations â€” those who use cell phones while driving, and those who do not use cell phones while driving. In this perspective, we are interested in comparing these two populations. From this perspective, we imagine sampling from each population separately. Specifically, let <span class="math inline">\(Y_{j,i}\)</span> be the response for the <span class="math inline">\(i\)</span>-th observation in population <span class="math inline">\(j\)</span>, <span class="math inline">\(i = 1, 2, \dotsc, n_j\)</span>, where <span class="math inline">\(n_j\)</span> is the number of subjects observed from population <span class="math inline">\(j\)</span> and <span class="math inline">\(j = 1, 2, \dotsc, J\)</span>, where <span class="math inline">\(J\)</span> is the number of populations being compared. Further, we assume <span class="math inline">\(Y_{j, i} \stackrel{IID}{\sim} f_j\left(y \mid \boldsymbol{\theta}_j\right)\)</span>; that is, we have a random sample from each population. Notice that we are potentially allowing the form of the data generating process <span class="math inline">\(f\)</span> to vary for each population; and, we certainly allow the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> to vary for each population. Since we consider each sample independent of one another, we have that the likelihood has the form</p>
<p><span class="math display">\[f(\mathbf{y} \mid \boldsymbol{\theta}) = \prod_{j=1}^{J} \prod_{i=1}^{n_j} f_j\left(y_{j, i} \mid \boldsymbol{\theta}_j\right).\]</span></p>
<p>It now remains to specify a prior on each <span class="math inline">\(\boldsymbol{\theta}_j\)</span>. Of course, if the groups are independent of one another, it is also reasonable to assume the parameters are independent of one another. This leads to</p>
<p><span class="math display">\[\pi(\boldsymbol{\theta}) = \prod_{j=1}^{J} \pi_j(\boldsymbol{\theta}_j).\]</span></p>
<p>An alternative perspective is to see this as one population (those who drive) exposed to two different treatments (distracted and not). In this perspective, we are interested in comparing the impact of the treatments on the response. We let <span class="math inline">\(Y_i\)</span> be the response for the <span class="math inline">\(i\)</span>-th observation in the population, and we let <span class="math inline">\(x_i\)</span> be a variable that indicates to which treatment the <span class="math inline">\(i\)</span>-th observation has been exposed; that is, <span class="math inline">\(x_i = j\)</span> if the <span class="math inline">\(i\)</span>-th observation has been exposed to the <span class="math inline">\(j\)</span>-th treatment. We then have that <span class="math inline">\(Y_i \stackrel{Ind}{\sim} f\left(y \mid \boldsymbol{\theta}_{x_i}\right)\)</span>. Notice that we do not say that the responses are identically distributed; while the family of distributions is the same (all have the same <span class="math inline">\(f\)</span>), the parameters on which each distribution depends is allowed to differ (based on the value of <span class="math inline">\(x_i\)</span>). Therefore, we are only willing to say the observations are independent. The likelihood then has the form</p>
<p><span class="math display">\[f(\mathbf{y} \mid \boldsymbol{\theta}) = \prod_{i=1}^{n} f\left(y \mid \boldsymbol{\theta}_{x_i}\right).\]</span></p>
<p>It now remains to specify a prior on each <span class="math inline">\(\boldsymbol{\theta}_j\)</span>. If we are willing to assume the treatment groups are independent of one another, it is also reasonable to assume the parameters are independent of one another. This leads to</p>
<p><span class="math display">\[\pi(\boldsymbol{\theta}) = \prod_{j=1}^{J} \pi_j(\boldsymbol{\theta}_j).\]</span></p>
<p>These two perspectives are equivalent.</p>
<div class="rmdtip">
<p>When comparing groups, whether you view the groups as separate populations or a single population exposed to different treatments, the inference is the same provided the groups are independent.</p>
</div>
<p>In order to extend inference from a single variable to comparing groups, we essentially allow our parameters to be based on the group from which the data was taken. That is, we add a second layer to our model.</p>
<div id="bridge-sampling" class="section level2" number="20.1">
<h2><span class="header-section-number">20.1</span> Bridge Sampling</h2>
<p>In Chapter <a href="hypothesis-testing.html#hypothesis-testing">14</a> we introduced the idea of model comparison. This is especially useful when comparing groups. Consider a simple case in which we have two groups. Specifically, suppose</p>
<p><span class="math display">\[
\begin{aligned}
  Y_{1,i} &amp;\stackrel{IID}{\sim} f(y \mid \theta_1) \\
  Y_{2,i} &amp;\stackrel{IID}{\sim} f(y \mid \theta_2) \\
  Y_{1,i} &amp;\perp\negthickspace\negmedspace\perp Y_{2,j} \quad \forall i, j
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\perp\negthickspace\negmedspace\perp\)</span> refers to independence. In this example, the data generating process for each group is distinguished only by a potentially different value of the parameter. It would be natural in such settings to consider the hypotheses</p>
<p><span class="math display">\[H_0: \theta_1 = \theta_2 \qquad \text{vs.} \qquad H_1: \theta_1 \neq \theta_2\]</span></p>
<p>If we place a continuous prior on the parameters, then the probability of <span class="math inline">\(H_0\)</span> must be 0. One way of addressing this problem is to consider a clever prior which places mass along the <em>line</em> equating the two parameters. A more natural solution is to think of this as a model comparison problem:</p>
<p><span class="math display">\[
\begin{aligned}
  \mathcal{M}_0:&amp; \quad Y_{j, i} \stackrel{IID}{\sim} f(y \mid \theta) \\
    &amp; \quad \theta \sim \pi_0(\theta) \\
  \mathcal{M}_1:&amp; \quad Y_{j, i} \stackrel{Ind}{\sim} f(y \mid \theta_j) \\
    &amp; \quad \theta_j \stackrel{Ind}{\sim} \pi_j(\theta_j)
\end{aligned}
\]</span></p>
<p>for <span class="math inline">\(j = 1, 2\)</span>. Notice that under Model <span class="math inline">\(\mathcal{M}_0\)</span>, there is only a single parameter, capturing the null hypothesis <span class="math inline">\(\theta_1 = \theta_2\)</span>. And, model <span class="math inline">\(\mathcal{M}_1\)</span> allows the flexibility for two parameters, capturing the alternative hypothesis. We would therefore be interested in computing a Bayes Factor comparing these two models, as described in Definition <a href="hypothesis-testing.html#def:defn-bayes-factor">14.2</a>. The problem is, the Bayes Factor depends on the evidence</p>
<p><span class="math display">\[f(\mathbf{y} \mid \mathcal{M}_j) = \int f(\mathbf{y} \mid \boldsymbol{\theta}, \mathcal{M}_j) \pi(\boldsymbol{\theta} \mid \mathcal{M}_j) d\boldsymbol{\theta}.\]</span></p>
<p>Unfortunately, this integral is quite difficult to estimate. Using the prior and performing MC methods is not typically very good because the prior is often too vague. The solution is to use the posterior along with bridge sampling.</p>
<div class="definition">
<p><span id="def:defn-bridge-sampling" class="definition"><strong>Definition 20.1  (Bridge Sampling) </strong></span>The bridge sampling estimator of the marginal likelihood <span class="math inline">\(m(\mathbf{y})\)</span> is given by</p>
<p><span class="math display">\[\begin{aligned}
  m(\mathbf{y}) 
    &amp;= \int f(\mathbf{y} \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta}) d\boldsymbol{\theta} \\
    &amp;= \frac{E_g\left[h(\boldsymbol{\theta}) f(\mathbf{y} \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta})\right]}{E_{\pi}\left[h(\boldsymbol{\theta}) g(\boldsymbol{\theta}) \right]} \\
    &amp;\approx \frac{m^{-1}\sum_{j=1}^{m} h\left(\tilde{\boldsymbol{\theta}}_j\right) f\left(\mathbf{y} \mid \tilde{\boldsymbol{\theta}}_j\right) \pi\left(\tilde{\boldsymbol{\theta}}_j\right)}{m^{-1}\sum_{i=1}^{m} h\left(\boldsymbol{\theta}^*_j\right) g\left(\boldsymbol{\theta}^*_j\right)}
\end{aligned}\]</span></p>
<p>where <span class="math inline">\(h(\boldsymbol{\theta})\)</span> is called the bridge function and <span class="math inline">\(g(\boldsymbol{\theta})\)</span> is the proposal distribution. Here, <span class="math inline">\(\tilde{\boldsymbol{\theta}}\)</span> denotes a random variate from the proposal distribution and <span class="math inline">\(\boldsymbol{\theta}^*\)</span> a random variate from the posterior, and <span class="math inline">\(E_g\)</span> denotes taking an expectation with respect to the proposal distribution an <span class="math inline">\(E_\pi\)</span> denotes taking an expectation with respect to the posterior distribution.</p>
</div>
<p>Typically, a Normal distribution is used for the proposal distribution. This process had been implemented in some computing software. While the details of bridge sampling are beyond the scope of this text, we do want to note that the definition comes from making the following observation:</p>
<p><span class="math display">\[\begin{aligned}
  1 &amp;= \frac{\int f(\mathbf{y} \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta}) g(\boldsymbol{\theta}) h(\boldsymbol{\theta})d\boldsymbol{\theta}}{\int f(\mathbf{y} \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta}) g(\boldsymbol{\theta}) h(\boldsymbol{\theta})d\boldsymbol{\theta}} \\
  m(\mathbf{y}) &amp;= m(\mathbf{y}) \frac{\int f(\mathbf{y} \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta}) g(\boldsymbol{\theta}) h(\boldsymbol{\theta})d\boldsymbol{\theta}}{\int f(\mathbf{y} \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta}) g(\boldsymbol{\theta}) h(\boldsymbol{\theta})d\boldsymbol{\theta}} 
\end{aligned}\]</span></p>
<p>where we simply multiply both sides by the marginal distribution, which will be non-negative on its support. Next, next recognize that the integral is with respect to <span class="math inline">\(\boldsymbol{\theta}\)</span>; therefore, we can move the marginal distribution inside the integral in the denominator to get</p>
<p><span class="math display">\[\begin{aligned}
  m(\mathbf{y}) &amp;= \frac{\int f(\mathbf{y} \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta}) h(\boldsymbol{\theta}) g(\boldsymbol{\theta})d\boldsymbol{\theta}}{\int \frac{f(\mathbf{y} \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta})}{m(\mathbf{y})} h(\boldsymbol{\theta}) g(\boldsymbol{\theta})d\boldsymbol{\theta}} \\
    &amp;= \frac{\int f(\mathbf{y} \mid \boldsymbol{\theta}) \pi(\boldsymbol{\theta}) h(\boldsymbol{\theta}) g(\boldsymbol{\theta})d\boldsymbol{\theta}}{\int \pi(\boldsymbol{\theta} \mid \mathbf{y}) h(\boldsymbol{\theta}) g(\boldsymbol{\theta})d\boldsymbol{\theta}}
\end{aligned}\]</span></p>
<p>where the last equality makes use of the definition of the posterior distribution from Bayes Theorem. Now, the top integral can be viewed in terms of an expectation over a random variable <span class="math inline">\(\boldsymbol{\theta} \sim g(\boldsymbol{\theta})\)</span>, and the denominator can be viewed as an expectation over a random variable which follows the posterior distribution.</p>
<div class="rmdkeyidea">
<p>Bridge sampling is an efficient algorithm for estimating the evidence of a model, allowing for computation of the Bayes Factor.</p>
</div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="study-design.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="dependent-groups.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
