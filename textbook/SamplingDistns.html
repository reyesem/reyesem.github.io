<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Statistical Foundations for Engineers and Scientists</title>
  <meta name="description" content="Course notes for MA223 (Engineering Statistics I) at Rose-Hulman Institute of Technology.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Statistical Foundations for Engineers and Scientists" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course notes for MA223 (Engineering Statistics I) at Rose-Hulman Institute of Technology." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Statistical Foundations for Engineers and Scientists" />
  
  <meta name="twitter:description" content="Course notes for MA223 (Engineering Statistics I) at Rose-Hulman Institute of Technology." />
  

<meta name="author" content="Eric M Reyes">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="Summaries.html">
<link rel="next" href="NullDistns.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Modeling</a></li>

<li class="divider"></li>
<li class="part"><span><b>I Unit I: Language and Logic of Inference</b></span></li>
<li class="chapter" data-level="1" data-path="Basics.html"><a href="Basics.html"><i class="fa fa-check"></i><b>1</b> The Statistical Process</a><ul>
<li class="chapter" data-level="1.1" data-path="Basics.html"><a href="Basics.html#overview-of-drawing-inference"><i class="fa fa-check"></i><b>1.1</b> Overview of Drawing Inference</a></li>
<li class="chapter" data-level="1.2" data-path="Basics.html"><a href="Basics.html#anatomy-of-a-dataset"><i class="fa fa-check"></i><b>1.2</b> Anatomy of a Dataset</a></li>
<li class="chapter" data-level="1.3" data-path="Basics.html"><a href="Basics.html#a-note-on-codebooks"><i class="fa fa-check"></i><b>1.3</b> A Note on Codebooks</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="CaseDeepwater.html"><a href="CaseDeepwater.html"><i class="fa fa-check"></i><b>2</b> Case Study: Health Effects of the Deepwater Horizon Oil Spill</a></li>
<li class="chapter" data-level="3" data-path="Questions.html"><a href="Questions.html"><i class="fa fa-check"></i><b>3</b> Asking the Right Questions</a><ul>
<li class="chapter" data-level="3.1" data-path="Questions.html"><a href="Questions.html#characterizing-a-variable"><i class="fa fa-check"></i><b>3.1</b> Characterizing a Variable</a></li>
<li class="chapter" data-level="3.2" data-path="Questions.html"><a href="Questions.html#framing-the-question"><i class="fa fa-check"></i><b>3.2</b> Framing the Question</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Data.html"><a href="Data.html"><i class="fa fa-check"></i><b>4</b> Gathering the Evidence (Data Collection)</a><ul>
<li class="chapter" data-level="4.1" data-path="Data.html"><a href="Data.html#what-makes-a-sample-reliable"><i class="fa fa-check"></i><b>4.1</b> What Makes a Sample Reliable</a></li>
<li class="chapter" data-level="4.2" data-path="Data.html"><a href="Data.html#poor-methods-of-data-collection"><i class="fa fa-check"></i><b>4.2</b> Poor Methods of Data Collection</a></li>
<li class="chapter" data-level="4.3" data-path="Data.html"><a href="Data.html#preferred-methods-of-sampling"><i class="fa fa-check"></i><b>4.3</b> Preferred Methods of Sampling</a></li>
<li class="chapter" data-level="4.4" data-path="Data.html"><a href="Data.html#two-types-of-studies"><i class="fa fa-check"></i><b>4.4</b> Two Types of Studies</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Summaries.html"><a href="Summaries.html"><i class="fa fa-check"></i><b>5</b> Presenting the Evidence (Summarizing Data)</a><ul>
<li class="chapter" data-level="5.1" data-path="Summaries.html"><a href="Summaries.html#characteristics-of-a-distribution-summarizing-a-single-variable"><i class="fa fa-check"></i><b>5.1</b> Characteristics of a Distribution (Summarizing a Single Variable)</a></li>
<li class="chapter" data-level="5.2" data-path="Summaries.html"><a href="Summaries.html#summarizing-relationships"><i class="fa fa-check"></i><b>5.2</b> Summarizing Relationships</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="SamplingDistns.html"><a href="SamplingDistns.html"><i class="fa fa-check"></i><b>6</b> Assessing the Evidence (Quantifying the Variability in Estimates)</a><ul>
<li class="chapter" data-level="6.1" data-path="SamplingDistns.html"><a href="SamplingDistns.html#conceptualizing-the-sampling-distribution"><i class="fa fa-check"></i><b>6.1</b> Conceptualizing the Sampling Distribution</a></li>
<li class="chapter" data-level="6.2" data-path="SamplingDistns.html"><a href="SamplingDistns.html#example-of-a-sampling-distribution"><i class="fa fa-check"></i><b>6.2</b> Example of a Sampling Distribution</a></li>
<li class="chapter" data-level="6.3" data-path="SamplingDistns.html"><a href="SamplingDistns.html#modeling-the-sampling-distribution"><i class="fa fa-check"></i><b>6.3</b> Modeling the Sampling Distribution</a></li>
<li class="chapter" data-level="6.4" data-path="SamplingDistns.html"><a href="SamplingDistns.html#using-a-model-for-the-sampling-distributions-confidence-intervals"><i class="fa fa-check"></i><b>6.4</b> Using a Model for the Sampling Distributions (Confidence Intervals)</a></li>
<li class="chapter" data-level="6.5" data-path="SamplingDistns.html"><a href="SamplingDistns.html#bringing-it-all-together"><i class="fa fa-check"></i><b>6.5</b> Bringing it All Together</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="NullDistns.html"><a href="NullDistns.html"><i class="fa fa-check"></i><b>7</b> Quantifying the Evidence (Rejecting Bad Models)</a><ul>
<li class="chapter" data-level="7.1" data-path="NullDistns.html"><a href="NullDistns.html#some-subtleties"><i class="fa fa-check"></i><b>7.1</b> Some Subtleties</a></li>
<li class="chapter" data-level="7.2" data-path="NullDistns.html"><a href="NullDistns.html#assuming-the-null-hypothesis"><i class="fa fa-check"></i><b>7.2</b> Assuming the Null Hypothesis</a></li>
<li class="chapter" data-level="7.3" data-path="NullDistns.html"><a href="NullDistns.html#using-the-null-distribution"><i class="fa fa-check"></i><b>7.3</b> Using the Null Distribution</a></li>
<li class="chapter" data-level="7.4" data-path="NullDistns.html"><a href="NullDistns.html#sampling-distributions-vs.null-distributions"><i class="fa fa-check"></i><b>7.4</b> Sampling Distributions vs.Â Null Distributions</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="RecapLanguage.html"><a href="RecapLanguage.html"><i class="fa fa-check"></i><b>8</b> Using the Tools Together</a><ul>
<li class="chapter" data-level="8.1" data-path="RecapLanguage.html"><a href="RecapLanguage.html#framing-the-question-fundamental-idea-i"><i class="fa fa-check"></i><b>8.1</b> Framing the Question (Fundamental Idea I)</a></li>
<li class="chapter" data-level="8.2" data-path="RecapLanguage.html"><a href="RecapLanguage.html#getting-good-data-fundamental-idea-ii"><i class="fa fa-check"></i><b>8.2</b> Getting Good Data (Fundamental Idea II)</a></li>
<li class="chapter" data-level="8.3" data-path="RecapLanguage.html"><a href="RecapLanguage.html#presenting-the-data-fundamental-idea-iii"><i class="fa fa-check"></i><b>8.3</b> Presenting the Data (Fundamental Idea III)</a></li>
<li class="chapter" data-level="8.4" data-path="RecapLanguage.html"><a href="RecapLanguage.html#quantifying-the-variability-in-the-estimate-fundamental-idea-iv"><i class="fa fa-check"></i><b>8.4</b> Quantifying the Variability in the Estimate (Fundamental Idea IV)</a></li>
<li class="chapter" data-level="8.5" data-path="RecapLanguage.html"><a href="RecapLanguage.html#quantifying-the-evidence-fundamental-idea-v"><i class="fa fa-check"></i><b>8.5</b> Quantifying the Evidence (Fundamental Idea V)</a></li>
<li class="chapter" data-level="8.6" data-path="RecapLanguage.html"><a href="RecapLanguage.html#summary"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
</ul></li>
<li class="part"><span><b>II Unit II: Comparing the Average Response Across Groups</b></span></li>
<li class="chapter" data-level="9" data-path="CaseOrganic.html"><a href="CaseOrganic.html"><i class="fa fa-check"></i><b>9</b> Case Study: Organic Foods and Superior Morals</a></li>
<li class="chapter" data-level="10" data-path="ANOVAquestions.html"><a href="ANOVAquestions.html"><i class="fa fa-check"></i><b>10</b> Framing the Question</a><ul>
<li class="chapter" data-level="10.1" data-path="ANOVAquestions.html"><a href="ANOVAquestions.html#general-setting"><i class="fa fa-check"></i><b>10.1</b> General Setting</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="ANOVAdata.html"><a href="ANOVAdata.html"><i class="fa fa-check"></i><b>11</b> Study Design</a><ul>
<li class="chapter" data-level="11.1" data-path="ANOVAdata.html"><a href="ANOVAdata.html#aspects-of-a-well-designed-experiment"><i class="fa fa-check"></i><b>11.1</b> Aspects of a Well Designed Experiment</a></li>
<li class="chapter" data-level="11.2" data-path="ANOVAdata.html"><a href="ANOVAdata.html#collecting-observational-data"><i class="fa fa-check"></i><b>11.2</b> Collecting Observational Data</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ANOVAsummaries.html"><a href="ANOVAsummaries.html"><i class="fa fa-check"></i><b>12</b> Presenting the Data</a></li>
<li class="chapter" data-level="13" data-path="ANOVAteststat.html"><a href="ANOVAteststat.html"><i class="fa fa-check"></i><b>13</b> Quantifying the Evidence</a><ul>
<li class="chapter" data-level="13.1" data-path="ANOVAteststat.html"><a href="ANOVAteststat.html#partitioning-variability"><i class="fa fa-check"></i><b>13.1</b> Partitioning Variability</a></li>
<li class="chapter" data-level="13.2" data-path="ANOVAteststat.html"><a href="ANOVAteststat.html#forming-a-standardized-test-statistic"><i class="fa fa-check"></i><b>13.2</b> Forming a Standardized Test Statistic</a></li>
<li class="chapter" data-level="13.3" data-path="ANOVAteststat.html"><a href="ANOVAteststat.html#obtaining-a-p-value"><i class="fa fa-check"></i><b>13.3</b> Obtaining a P-value</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ANOVAmodel.html"><a href="ANOVAmodel.html"><i class="fa fa-check"></i><b>14</b> Building the Statistical Model</a><ul>
<li class="chapter" data-level="14.1" data-path="ANOVAmodel.html"><a href="ANOVAmodel.html#general-formulation"><i class="fa fa-check"></i><b>14.1</b> General Formulation</a></li>
<li class="chapter" data-level="14.2" data-path="ANOVAmodel.html"><a href="ANOVAmodel.html#statistical-model-for-a-quantitative-response-and-a-categorical-predictor"><i class="fa fa-check"></i><b>14.2</b> Statistical Model for A Quantitative Response and a Categorical Predictor</a></li>
<li class="chapter" data-level="14.3" data-path="ANOVAmodel.html"><a href="ANOVAmodel.html#conditions-on-the-error-distribution"><i class="fa fa-check"></i><b>14.3</b> Conditions on the Error Distribution</a></li>
<li class="chapter" data-level="14.4" data-path="ANOVAmodel.html"><a href="ANOVAmodel.html#simulating-the-null-distribution"><i class="fa fa-check"></i><b>14.4</b> Simulating the Null Distribution</a></li>
<li class="chapter" data-level="14.5" data-path="ANOVAmodel.html"><a href="ANOVAmodel.html#recap"><i class="fa fa-check"></i><b>14.5</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="ANOVAclassical.html"><a href="ANOVAclassical.html"><i class="fa fa-check"></i><b>15</b> Classical ANOVA Model</a><ul>
<li class="chapter" data-level="15.1" data-path="ANOVAclassical.html"><a href="ANOVAclassical.html#modeling-the-population"><i class="fa fa-check"></i><b>15.1</b> Modeling the Population</a></li>
<li class="chapter" data-level="15.2" data-path="ANOVAclassical.html"><a href="ANOVAclassical.html#adding-the-assumption-of-normality"><i class="fa fa-check"></i><b>15.2</b> Adding the Assumption of Normality</a></li>
<li class="chapter" data-level="15.3" data-path="ANOVAclassical.html"><a href="ANOVAclassical.html#impact-of-normality-assumption"><i class="fa fa-check"></i><b>15.3</b> Impact of Normality Assumption</a></li>
<li class="chapter" data-level="15.4" data-path="ANOVAclassical.html"><a href="ANOVAclassical.html#analysis-of-organic-food-case-study"><i class="fa fa-check"></i><b>15.4</b> Analysis of Organic Food Case Study</a></li>
<li class="chapter" data-level="15.5" data-path="ANOVAclassical.html"><a href="ANOVAclassical.html#recap-1"><i class="fa fa-check"></i><b>15.5</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="ANOVAassessment.html"><a href="ANOVAassessment.html"><i class="fa fa-check"></i><b>16</b> Assessing Modeling Assumptions</a><ul>
<li class="chapter" data-level="16.1" data-path="ANOVAassessment.html"><a href="ANOVAassessment.html#assessing-independence"><i class="fa fa-check"></i><b>16.1</b> Assessing Independence</a></li>
<li class="chapter" data-level="16.2" data-path="ANOVAassessment.html"><a href="ANOVAassessment.html#assessing-homoskedasticity"><i class="fa fa-check"></i><b>16.2</b> Assessing Homoskedasticity</a></li>
<li class="chapter" data-level="16.3" data-path="ANOVAassessment.html"><a href="ANOVAassessment.html#assessing-normality"><i class="fa fa-check"></i><b>16.3</b> Assessing Normality</a></li>
<li class="chapter" data-level="16.4" data-path="ANOVAassessment.html"><a href="ANOVAassessment.html#general-tips-for-assessing-assumptions"><i class="fa fa-check"></i><b>16.4</b> General Tips for Assessing Assumptions</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="ANOVArecap.html"><a href="ANOVArecap.html"><i class="fa fa-check"></i><b>17</b> Using the Tools Together</a><ul>
<li class="chapter" data-level="17.1" data-path="ANOVArecap.html"><a href="ANOVArecap.html#framing-the-question-fundamental-idea-i-1"><i class="fa fa-check"></i><b>17.1</b> Framing the Question (Fundamental Idea I)</a></li>
<li class="chapter" data-level="17.2" data-path="ANOVArecap.html"><a href="ANOVArecap.html#getting-good-data-fundamental-idea-ii-1"><i class="fa fa-check"></i><b>17.2</b> Getting Good Data (Fundamental Idea II)</a></li>
<li class="chapter" data-level="17.3" data-path="ANOVArecap.html"><a href="ANOVArecap.html#presenting-the-data-fundamental-idea-iii-1"><i class="fa fa-check"></i><b>17.3</b> Presenting the Data (Fundamental Idea III)</a></li>
<li class="chapter" data-level="17.4" data-path="ANOVArecap.html"><a href="ANOVArecap.html#quantifying-the-variability-in-the-estimate-fundamental-idea-iv-1"><i class="fa fa-check"></i><b>17.4</b> Quantifying the Variability in the Estimate (Fundamental Idea IV)</a></li>
<li class="chapter" data-level="17.5" data-path="ANOVArecap.html"><a href="ANOVArecap.html#quantifying-the-evidence-fundamental-idea-v-1"><i class="fa fa-check"></i><b>17.5</b> Quantifying the Evidence (Fundamental Idea V)</a></li>
<li class="chapter" data-level="17.6" data-path="ANOVArecap.html"><a href="ANOVArecap.html#conclusion"><i class="fa fa-check"></i><b>17.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ANOVAblocking.html"><a href="ANOVAblocking.html"><i class="fa fa-check"></i><b>18</b> Analyzing a Design that Incorporates Blocking</a><ul>
<li class="chapter" data-level="18.1" data-path="ANOVAblocking.html"><a href="ANOVAblocking.html#what-is-the-big-deal"><i class="fa fa-check"></i><b>18.1</b> What is the Big Deal?</a></li>
<li class="chapter" data-level="18.2" data-path="ANOVAblocking.html"><a href="ANOVAblocking.html#solution-partition-the-variability"><i class="fa fa-check"></i><b>18.2</b> Solution: Partition the Variability</a></li>
<li class="chapter" data-level="18.3" data-path="ANOVAblocking.html"><a href="ANOVAblocking.html#interpreting-the-analysis"><i class="fa fa-check"></i><b>18.3</b> Interpreting the Analysis</a></li>
</ul></li>
<li class="part"><span><b>III Unit III: Modeling the Average Response as a Function of Several Predictors</b></span></li>
<li class="chapter" data-level="19" data-path="CaseGreece.html"><a href="CaseGreece.html"><i class="fa fa-check"></i><b>19</b> Case Study: Seismic Activity in Greece</a></li>
<li class="chapter" data-level="20" data-path="Regquestions.html"><a href="Regquestions.html"><i class="fa fa-check"></i><b>20</b> Myriad of Potential Questions</a></li>
<li class="chapter" data-level="21" data-path="Regdata.html"><a href="Regdata.html"><i class="fa fa-check"></i><b>21</b> Nature of Collecting Multivariable Data</a></li>
<li class="chapter" data-level="22" data-path="Regsummaries.html"><a href="Regsummaries.html"><i class="fa fa-check"></i><b>22</b> Summarizing Multivariable Data</a><ul>
<li class="chapter" data-level="22.1" data-path="Regsummaries.html"><a href="Regsummaries.html#characterizing-the-marginal-relationship-of-two-quantitative-variables"><i class="fa fa-check"></i><b>22.1</b> Characterizing the Marginal Relationship of Two Quantitative Variables</a></li>
<li class="chapter" data-level="22.2" data-path="Regsummaries.html"><a href="Regsummaries.html#visualizing-the-impact-of-a-third-variable-on-the-marginal-relationship"><i class="fa fa-check"></i><b>22.2</b> Visualizing the Impact of a Third Variable on the Marginal Relationship</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="Regmodel.html"><a href="Regmodel.html"><i class="fa fa-check"></i><b>23</b> Extending Our Statistical Model</a><ul>
<li class="chapter" data-level="23.1" data-path="Regmodel.html"><a href="Regmodel.html#statistical-model-for-a-quantitative-response-and-quantitative-predictors"><i class="fa fa-check"></i><b>23.1</b> Statistical Model for A Quantitative Response and Quantitative Predictor(s)</a><ul>
<li class="chapter" data-level="23.1.1" data-path="Regmodel.html"><a href="Regmodel.html#including-multiple-precitors"><i class="fa fa-check"></i><b>23.1.1</b> Including Multiple Precitors</a></li>
<li class="chapter" data-level="23.1.2" data-path="Regmodel.html"><a href="Regmodel.html#including-categorical-predictors"><i class="fa fa-check"></i><b>23.1.2</b> Including Categorical Predictors</a></li>
<li class="chapter" data-level="23.1.3" data-path="Regmodel.html"><a href="Regmodel.html#general-model-formulation"><i class="fa fa-check"></i><b>23.1.3</b> General Model Formulation</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="Regmodel.html"><a href="Regmodel.html#estimating-the-parameters"><i class="fa fa-check"></i><b>23.2</b> Estimating the Parameters</a></li>
<li class="chapter" data-level="23.3" data-path="Regmodel.html"><a href="Regmodel.html#embedding-our-questions-into-a-statistical-framework"><i class="fa fa-check"></i><b>23.3</b> Embedding Our Questions into a Statistical Framework</a></li>
<li class="chapter" data-level="23.4" data-path="Regmodel.html"><a href="Regmodel.html#recap-2"><i class="fa fa-check"></i><b>23.4</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="Regconditions.html"><a href="Regconditions.html"><i class="fa fa-check"></i><b>24</b> Conditions on the Error Term of a Regression Model</a><ul>
<li class="chapter" data-level="24.1" data-path="Regconditions.html"><a href="Regconditions.html#classical-regression-model"><i class="fa fa-check"></i><b>24.1</b> Classical Regression Model</a></li>
<li class="chapter" data-level="24.2" data-path="Regconditions.html"><a href="Regconditions.html#interpretation-of-the-parameters"><i class="fa fa-check"></i><b>24.2</b> Interpretation of the Parameters</a></li>
<li class="chapter" data-level="24.3" data-path="Regconditions.html"><a href="Regconditions.html#addressing-confounding-through-our-interpretation"><i class="fa fa-check"></i><b>24.3</b> Addressing Confounding through our Interpretation</a></li>
<li class="chapter" data-level="24.4" data-path="Regconditions.html"><a href="Regconditions.html#empirical-model-for-the-sampling-distribution"><i class="fa fa-check"></i><b>24.4</b> Empirical Model for the Sampling Distribution</a></li>
<li class="chapter" data-level="24.5" data-path="Regconditions.html"><a href="Regconditions.html#recap-3"><i class="fa fa-check"></i><b>24.5</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="Regassessment.html"><a href="Regassessment.html"><i class="fa fa-check"></i><b>25</b> Assessing Modeling Assumptions</a><ul>
<li class="chapter" data-level="25.1" data-path="Regassessment.html"><a href="Regassessment.html#computing-residuals"><i class="fa fa-check"></i><b>25.1</b> Computing Residuals</a></li>
<li class="chapter" data-level="25.2" data-path="Regassessment.html"><a href="Regassessment.html#assessing-mean-0"><i class="fa fa-check"></i><b>25.2</b> Assessing Mean 0</a></li>
<li class="chapter" data-level="25.3" data-path="Regassessment.html"><a href="Regassessment.html#assessing-independence-1"><i class="fa fa-check"></i><b>25.3</b> Assessing Independence</a></li>
<li class="chapter" data-level="25.4" data-path="Regassessment.html"><a href="Regassessment.html#assessing-homoskedasticity-1"><i class="fa fa-check"></i><b>25.4</b> Assessing Homoskedasticity</a></li>
<li class="chapter" data-level="25.5" data-path="Regassessment.html"><a href="Regassessment.html#assessing-normality-1"><i class="fa fa-check"></i><b>25.5</b> Assessing Normality</a></li>
<li class="chapter" data-level="25.6" data-path="Regassessment.html"><a href="Regassessment.html#general-tips-for-assessing-assumptions-1"><i class="fa fa-check"></i><b>25.6</b> General Tips for Assessing Assumptions</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="Reginteractions.html"><a href="Reginteractions.html"><i class="fa fa-check"></i><b>26</b> Modifying an Effect</a><ul>
<li class="chapter" data-level="26.1" data-path="Reginteractions.html"><a href="Reginteractions.html#building-an-effect-modifier-into-the-model"><i class="fa fa-check"></i><b>26.1</b> Building an Effect-Modifier into the Model</a></li>
<li class="chapter" data-level="26.2" data-path="Reginteractions.html"><a href="Reginteractions.html#inference-for-effect-modifications"><i class="fa fa-check"></i><b>26.2</b> Inference for Effect Modifications</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="Regquality.html"><a href="Regquality.html"><i class="fa fa-check"></i><b>27</b> Quantifying the Quality of a Model Fit</a><ul>
<li class="chapter" data-level="27.1" data-path="Regquality.html"><a href="Regquality.html#partitioning-variability-1"><i class="fa fa-check"></i><b>27.1</b> Partitioning Variability</a></li>
<li class="chapter" data-level="27.2" data-path="Regquality.html"><a href="Regquality.html#r-squared"><i class="fa fa-check"></i><b>27.2</b> R-squared</a></li>
<li class="chapter" data-level="27.3" data-path="Regquality.html"><a href="Regquality.html#overfitting"><i class="fa fa-check"></i><b>27.3</b> Overfitting</a></li>
<li class="chapter" data-level="27.4" data-path="Regquality.html"><a href="Regquality.html#goal-of-modeling"><i class="fa fa-check"></i><b>27.4</b> Goal of Modeling</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="Regrecap.html"><a href="Regrecap.html"><i class="fa fa-check"></i><b>28</b> Puting it All Together</a><ul>
<li class="chapter" data-level="28.1" data-path="Regrecap.html"><a href="Regrecap.html#graphical-summary"><i class="fa fa-check"></i><b>28.1</b> Graphical Summary</a></li>
<li class="chapter" data-level="28.2" data-path="Regrecap.html"><a href="Regrecap.html#development-of-statistical-model"><i class="fa fa-check"></i><b>28.2</b> Development of Statistical Model</a></li>
<li class="chapter" data-level="28.3" data-path="Regrecap.html"><a href="Regrecap.html#assessment-of-conditions"><i class="fa fa-check"></i><b>28.3</b> Assessment of Conditions</a></li>
<li class="chapter" data-level="28.4" data-path="Regrecap.html"><a href="Regrecap.html#summary-of-model-fit"><i class="fa fa-check"></i><b>28.4</b> Summary of Model Fit</a></li>
</ul></li>
<li class="part"><span><b>IV Unit IV: Special Cases</b></span></li>
<li class="chapter" data-level="29" data-path="CaseBabies.html"><a href="CaseBabies.html"><i class="fa fa-check"></i><b>29</b> Case Study: Birth Weights of Babies</a></li>
<li class="chapter" data-level="30" data-path="OneMean.html"><a href="OneMean.html"><i class="fa fa-check"></i><b>30</b> Inference on the Mean of a Single Population (One-Sample t-Tests)</a><ul>
<li class="chapter" data-level="30.1" data-path="OneMean.html"><a href="OneMean.html#framing-the-question-1"><i class="fa fa-check"></i><b>30.1</b> Framing the Question</a></li>
<li class="chapter" data-level="30.2" data-path="OneMean.html"><a href="OneMean.html#classical-approach"><i class="fa fa-check"></i><b>30.2</b> Classical Approach</a></li>
<li class="chapter" data-level="30.3" data-path="OneMean.html"><a href="OneMean.html#connection-to-modeling"><i class="fa fa-check"></i><b>30.3</b> Connection to Modeling</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="TwoMeans.html"><a href="TwoMeans.html"><i class="fa fa-check"></i><b>31</b> Comparing the Means of Two Independent Groups (Two-Sample t-Tests)</a><ul>
<li class="chapter" data-level="31.1" data-path="TwoMeans.html"><a href="TwoMeans.html#framing-the-question-2"><i class="fa fa-check"></i><b>31.1</b> Framing the Question</a></li>
<li class="chapter" data-level="31.2" data-path="TwoMeans.html"><a href="TwoMeans.html#classical-approach-1"><i class="fa fa-check"></i><b>31.2</b> Classical Approach</a></li>
<li class="chapter" data-level="31.3" data-path="TwoMeans.html"><a href="TwoMeans.html#connection-to-modeling-1"><i class="fa fa-check"></i><b>31.3</b> Connection to Modeling</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Foundations for Engineers and Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="SamplingDistns" class="section level1">
<h1><span class="header-section-number">6</span> Assessing the Evidence (Quantifying the Variability in Estimates)</h1>
<p>Again, the goal of statistical inference is to use the sample as a snapshot of the underlying population (Figure <a href="SamplingDistns.html#fig:samplingdistns-statistical-process">6.1</a>). There are generally three reasons people distrust this process:</p>
<ol style="list-style-type: decimal">
<li>Fear that the sample does not represent what is going on in the population.</li>
<li>Fear that we cannot make a conclusion with a sample of size <span class="math inline">\(n\)</span> (wanting more data).</li>
<li>Fear that one study is not enough to make a conclusion.</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:samplingdistns-statistical-process"></span>
<img src="images/Basics-Stat-Process.jpg" alt="Illustration of the statistical process (reprinted from Chapter 1)." width="80%" />
<p class="caption">
Figure 6.1: Illustration of the statistical process (reprinted from Chapter 1).
</p>
</div>
<p>We have already tackled the first reason in Chapter @ref(#Data); if we are to trust statistical results, we must collect data that is representative of the underlying population. The second and third fears above are tied together, though maybe not obviously. Before launching into a slightly more formal discussion, consider the following thought experiment.</p>

<div class="example">
<span id="exm:samplingdistns-free-throws" class="example"><strong>Example 6.1  (Free Throws)  </strong></span>Your friends Dave lives for his Wednesday âpick-upâ basketball games at the gym. One afternoon, while waiting for a few more players to arrive Dave shoots 10 free throws, of which he makes 3.
</div>
<p></p>
<p>I imagine no one is ready to claim <em>definitively</em> that Dave has a 30% success rate from the free throw line. So, what can we say? Well, if this set of 10 free throws is representative of Daveâs free throw performance, then we would say that 30% is an estimate for his success rate; that is, the statistic 30% is a good guess at the unknown parameter (overall success rate). There are two ways we might impove our confidence in this estimate. First, we might consider a larger sample size (make Dave shoot more free throws).</p>

<div class="example">
<span id="exm:samplingdistns-free-throws2" class="example"><strong>Example 6.2  (Free Throws (cont.))  </strong></span>Joe has also been waiting for a few more players to arrive; however, Joe shoots 100 free throws (clearly he has more time on his hands) of which he makes 30.
</div>
<p></p>
<p>Again, we probably wouldnât claim <em>definitively</em> that Joe has a 30% success rate from the free throw line. But, assuming this set of 100 free throws is representative of his overall performance, then we would be more confident in our guess for Joeâs overall performance compared with our guess for Daveâs. The more shots we observe, the more confidence we have in our estimate. This idea is known as the <strong>Law of Large Numbers</strong>.</p>

<div class="definition">
<span id="def:defn-lln" class="definition"><strong>Definition 6.1  (Law of Large Numbers)  </strong></span>For our purposes, this essentially says that as a sample size gets really large, a statistic will become arbitrarily close (extremely good guess) of the parameter it estimates.
</div>
<p></p>
<p>Unfortunately, we may not be able to take a really large sample. It is probably not feasible to have Dave or Joe shoot thousands of free throws, for example. Our goal then becomes to somehow quantify the confidence we have in our estimates <em>given the sample size we have available</em>. That is, given that we only saw Dave shoot 10 free throws, can we quantify our confidence in that 30% estimate of his free throw success? Our âconfidenceâ in an estimate is tied to the estimateâs repeatability â âif we were to repeat the study, how much would we expect our estimate to change?â This gets at the last fear; we know that if we repeat a study, the results will change. Our job is to quantify (keeping the sample size in mind) the degree to which the results will change. That is, we need to quantify the <em>variability</em> in the estimate across repeated studies (known as sampling variability; we told you statistics was all about variability). This is known as a <strong>sampling distribution</strong>.</p>

<div class="definition">
<span id="def:defn-sampling-distribution" class="definition"><strong>Definition 6.2  (Sampling Distribution)  </strong></span>The distribution of a <em>statistic</em> across repeated samples.
</div>
<p></p>
<p>This is perhaps the most important of the <em>Distributional Quartet</em>; it is the holy grail of statistical inference. Once we have the sampling distribution, inference is straight-forward.</p>

<div class="rmdfivefund">
<strong>Fundamental Idea IV</strong>: Variability is inherent in any process, and as a result, our estimates are subject to sampling variability. However, these estimates often vary across samples in a predictable way; that is, they have a distribution that can be modeled.
</div>
<p></p>
<div id="conceptualizing-the-sampling-distribution" class="section level2">
<h2><span class="header-section-number">6.1</span> Conceptualizing the Sampling Distribution</h2>
<p>The sampling distribution of a statistic is one of the most fundamental, and yet one of the most abstract, concepts in statistics. Itâs name is even confusing; the âdistribution of the sampleâ (Definition <a href="Summaries.html#def:defn-distribution-sample">5.6</a>) and the âsampling distributionâ (Definition <a href="SamplingDistns.html#def:defn-sampling-distribution">6.2</a>) are two different things. In this section, we develop the idea of a sampling distribution; then, we turn toward actually constructing it.</p>
<p>For the <a href="#DeepwaterCase">Deepwater Horizon Case Study</a>, consider the following question:</p>
<blockquote>
<p>What proportion of volunteers assigned to clean wildlife will develop adverse respiratory symptoms?</p>
</blockquote>
<p>In the sample, we observed 15 out of 54 such volunteers (27.8% or a proportion of 0.278). This proportion is a good estimate of the rate of adverse symptoms in the population (assuming the sample is representative, of course). Now, imagine randomly selecting 54 new volunteers from the population (repeating the study). We could determine what fraction of volunteers in this new sample experienced adverse symptoms, expecting this value to be a bit different than what we obtained in the first sample. Since this second sample is also representative, it provides a good estimate of the parameter. Now, we could take a third random sample of 54 volunteers and compute the fraction in this third sample which experienced adverse symptoms. This third sample also provides a good (and potentially unique) estimate of the parameter. We could continue this process <span class="math inline">\(m\)</span> times, for some large number <span class="math inline">\(m\)</span>. This process is illustrated in Figure <a href="SamplingDistns.html#fig:samplingdistns-sampling-distribution">6.2</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:samplingdistns-sampling-distribution"></span>
<img src="images/SamplingDistns-Sampling-Distribution.jpg" alt="Illustration of repeatedly sampling from a population." width="80%" />
<p class="caption">
Figure 6.2: Illustration of repeatedly sampling from a population.
</p>
</div>
<p>Consider what we are describing. With each representative sample, we have constructed an estimate of the parameter. What we have kept from each repetion is <em>not</em> the values of the variables themselves (whether the volunteers experienced adverse respiratory symptoms) but rather we have retained the <em>statistic</em> from each of <span class="math inline">\(m\)</span> whole new studies. So, which of these <span class="math inline">\(m\)</span> estimates do we trust? All of them. Since each sample is representative of the population, each estimate is a good (not perfect) estimate of the parameter. Since we have all these estimates, we could think about pooling the information from all of them; describing the way in which they change from one sample to another is the sampling distribution.</p>
<p>Notice that the sampling distribution is not describing a variable, it is describing a <em>statistic</em>. In order to construct a sampling distribution, we would go through the following steps:</p>
<ol style="list-style-type: decimal">
<li>Take a sample; record variables of interest.</li>
<li>Compute the statistic which estimates the parameter.</li>
<li>Repeat steps 1 and 2 a large number of times.</li>
<li>Examine the statistics collected.</li>
</ol>
<p>So, the sampling distribution is not a plot of the raw values of a variable on individual subjects but a plot of statistics which summarize entire samples. That is, the unit of observation has changed. While a sample consists of individual subjects from the population, the sampling distribution consists of individual samples from the population.</p>

<div class="rmdtip">
Re-read the description of a sampling distribution several times, and return to it often as you read through the text. It takes a while for this to sink in, but if you truly grasp this one concept, the remainder of statistical inference becomes much more accessible.
</div>
<p></p>
</div>
<div id="example-of-a-sampling-distribution" class="section level2">
<h2><span class="header-section-number">6.2</span> Example of a Sampling Distribution</h2>
<p>Since this idea is so critical to grasping statistical inference, we are going to walk through the process of generating a sampling distribution for a known data generating process.</p>

<div class="example">
<p><span id="exm:samplingdistns-dice" class="example"><strong>Example 6.3  (Dice Experiment)  </strong></span>Consider an ordinary six-sided die; we are interested in the proportion of times that rolling the die will result in a 1. Putting this in the language of the statistics, we have the following:</p>
<ul>
<li>The <em>population</em> of interest is all rolls of the die. Notice that this population is infinitely large as we could roll the die forever.</li>
<li>The <em>variable</em> is the resulting value from the roll. Since this can take on only one of six values, this is a categorical variable.</li>
<li>The <em>parameter</em> of interest is the proportion of rolls that result in a 1.</li>
</ul>
Our goal is to construct the sampling distribution of the proportion of rolls that result in a 1 when the die is rolled 20 times.
</div>
<p></p>
<p>What makes this example unique is that we know the value of the parameter. Because of the physical properties of a die, we know that the probability a roll results in a 1 is <span class="math inline">\(\theta = 1/6\)</span>. So, statistical inference is not needed here. This example simply provides a simple vehicle for studying sampling distributions. Going back to the steps for creating a sampling distribution described in the previous section, we have the following steps:</p>
<ol style="list-style-type: decimal">
<li>Roll a die 20 times, each time recording the resulting value.</li>
<li>Compute the proportion of times (out of the 20) the resulting value was a 1.</li>
<li>Repeat steps 1 and 2 a large number of times (letâs say 500).</li>
<li>Plot the resulting values; there should be 500 proportions that we are keeping.</li>
</ol>
<p>Notice that we are actually rolling a die 10000 times (20 rolls repeated 500 times); we only keep 500 values (one proportion for each set of 20 rolls). This is something you could physically do at home. For example, the first sample might look like that in Figure <a href="SamplingDistns.html#fig:samplingdistns-dice-example">6.3</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:samplingdistns-dice-example"></span>
<img src="images/SamplingDistns-Dice-Example.jpg" alt="Potential sample of rolling a cie 20 times." width="80%" />
<p class="caption">
Figure 6.3: Potential sample of rolling a cie 20 times.
</p>
</div>
<p>For this particular sample, the proportion in the sample (our statistic of interest) would be 0.25 (<span class="math inline">\(5/20\)</span>). That is the value we would record. We then repeat this 499 more times. You could try a few out yourself using <a href="https://www.random.org/dice/?num=20">an online simulator</a>. Figure <a href="SamplingDistns.html#fig:samplingdistns-dice-dotplot">6.4</a> shows the resulting proportions for 500 samples of size 20 each.</p>
<div class="figure" style="text-align: center"><span id="fig:samplingdistns-dice-dotplot"></span>
<img src="images/samplingdistns-dice-dotplot-1.png" alt="Sampling distribution for the proportion of 20 dice rolls which result in a 1.  The distribution is based on repeating the sampling process 500 times." width="80%" />
<p class="caption">
Figure 6.4: Sampling distribution for the proportion of 20 dice rolls which result in a 1. The distribution is based on repeating the sampling process 500 times.
</p>
</div>
<p>With modern computing power, there is no need to restrain ourselves to repeating the study 500 times. A simple computer program could replicate rolling the dice thousands of times. Figure <a href="SamplingDistns.html#fig:samplingdistns-dice-histogram">6.5</a> is the sampling distribution for the proportion of rolls that result in a 1 based on a sample of size 20 repeating the study 50000 times.</p>
<div class="figure" style="text-align: center"><span id="fig:samplingdistns-dice-histogram"></span>
<img src="images/samplingdistns-dice-histogram-1.png" alt="Sampling distribution for the proportion of 20 dice rolls which result in a 1.  The distribution is based on repeating the sampling process 50000 times." width="80%" />
<p class="caption">
Figure 6.5: Sampling distribution for the proportion of 20 dice rolls which result in a 1. The distribution is based on repeating the sampling process 50000 times.
</p>
</div>
<p>Notice that the sampling distribution is centered around the true value of the parameter (<span class="math inline">\(\theta = 1/6\)</span>). In general, the sampling distribution of statistics, when taken from a random sample, center on the true value of the parameter. This is the unbiased nature of the data coming out; random samples are representative of the population. Similarly, note that while no one sample (remember, each value in the distribution represents a statistic from a sample of 20 values) is perfect, no samples produce values which are far from the true parameter. That is, a representative sample may not be perfect, but it will give a <em>reasonable</em> estimate of the parameter. Notice that these properties hold even though we had a relatively small sample size (<span class="math inline">\(n = 20\)</span> coin flips).</p>

<div class="rmdkeyidea">
The size of the sample is not as important as whether it is representative. A small representative sample is better for making inference than a large sample which is biased.
</div>
<p></p>
<p>One of the most useful things about the sampling distribution is that it gives us an idea of how much we might expect our statistic to change from one sample to another. Based on Figure <a href="SamplingDistns.html#fig:samplingdistns-dice-histogram">6.5</a>, we could say that if we roll a die 20 times, the proportion of rolls which result in a 1 is most likely to be between 0.05 and 0.30 (so somewhere between 1 and 6 ones out of the 20 rolls). It would be <em>extremely</em> rare to have 12 of the 20 rolls result in a 1 (notice how small the bar is on the 0.6 proportion). The sampling distribution is therefore giving us an idea of the variability in our statistic.</p>
<p>Remember, our goal was to account for the variability in the statistic (how much it changes from one sample to another) <em>while accounting for the sample size</em>. How is this done? When forming the sampling distribution, we repeated the study. For each replication, we obtained a new sample that <em>had the same size as the original</em>. So, the sample size is baked into the sampling distribution. To see the impact of taking a larger sample, consider rolling a six-sided die 60 times instead of 20 times. When we build the sampling distribution, each replication will then involve repeating the process with 40 new rolls. Figure <a href="SamplingDistns.html#fig:samplingdistns-dice-histogram2">6.6</a> shows the sampling distribution of the proportion of 60 rolls which result in a 1 using 50000 replications. Notice that the distribution is still centered on the true parameter <span class="math inline">\(\theta = 1/6\)</span>. The primary difference between this figure and the last is that when we increased the sample size, the sampling distribution narrowed.</p>
<div class="figure" style="text-align: center"><span id="fig:samplingdistns-dice-histogram2"></span>
<img src="images/samplingdistns-dice-histogram2-1.png" alt="Sampling distribution for the proportion of 60 dice rolls which result in a 1.  The distribution is based on repeating the sampling process 50000 times." width="80%" />
<p class="caption">
Figure 6.6: Sampling distribution for the proportion of 60 dice rolls which result in a 1. The distribution is based on repeating the sampling process 50000 times.
</p>
</div>
<p>We all have this intuition that âmore data is better.â In truth, we should say âmore <em>good</em> data is better.â By âbetter,â we mean that the statistic is less variable. Notice that we have to be careful here. We are not saying that the <em>sample</em> has less variability; we are saying the <em>statistic</em> has less variability. That is, we are more confident in our estimate because we do not expect it to change as much from one sample to the next. From Figure <a href="SamplingDistns.html#fig:samplingdistns-dice-histogram2">6.6</a>, we have that if we roll the die 60 times, we expect the proportion of 1âs to be somewhere between 0.1 and 0.25 (somewhere between 6 and 15 ones out of the 60 show up). The proportion is varying much less from one sample to the next.</p>

<div class="rmdkeyidea">
Larger samples result in <em>statistics</em> which are less variable. This shows itself in the sense that the sampling distribuiton is narrower.
</div>
<p></p>

<div class="rmdtip">
Students often believe that a large sample reduces the variability in the data. That is not true; a large sample reduces the variability in the <em>statistic</em>.
</div>
<p></p>
</div>
<div id="modeling-the-sampling-distribution" class="section level2">
<h2><span class="header-section-number">6.3</span> Modeling the Sampling Distribution</h2>
<p>Letâs return to the <a href="CaseDeepwater.html#CaseDeepwater">Deepwater Horizon Case Study</a>. In particular, suppose we are tyring to address the following question:</p>
<blockquote>
<p>What proportion of volunteers assigned to clean wildlife will develop adverse respiratory symptoms?</p>
</blockquote>
<p>We have an estimate for this proportion (<span class="math inline">\(p = 0.278\)</span>) based on the observed sample. Based on the discussion in the previous section, we know the sampling distribution of this proportion can help us quantify the variability in the estimate. Figure <a href="SamplingDistns.html#fig:samplingdistns-deepwater-histogram">6.7</a> represents the sampling distribution of this proportion. From the graphic, we would not expect the proportion of volunteers who experience adverse respiratory symptoms to move much beyond 0.15 and 0.4 if we were to repeat the study; it would almost certainly not move beyond 0.1 and 0.5 if we were to repeat the study.</p>
<div class="figure" style="text-align: center"><span id="fig:samplingdistns-deepwater-histogram"></span>
<img src="images/samplingdistns-deepwater-histogram-1.png" alt="Sampling distribution for the proportion of volunteers assigned to wildlife who will develop adverse symptoms based on a sample of 54 volunteers." width="80%" />
<p class="caption">
Figure 6.7: Sampling distribution for the proportion of volunteers assigned to wildlife who will develop adverse symptoms based on a sample of 54 volunteers.
</p>
</div>
<p>Now, you might ask âwait, where did this sampling distribution come from? There is no way you actually repeated the study 50000 times, right?â Right. In the previous section, we described building the sampling distribution through repeated sampling. But, in practice, this is never practical; if it were, we would have just conducted a bigger sample to begin with. Generally, cost is the limiting factor in choosing a sample size; so, we only have a limited set of data to work with. So, the sampling distribution is critical to making inference, but we cannot take multiple samples to make it. Where does that leave us? The answerâ¦modeling. Our goal is to construct a model of the sampling distribution that we can use to make inference.</p>
<p>There are three general techniques for modeling the sampling distribution of a statistic:</p>
<ol style="list-style-type: decimal">
<li>Build an empirical model.</li>
<li>Build an analytical model using probability theory.</li>
<li>Build an analytical model appealing to approximations.</li>
</ol>
<p>We will focus on the first approach; the latter two approaches are discussed in the last unit of the text. The idea in constructing an empirical model is to mimic the discussion above regarding the construction of a sampling distribution. Our description references Figure <a href="SamplingDistns.html#fig:samplingdistns-bootstrap">6.8</a> often. We are limited by our resources; because of time and money constraints, we cannot resample from the population (crossed off resamples). So, we pretend for a moment that our original sample (colored in green in the figure) is the population for a moment. Our idea is to randomly sample from this original sample, creating a <em>bootstrap resample</em> (colored in orange in the figure). Forgive the non-technical terms here, but since the orange âblobâ is a random sample from the green âblob,â then it is representative of the green blob. Therefore, if we construct an estimate <span class="math inline">\(\widehat{\theta}^*\)</span> from the orange blob (the star denotes a statistic from a resample), then it should be close to the statistic <span class="math inline">\(\widehat{\theta}\)</span> from the green blob; but, since this green blob is representative of the population, <span class="math inline">\(\widehat{\theta}\)</span> should be close to the true parameter <span class="math inline">\(\theta\)</span>. Therefore, we have that <span class="math display">\[
\widehat{\theta}^* \approx \widehat{\theta} \approx \theta \Rightarrow \widehat{\theta}^* \approx \theta
\]</span></p>
<p>That is, the bootstrap resamples produce statistics which are good estimates of the parameter from the underlying population. The benefit here is that the bootstrap resamples are constructed in the computer. And, given todayâs computing power, we are not limited by time or money (10000 bootstrap resamples can often be taken in a matter of seconds). If you want to see this process in action, we encourage you to check out the free online app located at <a href="http://www.lock5stat.com/StatKey/bootstrap_1_cat/bootstrap_1_cat.html" class="uri">http://www.lock5stat.com/StatKey/bootstrap_1_cat/bootstrap_1_cat.html</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:samplingdistns-bootstrap"></span>
<img src="images/SamplingDistns-Bootstrap.jpg" alt="Illustration of modeling the sampling distribution via the Bootstrap." width="80%" />
<p class="caption">
Figure 6.8: Illustration of modeling the sampling distribution via the Bootstrap.
</p>
</div>
<p>Again, the idea is to mimic in the computer the resampling that we were unable to do in real life. This process is known as the <strong>bootstrap</strong> procedure.</p>

<div class="definition">
<span id="def:defn-bootstrap" class="definition"><strong>Definition 6.3  (Bootstrap)  </strong></span>A method of modeling the sampling distribution by repeatedly resampling from the original data.
</div>
<p></p>
<p>A couple of notes on the actual implementation of a bootstrap procedure:</p>
<ol style="list-style-type: decimal">
<li>Each resample is the same size as the original sample.</li>
<li>Each resample is taken <em>with replacement</em>; that means that values from the original sample can show up multiple times. This is like âcatch and releaseâ fishing.</li>
<li>Typically, between 3000 and 10000 bootstrap resamples are taken.</li>
</ol>
<p>We will avoid actual computation throughout the text, but several resources are available for implementing the bootstrap procedure (and its many variants) in various computer programming languages and software packages.</p>

<div class="rmdtip">
Students often believe that the bootstrap âcreates more data.â This is not true. Instead, the boostrap resamples from the existing data. This highlights the need to have a representative sample when performing analysis.
</div>
<p></p>
<p>As an example, for the <a href="CaseDeepwater.html#CaseDeepwater">Deepwater Horizon Case Study</a>, we performed the following steps to create Figure <a href="SamplingDistns.html#fig:samplingdistns-deepwater-histogram">6.7</a>:</p>
<ol style="list-style-type: decimal">
<li>Select 54 volunteers at random (with replacement) from the original sample of 54 volunteers who had been assigned to clean wildlife.</li>
<li>For our resample, we computed the proportion of those individuals who had experienced adverse respiratory symptoms.</li>
<li>We repeated steps 1 and 2 several thousand times, retaining the bootstrap statistics from each bootstrap resample.</li>
<li>We plotted the distribution of the bootstrap statistics.</li>
</ol>
</div>
<div id="using-a-model-for-the-sampling-distributions-confidence-intervals" class="section level2">
<h2><span class="header-section-number">6.4</span> Using a Model for the Sampling Distributions (Confidence Intervals)</h2>
<p>From Figure <a href="SamplingDistns.html#fig:samplingdistns-deepwater-histogram">6.7</a>, we observed that we would not expect the proportion of volunteers who had experienced adverse symptoms to move much beyond 0.15 to 0.4 if we were to repeat the study. How does this help us in performing inference? Remember that each value in the bootstrap model for the sampling distribution is an estimate of the underlying parameter. So, we can think of the above model as showing us what good estimates of the parameter look like. Another way of saying it: the model for the sampling distribution shows us the <em>reasonable</em> (or <em>plausbile</em>) values of the parameter. Here, by âreasonable,â we mean values of the parameter for which the data is <em>consistent</em>. Consider the following statements (which are equivalent):</p>
<ul>
<li>Based on our sample of 54 volunteers, it is reasonable that the proportion of volunteers assigned to clean wildlife who would experience adverse respiratory symptoms is between 0.15 and 0.4.</li>
<li>Our sample of 54 volunteers is consistent with between 15% and 40% of all volunteers assigned to clean wildlife experiencing adverse respiratory symptoms.</li>
</ul>
<p>We have just conducted inference for âestimationâ type questions. We are able to provide an estimate for the parameter which acknowledges that the data is not perfect and there is variability in sampling procedures. That variability incorporated itself into constructing an estimate that is an interval instead of a single point.</p>
<p>The above interval was chosen arbitrarily by just looking at the sampling distribution and capturing the peak of the distribution. If we want to be more formal, we might try to capture the middle 95% of values. This is known as a <strong>confidence interval</strong>.</p>

<div class="definition">
<span id="def:defn-confidence-interval" class="definition"><strong>Definition 6.4  (Confidence Interval)  </strong></span>An interval (range of values) estimate of a parameter that incorporates the variability in the statistic. A k% confidence interval will contain the parameter of interest in k% of repeated studies.
</div>
<p></p>
<p>If we were to capture the middle 95% of statistics, a 95% confidence interval, we would obtain an interval of (0.167, 0.407), as shown in Figure <a href="SamplingDistns.html#fig:samplingdistns-deepwater-95ci">6.9</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:samplingdistns-deepwater-95ci"></span>
<img src="images/samplingdistns-deepwater-95ci-1.png" alt="Construction of a 95% confidence interval via bootstrapping for the proportion of volunteers assigned to wildlife who will develop adverse symptoms based on a sample of 54 volunteers." width="80%" />
<p class="caption">
Figure 6.9: Construction of a 95% confidence interval via bootstrapping for the proportion of volunteers assigned to wildlife who will develop adverse symptoms based on a sample of 54 volunteers.
</p>
</div>
<p>Confidence intervals are often misinterpreted; this comes from their dependence on repeated sampling. When thinking about confidence intervals, think about playing a game of ring toss: you toss a ring in hopes of landing on top of a target. The target is the parameter characterizing the population. The confidence interval is like a ring. Since the confidence interval is constructed from a model of the sampling distribution, it changes with each sample; that is, the confidence interval itself is a statistic. Just like in ring toss, the ring moves with each toss, the confidence interval moves with each sample. However, the target stays fixed. Because of this, the following interpretations are <em>incorrect</em>:</p>
<ul>
<li>There is a 95% chance that the proportion of volunteers assigned to clean wildlife who will experience adverse symptoms is between 0.167 and 0.407.</li>
<li>95% of volunteers assigned to clean wildlife in our sample had a value between 0.167 and 0.407.</li>
</ul>
<p>The first statement is incorrect because it treats the parameter as the thing that is moving. Once the data has been collected, the confidence interval is a fixed quantity; neither the estimate or the parameter is moving; so, there is no probability left. Again, think about tossing a ring; once the ring is tossed, you either captured the target or you did not. There is no âI captured the target with 95% probability.â</p>
<p>The second statement is absurd in this case. A volunteer either had respiratory symptoms or they did not; so, saying they had a value between 0.167 and 0.407 is ridiculous. However, this is a common misconception with confidence intervals. They are describing reasonable values of the parameter, not values of the variable in the sample or population. We recommend sticking to interpreting a confidence interval as specifying reasonable values for the parameter.</p>

<div class="rmdtip">
Confidence intervals <em>do not</em> provide a probability that the parameter is inside. Nor do they tell you anything about the individual values in a sample or population. They describe reasonable values of the parameter.
</div>
<p></p>

<div class="rmdkeyidea">
Confidence intervals specify <em>reaonable</em> values of the parameter based on the data observed.
</div>
<p></p>
<p>This is a difficult concept to wrap our heads around; it seems natural to associate the percentage with the values we have obtained. However, our confidence is in the <em>process</em>, not the resulting interval itself. That is, a 95% confidence intervals work 95% of the time; however, this statement is about the process of constructing confidence intervals. Once we have computed a convidence interval, it either has worked or not; the problem is of course, that since we do not know the parameter, we will never know if it worked or not. For this reason, we prefer the interpretation of a confidence interval which avoids these subtleties: a confidence interval specifies the reasonable values of the parameter. The percentage (95% vs 99% for example) then just specifies what we mean by âreasonable.â</p>
<p>It may seem like a good idea to make a 100% confidence interval to be sure we always capture the parameter. But, such intervals are not helpful in practice. For example, a 100% confidence interval for the proportion of volunteers experiencing adverse symptoms would be (0, 1). But, this is useless; it essentially says that the proportion has to be a number between 0 and 1, but we already knew that. Therefore, we must balance the confidence we desire with the amount of information the interval conveys.</p>

<div class="rmdtip">
If you want both a high level of confidence but also a narrow interval, increase the sample size. As the sample size increases, the variability in the statistic decreases leading to a narrower interval.
</div>
<p></p>

<div class="rmdtip">
95% confidence intervals are the most common in practice; however, 90%, 98%, and 99% intervals are also used. It is extremely rare to use less than a 90% CI.
</div>
<p></p>
</div>
<div id="bringing-it-all-together" class="section level2">
<h2><span class="header-section-number">6.5</span> Bringing it All Together</h2>
<p>Consider the following question:</p>
<blockquote>
<p>Is there evidence that more than 1 in 5 volunteers assigned to clean wildlife will develop adverse respiratory symptoms?</p>
</blockquote>
<p>Letâs answer this question using a confidence interval. Based on the data obtained, we found that the 95% confidence interval (CI) for the proportion of volunteers experiencing adverse symptoms to be (0.167, 0.407). Is this data consistent with more than 1 in 5 volunteers developing adverse symptoms? Yes, since there are proportions within this interval which are larger than 0.2. But, <em>consistency</em> is not the same as <em>evidence</em>; remember, evidence is the idea of âbeyond a reasonable doubt.â After all, is this data <em>consistent</em> with less than 1 in 5 volunteers developing adverse symptoms? Yes, since there are proportions within this interval which are less than 0.2.</p>
<p>Confidence intervals specify reasonable values â those values of the parameter which are consistent with the data. This data is then consistent with proportions that are both less than 0.2 and greater than 0.2. So, what can we say then? We can say that there is <em>not</em> evidence that more than 1 in 5 volunteers assigned to clean wildlife will develop adverse respiratory symptoms, but the data is consistent with this claim.</p>
<p>More, we can say that there <em>is evidence</em> that the proportion of volunteers who will develop symptoms is less than 0.5; further, the proportion of volunteers who will develop symptoms is larger than 0.1. That is, the data provides evidence that more than 10% of volunteers will develop adverse symptoms, but this percentage will not be larger than 50%. How do we know? Because values less than 10% are not reasonble values of the parameter based on the 95% CI. Values like 0.1 are outside of the confidence interval and are therefore not reasonable. Similarly, values above 0.5 are outside the confidence interval and are therefore not reasonable.</p>
<p>The power of a model for the sampling distribution is that it allows us to determine which values of a parameter are reasonable and which values are not.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="Summaries.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="NullDistns.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"download": ["MA223CourseNotes.pdf"],
"toc": {
"collapse": "section"
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
