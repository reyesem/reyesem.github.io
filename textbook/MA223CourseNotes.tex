\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Statistical Foundations for Engineers and Scientists},
            pdfauthor={Eric M Reyes},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{Statistical Foundations for Engineers and Scientists}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Eric M Reyes}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{Last Updated: 2017-09-06}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% File: style.tex
% Description: Create styles for custom block elements.

\newenvironment{rmdkeyidea}%
  {\begin{center}
    \begin{tabular}{|p{0.9\textwidth}|}
    \hline\\
    \textbf{Key Idea: }
  }
  {
  \\\\\hline
  \end{tabular}
  \end{center}
  }

\newenvironment{rmdfivefund}%
  {\begin{center}
    \begin{tabular}{|p{0.9\textwidth}|}
    \hline\\
    \textbf{Fundamental Idea: }
  }
  {
  \\\\\hline
  \end{tabular}
  \end{center}
  }

\newenvironment{rmdwarning}%
  {\begin{center}
    \begin{tabular}{|p{0.9\textwidth}|}
    \hline\\
    \textbf{Warning: }
  }
  {
  \\\\\hline
  \end{tabular}
  \end{center}
  }

\newenvironment{rmdtip}%
  {\begin{center}
    \begin{tabular}{|p{0.9\textwidth}|}
    \hline\\
    \textbf{Tip: }
  }
  {
  \\\\\hline
  \end{tabular}
  \end{center}
  }

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\let\BeginKnitrBlock\begin \let\EndKnitrBlock\end
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\part{Unit I: Language and Logic of
Inference}\label{part-unit-i-language-and-logic-of-inference}

\chapter{The Statistical Process}\label{Basics}

Is driving while texting as dangerous as driving while intoxicated? Is
there a relationship between a student's college grade point average and
their starting salary following graduation? If so, does that
relationship differ across academic departments? Regardless of your
future career path, you will eventually need to answer a question. The
discipline of statistics is about using data to address questions by
converting that data into valuable information.

\BeginKnitrBlock{rmdkeyidea}
Statistics is the discipline of converting data into information.
\EndKnitrBlock{rmdkeyidea}

It might be natural at this point to ask ``do I really need an entire
class about answering questions with data? Isn't this simple?''
Sometimes, it is simple; other times, it can be far from it. Let's
illustrate with the following example from Tintle et al.
(\protect\hyperlink{ref-Tintle2015}{2015}).

\BeginKnitrBlock{example}[Organ Donation]
\protect\hypertarget{ex:basics-organ-donation}{}{\label{ex:basics-organ-donation}
\iffalse (Organ Donation) \fi{} }Even though organ donations save lives,
recruiting organ donors is difficult. Interestingly, surveys show that
about 85\% of Americans approve of organ donation in principle and many
states offer a simple organ donor registration process when people apply
for a driver's license. However, only about 38\% of licensed drivers in
the United States are registered to be organ donors. Some people prefer
not to make an active decision about organ donation because the topic
can be unpleasant to think about. But perhaps phrasing the question
differently could affect people's willingness to become a donor.

Johnson and Goldstein (\protect\hyperlink{ref-Johnson2003}{2003})
recruited 161 participants for a study, published in the journal
\emph{Science}, to address the question of organ donor recruitment. The
participants were asked to imagine they had moved to a new state and
were applying for a driver's license. As part of this application, the
participants were to decide whether or not to become an organ donor.
Participants were presented with one of three different default choices:

\begin{itemize}
\tightlist
\item
  Some of the participants were forced to make a choice of becoming a
  donor or not, without being given a default option (the ``neutral''
  group).
\item
  Other participants were told that the default option was not to be a
  donor but that they could choose to become a donor if they wished (the
  ``opt-in'' group).
\item
  The remaining participants were told that the default option was to be
  a donor but that they could choose not to become a donor if they
  wished (the ``opt-out'' group).
\end{itemize}

The results of this study were 79\% agreeing to become donors in the
neutral group, 42\% for the opt-in group, and 82.0\% for the opt-out
group.
\EndKnitrBlock{example}

The results of the study are presented in Figure
\ref{fig:basics-organ-plot}. It seems obvious that using the ``opt-in''
strategy results in fewer people agreeing to organ donation. However,
does the ``opt-out'' strategy, in which people are by default declared
organ donors, result in more people agreeing to organ donation compared
to the ``neutral'' strategy? On the one hand, a higher percentage did
agree to organ donation under the ``opt-out'' (82\% compared to 79\%).
However, since this study involved only a subset of Americans, is this
enough evidence to claim the ``opt-out'' strategy is really superior
compared to the ``neutral'' strategy? The discipline of statistics
provides a framework for addressing such ambiguity.




\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/basics-organ-plot-1} 

}

\caption{Summary of the responses for the Organ
Donation Study described in Example \ref{ex:basics-organ-donation}.}\label{fig:basics-organ-plot}
\end{figure}

\section{Overview of Drawing
Inference}\label{overview-of-drawing-inference}

Let's begin by taking a step back and considering the big picture of how
data is turned into information. Every research question we pose, at its
heart, is trying to characterize a \textbf{population}, the group of
subjects of ultimate interest.

\BeginKnitrBlock{definition}[Population]
\protect\hypertarget{def:defn-population}{}{\label{def:defn-population}
\iffalse (Population) \fi{} }The collection of subjects we would like to
say something about.
\EndKnitrBlock{definition}

In the Organ Donation study, the researchers would like to say something
about Americans who are of the age to consent to organ donation; in
particular, they would like to quantify how likely it is that someone
from this group agrees to organ donation. Therefore, the population is
the all Americans who are of the age to consent to organ donation. The
subjects in a population need not be people; the population could just
as easily be a collection of screws, sheet metal\ldots{}whatever
characterizes the objects from which we would \emph{like to} obtain
measurements. We use the phrase ``like to'' because in reality, it is
often impossible (or impractical) to observe the entire population.
Instead, we make observations on a subset of the population; this
smaller group is known as the \textbf{sample}.

\BeginKnitrBlock{definition}[Sample]
\protect\hypertarget{def:defn-sample}{}{\label{def:defn-sample}
\iffalse (Sample) \fi{} }The collection of subjects for which we
actually obtain measurements (data).
\EndKnitrBlock{definition}

For each subject within the sample, we obtain a collection of
measurements forming our set of data. The goal of statistical modeling
is to use the sample (the group we actually observe) to say something
about the population of interest (the group we wish we had observed);
this process is known as \textbf{statistical inference}. This process is
illustrated in Figure \ref{fig:basics-statistical-process}.

\BeginKnitrBlock{definition}[Statistical Inference]
\protect\hypertarget{def:defn-inference}{}{\label{def:defn-inference}
\iffalse (Statistical Inference) \fi{} }Sometimes referred to as
``inference,'' the process of using a sample to characterize some aspect
of the population.
\EndKnitrBlock{definition}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/Basics-Stat-Process} 

}

\caption{Illustration of the statistical process.}\label{fig:basics-statistical-process}
\end{figure}

\section{Anatomy of a Dataset}\label{anatomy-of-a-dataset}

Once we have our sample, we take measurements on each of the subjects.
These measurements form the data. When we hear the word ``data,'' most
of us envision a large spreadsheet. In reality, data can take on many
forms --- spreadsheets, images, text files, unstructured text from a
Twitter feed, etc. Regardless of the form, all datasets contain
information for each subject in the sample; this information, the
various measurements, are called \textbf{variables}.

\BeginKnitrBlock{definition}[Variable]
\protect\hypertarget{def:defn-variable}{}{\label{def:defn-variable}
\iffalse (Variable) \fi{} }A measurement, or category, describing some
aspect of the subject.
\EndKnitrBlock{definition}

Variables come in one of two flavors. \textbf{Categorical} variables are
those which denote a grouping to which the subject belongs. Examples
include marital status, brand, and experimental treatment group.
\textbf{Numeric} variables are those which take on values for which
ordinary arithmetic (addition, multiplication) makes sense. Examples
include height, age of a product, and diameter. Note that sometimes
numeric values are used to represent the levels of a categorical
variable in a dataset; for example, 0 may indicate ``No'' and 1 may
indicate ``Yes'' for a variable capturing whether a person is a
registered organ donor. Therefore, just because a variable has a numeric
value does not make it a numeric variable; the key here is that numeric
variables are those for which arithmetic makes sense.

\BeginKnitrBlock{definition}[Categorical Variable]
\protect\hypertarget{def:defn-categorical}{}{\label{def:defn-categorical}
\iffalse (Categorical Variable) \fi{} }Also called a ``qualitative
variable,'' a measurement on a subject which denotes a grouping or
categorization.
\EndKnitrBlock{definition}

\BeginKnitrBlock{definition}[Numeric Variable]
\protect\hypertarget{def:defn-numeric}{}{\label{def:defn-numeric}
\iffalse (Numeric Variable) \fi{} }Also called a ``quantitative
variable,'' a measurement on a subject which takes on a numeric value
\emph{and} for which ordinary arithmetic makes sense.
\EndKnitrBlock{definition}

While it may be natural to think of a dataset as a spreadsheet, not all
spreadsheets are created equal. Here, we consider datasets which have
the following characteristics:

\begin{itemize}
\tightlist
\item
  Each column contains a unique variable.
\item
  Each record (row in the dataset) corresponds to a different
  observation of the variables.
\item
  If you have multiple datasets, they should include a column in the
  table that allows them to be linked (subject identifier).
\end{itemize}

These are characteristics of ``tidy data.'' Even unstructured data such
as images or Twitter feeds must be processed, often converted to tidy
data, prior to performing a statistical analysis. The above description
eliminates a common method of storing data in engineering ans scientific
disciplines --- storing each sample in a different column. To
illustrate, suppose we conduct a study comparing the lifetime (in hours)
of two brands of batteries. We measure the lifetime of five batteries of
Brand A and six of Brand B. It is common to see a dataset like that in
Table \ref{tab:basics-poor-dataset}; the problem here is that the first
record of the dataset contains information on two different
observations. We have the lifetime from a battery of Brand A in the same
row as the lifetime from a battery of Brand B.

\begin{table}

\caption{\label{tab:basics-poor-dataset}Example of a common data structure which does not represent tidy data.  Data is from a hypothetical study comparing battery lifetimes (hours).}
\centering
\begin{tabular}[t]{l|l}
\hline
Brand A & Brand B\\
\hline
8.3 & 8.4\\
\hline
5.1 & 8.6\\
\hline
3.3 & 3.8\\
\hline
5.3 & 4.1\\
\hline
5.7 & 4.5\\
\hline
 & 4.0\\
\hline
\end{tabular}
\end{table}

In order to adhere to the tidy structure, we can reformat this dataset
as illustrated in Table \ref{tab:basics-good-dataset}. Here, each record
represents a unique observation and each column is a different variable.
We have also added a unique identifier.

\begin{table}

\caption{\label{tab:basics-good-dataset}Example of a tidy dataset, a good way of storing data.  Data is from a hypothetical study comparing battery lifetimes (hours).}
\centering
\begin{tabular}[t]{r|l|r}
\hline
Battery & Brand & Lifetime\\
\hline
1 & A & 8.3\\
\hline
2 & A & 5.1\\
\hline
3 & A & 3.3\\
\hline
4 & A & 5.3\\
\hline
5 & A & 5.7\\
\hline
6 & B & 8.4\\
\hline
7 & B & 8.6\\
\hline
8 & B & 3.8\\
\hline
9 & B & 4.1\\
\hline
10 & B & 4.5\\
\hline
11 & B & 4.0\\
\hline
\end{tabular}
\end{table}

It may take some time to get used to storing data in this format, but it
makes analysis easier and avoids time spent managing the data later.

\section{A Note on Codebooks}\label{a-note-on-codebooks}

A dataset on its own is meaningless if you cannot understand what the
values represent. \emph{Before} you access a dataset, you should always
review any available \textbf{codebooks}.

\BeginKnitrBlock{definition}[Codebook]
\protect\hypertarget{def:defn-codebook}{}{\label{def:defn-codebook}
\iffalse (Codebook) \fi{} }Also called a ``data dictionary,'' these
provide complete information regarding the variables contained within a
dataset.
\EndKnitrBlock{definition}

Some codebooks are excellent, with detailed descriptions of how the
variables were collected and appropriate units. Other codebooks are not
as good, giving only an indication of what the variable represents.
Whenever you are working with previously collected data, reviewing a
codebook is the first step; and, you should be prepared to revisit the
codebook often throughout an analysis. When you are collecting your own
dataset, constructing a codebook is essential for others to make use of
your data.

\hypertarget{CaseDeepwater}{\chapter{Case Study: Health Effects of the
Deepwater Horizon Oil Spill}\label{CaseDeepwater}}

On the evening of April 20, 2010, the \emph{Deepwater Horizon}, an oil
drilling platform positioned off the coast of Louisiana, was engulfed in
flames as the result of an explosion. The drilling rig, leased and
operated by BP, had been tasked with drilling an oil well in water
nearly 5000 feet deep. Eleven personnel were killed in the explosion.
The following clip is from the initial coverage by the \emph{New York
Times}\footnote{\url{http://www.nytimes.com/2010/04/22/us/22rig.html?rref=collection\%2Ftimestopic\%2FOil\%20Spills\&action=click\&contentCollection=timestopics\&region=stream\&module=stream_unit\&version=search\&contentPlacement=1\&pgtype=collection}}:




\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./images/Case-Deepwater-NYTclip} 

}

\caption{\emph{New York Times} coverage of the
\emph{Deepwater Horizon} oil spill.}\label{fig:casedeepwater-nytclip}
\end{figure}

The incident is considered the worst oil spill in US history, creating
an environmental disaster along the Gulf Coast. In addition to studying
the effects on the local environment, researchers have undertaken
studies to examine the short and long-term health effects caused by the
incident. As an example, it is reasonable to ask whether volunteers who
were directly exposed to oil, such as when cleaning wildlife, are at
higher risk of respiratory irritation compared to those volunteers who
were helping with administrative tasks and therefore were not directly
exposed to the oil. An article appearing in \emph{The New England
Journal of Medicine} (B. D. Goldstein, Osofsky, and Lichtveld
\protect\hyperlink{ref-Goldstein2011}{2011}) reported the results from a
health symptom survey performed in the Spring and Summer of 2010 by the
National Institute for Occupational Safety and Health. Of 54 volunteers
assigned to wildlife cleaning and rehabilitation, 15 reported
experiencing ``nose irritation, sinus problems, or sore throat.'' Of 103
volunteers who had no exposure to oil, dispersants, cleaners, or other
chemicals, 16 reported experiencing ``nose irritation, sinus problems,
or sore throat.''

While a larger fraction of volunteers cleaning wildlife \emph{in the
study} reported respiratory symptoms compared to those who were not
directly exposed to irritants, would we expect similar results if we
were able to interview all volunteers? What about during a future oil
spill? Is there evidence that more than 1 in 5 volunteers who clean
wildlife will develop respiratory symptoms? What is a reasonable value
for the increased risk of respiratory symptoms for those volunteers with
direct exposure compared to those without?

In the first part of this text, we use this case study as the context
for discussing how research questions should be framed, methods for data
collection, summarizing and presenting data clearly, quantifying the
variability in an estimate, and quantifying the degree to which the data
disagrees with a proposed model. We capture these ideas in what we call
the \emph{Five Fundamental Ideas of Inference}. We see that any
statistical analysis iterates between the components of what we call the
\emph{Distributional Quartet}. These two frameworks allow us to describe
the language and logic of inference, serving as a foundation for the
statistical thinking and reasoning needed to address more complex
questions encountered later in the text.

\chapter{Asking the Right Questions}\label{Questions}

The discipline of statistics is about turning data into information in
order to address some question. While there may be no such thing as a
stupid question, there are ill-posed questions --- those which cannot be
answered as stated. Consider the
\protect\hyperlink{CaseDeepwater}{Deepwater Horizon Case Study}. It
might seem natural to ask ``if a volunteer cleans wildlife, will she
develop adverse respiratory symptoms?'' However, we quickly see that
this is an ill-posed question. Of the Of 54 volunteers assigned to
wildlife cleaning and rehabilitation, 15 reported experiencing adverse
respiratory symptoms (``nose irritation, sinus problems, or sore
throat''). So, while some volunteers developed symptoms, others did not.
What makes the question ill-posed is \emph{variability}, the fact that
not every volunteer had the same reaction when directly exposed to oil.

It is variability that creates a need for statistics; in fact, you could
think of statistics as the study and characterization of variability. We
must therefore learn to ask the \emph{right} questions --- those which
can be answered in the presence of variability.

\BeginKnitrBlock{definition}[Variability]
\protect\hypertarget{def:defn-variability}{}{\label{def:defn-variability}
\iffalse (Variability) \fi{} }The notion that measuremenets differ from
one observation to another.
\EndKnitrBlock{definition}

\BeginKnitrBlock{rmdkeyidea}
The presence of variability makes some questions ill-posed; statistics
concerns itself with how to address questions in the presence of
variability.
\EndKnitrBlock{rmdkeyidea}

\section{Characterizing a Variable}\label{characterizing-a-variable}

Recall that the goal of statistical inference is to say something about
the population; as a result, any question we ask should then be centered
on this larger group. The first step to constructing a well-posed
question is then to identify the population of interest for the study.
For the \protect\hyperlink{CaseDeepwater}{Deepwater Horizon Case Study},
it is unlikely that we are only interested in these 54 observed
volunteers assigned to wildlife cleaning. In reality, we might want to
say something about volunteers for any oil spill. In this case, the 54
volunteers in our dataset form the sample, a subset from all volunteers
who clean wildlife following an oil spill. That is, our population of
interest is comprised of all volunteers who clean wildlife following an
oil spill.

\BeginKnitrBlock{rmdtip}
When identifying the population of interest, be specific! Are you really
interested in \emph{all} trees, for example? Or, are you interested in
Maple trees within the city limits of Terre Haute, Indiana?
\EndKnitrBlock{rmdtip}

Since we expect that the reaction to oil exposure --- the primary
variable of interest for this study, sometimes called the
\textbf{response} --- to vary from one individual to another, we cannot
ask a question about the \emph{value} of the reaction (whether they
experienced symptoms or not). Instead, we want to characterize the
\textbf{distribution} of the response.

\BeginKnitrBlock{definition}[Response]
\protect\hypertarget{def:defn-response}{}{\label{def:defn-response}
\iffalse (Response) \fi{} }The primary variable of interest within a
study.
\EndKnitrBlock{definition}

\BeginKnitrBlock{definition}[Distribution]
\protect\hypertarget{def:defn-distribution}{}{\label{def:defn-distribution}
\iffalse (Distribution) \fi{} }The pattern of variability corresponding
to a set of values.
\EndKnitrBlock{definition}

Notice that in this case, the response is a categorical variable;
describing such a variable is equivalent to describing how individuals
in are divided among the possible groups. With a finite number of
observations, we could present the number of observations
(\textbf{frequeny}) within each group. For example, of the 54
volunteers, 15 experienced adverse symptoms and 39 did not. This works
well within the sample; however, as our population is infinitely large
(all volunteers cleaning wildlife on any oil spill), reporting the
frequencies is not appropriate. In this case, we report the fraction of
observations (\textbf{relative frequency}) falling within each group.

\BeginKnitrBlock{definition}[Frequency]
\protect\hypertarget{def:defn-frequency}{}{\label{def:defn-frequency}
\iffalse (Frequency) \fi{} }The number of observations falling into a
particular level of a categorical variable.
\EndKnitrBlock{definition}

\BeginKnitrBlock{definition}[Relative Frequency]
\protect\hypertarget{def:defn-relative-frequency}{}{\label{def:defn-relative-frequency}
\iffalse (Relative Frequency) \fi{} }Also called the ``proportion,'' the
fraction of observations falling into a particular level of a
categorical variable.
\EndKnitrBlock{definition}

Numeric quantities, like the proportion, which summarize the
distribution of a variable within the population are known as
\textbf{parameters}.

\BeginKnitrBlock{definition}[Parameter]
\protect\hypertarget{def:defn-parameter}{}{\label{def:defn-parameter}
\iffalse (Parameter) \fi{} }Numeric quantity which summarizes the
distribution of a variable within the \emph{population} of interest.
\EndKnitrBlock{definition}

While the \emph{value} of a variable may vary across the population, the
\emph{parameter} is a single fixed constant which summarizes the
variable for that population. Therefore, well-posed questions can be
constructed if we limit ourselves to questions about the parameter. The
second step in constructing well-posed questions is then to identify the
parameter of interest.

The questions we ask then generally fall into one of two categories:

\begin{itemize}
\tightlist
\item
  Estimation: what \emph{proportion} of volunteers who clean wildlife
  following an oil spill will experience adverse respiratory symptoms?
\item
  Model Consistency: is it reasonable that no more than 1 in 5
  volunteers who clean wildlife following an oil spill will experience
  adverse respiratory symptoms?
\end{itemize}

Now, since we do not get to observe the population (we only see the
sample), we cannot observe the value of the parameter. That is, we will
never know the true proportion of volunteers who will experience
symptoms. However, we can determine what the data suggests about the
population (that's inference).

\BeginKnitrBlock{definition}[Estimation]
\protect\hypertarget{def:defn-estimation}{}{\label{def:defn-estimation}
\iffalse (Estimation) \fi{} }Using the sample to approximate the value
of a parameter from the underlying population.
\EndKnitrBlock{definition}

\BeginKnitrBlock{definition}[Hypothesis Testing]
\protect\hypertarget{def:defn-hypothesis-testing}{}{\label{def:defn-hypothesis-testing}
\iffalse (Hypothesis Testing) \fi{} }Using a sample to determine if the
data is consistent with a working theory or if there is evidence to
suggest the data is not consistent with the theory.
\EndKnitrBlock{definition}

\BeginKnitrBlock{rmdkeyidea}
Parameters are unknown values and can, in general, never be known.
Parameters are generally denoted by Greek letters in statistical
formulas.
\EndKnitrBlock{rmdkeyidea}

It turns out, the vast majority of research questions can be framed in
terms of a parameter. In fact, this is the first of what we consider the
\emph{Five Fundamental Ideas of Inference}.

\BeginKnitrBlock{rmdfivefund}
\textbf{Fundamental Idea I}: A research question can often be framed in
terms of a parameter which characterizes the population. Framing the
question should then guide our analysis.
\EndKnitrBlock{rmdfivefund}

\section{Framing the Question}\label{framing-the-question}

In engineering and scientific applications, many questions fall under
the second category of model consistency. Examining such questions is
known as \textbf{hypothesis testing}. Specifically, data is collected to
help the researcher choose between two competing theories for the
parameter of interest. In this section, we consider the terminology
surrounding specifying such questions.

For the \protect\hyperlink{CaseDeepwater}{Deepwater Horizon Case Study}
suppose we are interested in addressing the following question:

\begin{quote}
Is there evidence that more than 1 in 5 volunteers who clean wildlife
following an oil spill will develop adverse respiratory symptoms?
\end{quote}

The question itself is about the population (all volunteers assigned to
clean wildlife following an oil spill) and is centered on a parameter
(the proportion who develop adverse respiratory symptoms). That is, this
is a well-posed question that can be answered with appropriate data. The
overall process for addressing these types of questions is similar to
conducting a trial in a court of law. In the United States, a trial has
the following essential steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Assume the defendant is innocent.
\item
  Evidence to establish guilt (to the contrary of innocence) is
  presented by the prosecution.
\item
  The jury considers the weight of the evidence.
\item
  If the evidence is ``beyond a reasonable doubt,'' the jury declares
  the defendant guilty; otherwise, the jury declares the subject not
  guilty.
\end{enumerate}

The process of conducting a hypothesis test has similar essential steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Assume the innocent of what we want the data to show (develop a
  working theory).
\item
  Gather data and compare it to the proposed model.
\item
  Quantify the likelihood of our data under the proposed model.
\item
  If the likelihood is small, conclude the data is not consistent with
  the working model (there is evidence for what we want to show);
  otherwise, conclude the data is consistent with the working model
  (there is no evidence for what we want to show).
\end{enumerate}

Notice that a trial focuses not on proving guilt but on disproving
innocence; similarly, in statistics, we are able to establish evidence
\emph{against} a specified theory. This process has several subtle
points including this one. We will discuss these subtleties at various
points throughout the text and revisit the overall concepts often. Here,
we focus solely on that first step --- developing a working theory that
we want to \emph{disprove}.

Consider the above question for the
\protect\hyperlink{CaseDeepwater}{Deepwater Horizon Case Study}. We want
to find evidence that the proportion experiencing adverse symptoms
exceeds 0.20 (1 in 5). Therefore, we would like to \emph{disprove} (or
provide evidence \emph{against}) the statement that the proportion
experiencing adverse symptoms is no more than 0.20. This is known as the
\textbf{null hypothesis}; the opposite of this statement, called the
\textbf{alternative hypothesis}, captures what we would like to
establish.

\BeginKnitrBlock{definition}[Null Hypothesis]
\protect\hypertarget{def:defn-null-hypothesis}{}{\label{def:defn-null-hypothesis}
\iffalse (Null Hypothesis) \fi{} }The statement (or theory) that we
would like to \emph{disprove}. This is denoted \(H_0\), read
``H-naught'' or ``H-zero''.
\EndKnitrBlock{definition}

\BeginKnitrBlock{definition}[Alternative Hypothesis]
\protect\hypertarget{def:defn-alternative-hypothesis}{}{\label{def:defn-alternative-hypothesis}
\iffalse (Alternative Hypothesis) \fi{} }The statement (or theory)
capturing what we would like to provide evidence \emph{for}; this is the
opposite of the null hypothesis. This is denoted \(H_1\) or \(H_a\),
read ``H-one'' and ``H-A'' respectively.
\EndKnitrBlock{definition}

For the \protect\hyperlink{CaseDeepwater}{Deepwater Horizon Case Study},
we write:

\begin{quote}
\(H_0:\) The proportion of volunteers assigned to wildlife following an
oil spill who experience adverse respiratory symptoms is no more than
0.20.\\
\(H_1:\) The proportion of volunteers assigned to wildlife following an
oil spill who experience adverse respiratory symptoms exceeds 0.20.
\end{quote}

Each hypothesis is a well-posed statement (about a parameter
characterizing the entire population), and the two statements are
exactly opposite of one another meaning only one can be a true
statement. We can now collect data to determine if it is consistent with
the null hypothesis (a statement similar to ``not guilty'') or if the
data provides evidence against the null hypothesis and in favor of the
alternative (a statement similar to ``guilty'').

Often these statements are written in a bit more of a mathematical
structure in which a Greek letter is used to represent the parameter of
interest. For example, we might write

\begin{quote}
Let \(\theta\) be the proportion of volunteers (assigned to wildlife
following an oil spill) who experience adverse respiratory symptoms.\\
\(H_0: \theta \leq 0.20\)\\
\(H_1: \theta > 0.20\)
\end{quote}

In the above statements, \(\theta\) represents the parameter of
interest; the value 0.20 is known as the \textbf{null value}.

\BeginKnitrBlock{definition}[Null Value]
\protect\hypertarget{def:defn-null-value}{}{\label{def:defn-null-value}
\iffalse (Null Value) \fi{} }The value associated with the equality
component of the null hypothesis; it forms the threshold or boundary
between the two hypothesis. Note: not all questions of interest require
a null value be specified.
\EndKnitrBlock{definition}

\BeginKnitrBlock{rmdkeyidea}
Hypothesis testing is a form of statistical inference in which we
quantify the evidence \emph{against} a working theory (captured by the
null hypothesis). We essentially argue that the data supports the
alternative if it is not consistent with the working theory.
\EndKnitrBlock{rmdkeyidea}

\BeginKnitrBlock{rmdtip}
\textbf{Process for Framing a Question} In order to frame a research
question, consider the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify the population of interest.
\item
  Identify the parameter(s) of interest.
\item
  Determine if you are interested in estimating the parameter(s) or
  quantifying the evidence against some working theory.
\item
  If you are interested in testing a working theory, make the null
  hypothesis the working theory and the alternative the exact opposite
  statement (what you want to provide evidence for).
\end{enumerate}
\EndKnitrBlock{rmdtip}

\chapter{Gathering the Evidence (Data Collection)}\label{Data}

Consider again the goal of statistical inference --- to use a sample as
a snapshot to say something about the underlying population (Figure
\ref{fig:data-statistical-process}). This generally provokes unease in
people, leading to a distrust of statistical results. In this section we
attack that distrust head on.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/Basics-Stat-Process} 

}

\caption{Illustration of the statistical process (reprinted from Chapter 1).}\label{fig:data-statistical-process}
\end{figure}

\section{What Makes a Sample
Reliable}\label{what-makes-a-sample-reliable}

If we are going to have some amount of faith in the statistical results
we produce, we must have data in which we can place our trust. \emph{The
Treachery of Images} (Figure @\ref(fig:data-pipe-img)) is a canvas
painting depicting a pipe, below which the artist wrote the French
phrase for ``This is not a pipe.'' Regarding the painting, the artist
said

\begin{quote}
The famous pipe. How people reproached me for it! And yet, could you
stuff my pipe? No, it's just a representation, is it not? So if I had
written on my picture ``This is a pipe,'' I'd have been lying!
\end{quote}




\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./images/Data-Pipe} 

}

\caption{\emph{The Treachery of Images} by Ren√©
Magritte.}\label{fig:data-pipe-img}
\end{figure}

Just as a painting is a representation of the object it depicts, so a
sample should be a representation of the underlying population from
which it was taken. This is the primary requirement if we are to rely on
the resulting data.

\BeginKnitrBlock{rmdkeyidea}
In order for a statistical analysis to be reliable, the sample must be
\emph{representative} of the underlying population.
\EndKnitrBlock{rmdkeyidea}

We need to be careful to not get carried away in our expectations. What
constitutes ``representative'' really depends on the question, just as
an artist chooses his depiction based on how he wants to represent the
object. Let's consider the following example.

\BeginKnitrBlock{example}[School Debt]
\protect\hypertarget{ex:data-school-debt}{}{\label{ex:data-school-debt}
\iffalse (School Debt) \fi{} }In addition to a degree, college graduates
also tend to leave with a large amount of debt due to college loans. In
2012, a graduate with a student loan had an average debt of \$29,400;
for graduates from private non-profit institutions, the average debt was
\$32,300\footnote{\url{http://ticas.org/sites/default/files/pub_files/Debt_Facts_and_Sources.pdf}}.

Suppose we are interested in determining the average amount of debt in
student loans carried by a graduating senior from Rose-Hulman Institute
of Technology, a small private non-profit engineering school. There are
many faculty at Rose-Hulman who choose to send their children to the
institute. Since I am also on the faculty, I know many of these
individuals. Suppose I were to ask each to report the amount of student
loans their children carried upon graduation from Rose-Hulman. I compile
the 25 responses and compute the average amount of debt. Further, I
report that based on this study, there is significant evidence that the
average debt carried by a graduate of Rose-Hulman is far below the
\$32,300 reported above (great news for this year's graduating class)!
Why might we be hesitant to trust these results?
\EndKnitrBlock{example}

When our distrust of a statistical result stems from a distrust of the
data on which it is based, it is generally a result of our doubting the
sample is representative of the population. Rose-Hulman, like many other
universities, has a policy that the children of faculty may attend their
university (assuming admittance) tuition-free. We would therefore expect
their children to carry much less debt than the typical graduating
senior.

This provides a nice backdrop for discussing what it means to be
representative. First, let's define our population; in this case, we are
interested in graduating seniors. The variable of interest is the amount
of debt carried in student loans; the parameter of interest is then the
average amount of debt in student loans carried by graduating seniors.
With regard to the grade point average of the students in our sample, it
is probably similar to all graduating seniors. Their starting salary is
probably similar; the fraction of mechanical engineering majors versus
math majors is probably similar. So, in many regards the sample is
representative of the population; however, it fails to be representative
with regard to the variable of interest. This is our concern. The amount
of debt carried by students in our sample is not representative of that
debt carried by all graduating seniors.

\BeginKnitrBlock{rmdtip}
When thinking about whether a sample is representative, focus your
attention to the characteristics specific to your research question.
\EndKnitrBlock{rmdtip}

Does that mean the sample is useless? Yes and no. The sample collected
cannot be used to answer our initial question of interest. No
statistical method can fix bad data; statistics adheres to the
``garbage-in, garbage-out'' phenomena. If the data is bad, no analysis
will undo that. While the sample cannot be used to answer our initial
question, it could be used to address a different question however:

\begin{quote}
What is the average amount of debt in student loans carried by
graduating seniors from Rose-Hulman whose parent is a faculty member at
the university?
\end{quote}

For this revised question, the sample may indeed be representative. If
we are working with previously collected data, we must consider the
population to which our results will generalize. That is, for what
population is the given sample representative. If we are collecting our
data, we need to be sure we collect data in such a way that the data is
representative. Let's first look at what \emph{not} to do.

\section{Poor Methods of Data
Collection}\label{poor-methods-of-data-collection}

Example \ref{ex:data-school-debt} is an example of a ``convenience
sample,'' when the subjects in the sample are chosen simply due to ease
of collection. Examples include surveying students only in your sorority
when you are interested in all females who are part of a sorority on
campus; taking soil samples from only your city when you are interested
in the soil for the entire state; and, obtaining measurements from only
one brand of phone, because it was the only one you could afford on your
budget, when you are interested in studying all cell phones on the
market. A convenience sample is unlikely to be representative if there
is a relationship between the ease of collection and the variable under
study. This was true in the School Debt example; the relationship of a
student to a faculty member was directly related to the amount of debt
they carried. As a result, the resulting sample was not representative
of the population.

When conducting a survey with human subjects, it is common to only
illicit responses from volunteers. Such ``volunteer samples'' tend to
draw in those with extreme opinions. Consider product ratings on Amazon.
Individual ratings tend to cluster around 5's and 1's. This is because
those customers who take time to submit a review (which is voluntary)
tend to be those who are really thrilled with their product (and want to
encourage others to purchase it) and those who are really disappointed
with their purchase (and want to encourage others to avoid it). Such
surveys often fail to capture those individuals in the population who
have intermediate opinions.

We could not possibly name all the poor methods for collecting a sample;
but, these methods all share something in common --- it is much more
likely the resulting sample is not representative. Failing to be
representative results in \textbf{biased} estimates of the parameter.

\BeginKnitrBlock{definition}[Bias]
\protect\hypertarget{def:defn-bias}{}{\label{def:defn-bias} \iffalse (Bias)
\fi{} }A set of measurements, or an estimate of a parameter, is said to
be biased if they are \emph{consistently} too high (or too low).
\EndKnitrBlock{definition}

To illustrate the concept of bias, consider shooting at a target as in
Figure \ref{fig:data-bias}. We can consider the center of our target to
be the parameter we would like to estimate within the population. The
values in our sample (the strikes on the target) will vary around the
parameter; while we do not expect any one value to hit the target
precisely, a ``representative'' sample is one in which the values tend
to be clustered about the parameter (unbiased). When the sample is not
representative, the values in the sample tend to cluster off the mark
(biased). Notice that to be unbiased, it may be that not a single value
in the sample is perfect, but aggregated together, they point in the
right direction. So, bias is not about an individual measurement being
an ``outlier,'' (more on those in a later chapter) but about repeatedly
shooting in the wrong direction.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./images/Data-Bias} 

}

\caption{Illustration of bias and variability.}\label{fig:data-bias}
\end{figure}

\BeginKnitrBlock{rmdkeyidea}
Biased results are typically due to poor sampling methods that result in
a sample which is not representative of the underlying population.
\EndKnitrBlock{rmdkeyidea}

The catch (there is always a catch) is that we will never know if a
sample is representative or not. But, we can employ methods that help to
minimize the chance that the sample is biased.

\section{Preferred Methods of
Sampling}\label{preferred-methods-of-sampling}

No method guarantees a perfectly representative sample; but, we can take
measures to reduce or eliminate bias. A useful strategy is to employ
\emph{randomization}. This is summarized in our second Fundamental Idea:

\BeginKnitrBlock{rmdfivefund}
\textbf{Fundamental Idea II}: If data is to be useful for making
conclusions about the population, a process referred to as drawing
inference, proper data collection is crucial. Randomization can play an
important role ensuring a sample is representative and that inferential
conclusions are appropriate.
\EndKnitrBlock{rmdfivefund}

Consider the School Debt example again. Suppose instead of the strategy
described there, we had constructed a list of all graduating seniors
from the university. We placed the name of each student on an index
card; then, I thoroughly shuffle the cards and choose the top 25 cards.
For these 25 individuals, I record the amount of debt in student loans
each carries. Using a lottery to select the sample is known as a
\textbf{simple random sample}. By conducting a lottery, we make it very
unlikely that our sample consists of only students with a very small
amount of student debt (as occurred when we used a convenience sample).

\BeginKnitrBlock{definition}[Simple Random Sample]
\protect\hypertarget{def:defn-simple-random-sample}{}{\label{def:defn-simple-random-sample}
\iffalse (Simple Random Sample) \fi{} }Often abbreviated SRS, this is a
sample of size \(n\) such that \emph{every} collection of size \(n\) is
equally likely to be the resulting sample. This is equivalent to a
lottery.
\EndKnitrBlock{definition}

There are situations in which a simple random sample does not suffice.
Again, consider our School Debt example. The Rose-Hulman student body is
predominantly domestic, with only about 3\% of the student body being
international students. But, suppose we are interested in comparing the
average debt carried between international and domestic students. It is
very likely that in a simple random sample of 25 students, none will be
international by chance alone. Instead of a simple random sample, we
might consider taking a sample of, say 13, domestic students and a
sample of 12 international students; this is an example of a
\textbf{stratified random sample}. This approach is useful when there is
a natural grouping of interest within the population.

\BeginKnitrBlock{definition}[Stratified Random Sample]
\protect\hypertarget{def:defn-stratified-random-sample}{}{\label{def:defn-stratified-random-sample}
\iffalse (Stratified Random Sample) \fi{} }A sample in which the
population is first divided into groups, or strata, based on a
characteristic of interest; a simple random sample is then taken within
each group.
\EndKnitrBlock{definition}

There are countless sampling techniques used in practice. The two
described above can be very useful starting point for developing a
custom method suitable for a particular application. Their benefit stems
from their use of randomization.

This section is entitled ``Preferred Methods'' because while these
methods are ideal, they are not always practical. For example, consider
the \protect\hyperlink{CaseDeepwater}{Deepwater Horizon Case Study};
conceptually, we can take a simple random sample of the volunteers for
our study. However, as with any study involving human subjects,
researchers would be required to obtain consent from each subject in the
study. That is, a volunteer has the right to refuse to participate in
the study. Therefore, it is unlikely that a simple random sample as
described above could be obtained. Again, the key is to obtain a
\emph{representative} sample; while random selection may be a nice tool
for accomplishing this, we may need to appeal to the composition of the
sample itself to justify its use. Based on the characteristics of those
willing to participate in the study, do we feel the study participants
form a representative group of all volunteers? That is the essential
question. This is often why studies report a table summarizing subject
demographics such as age, gender, etc. It is also why it is extremely
important for researchers to describe how subjects were selected so that
readers may make the judgement for themselves whether the sample is
representative.

\section{Two Types of Studies}\label{two-types-of-studies}

Thinking about how the data was collected helps us determine how the
results generalize beyond the sample itself (to what population the
results apply). When our question of interest is about the relationship
between two variables (as most questions are), we must also carefully
consider the study design. Too often separated from the statistical
analysis that follows, keeping the study design in mind should guide the
analysis as well as inform us about the conclusions we can draw.

In order to illustrate how study design can impact the results, consider
the following example.

\BeginKnitrBlock{example}[Kangaroo Care]
\protect\hypertarget{ex:data-kangaroo}{}{\label{ex:data-kangaroo}
\iffalse (Kangaroo Care) \fi{} }At birth, infants have low levels of
Vitamin K, a vitamin needed in order to form blood clots. Though rare,
without the ability for her blood to clot, an infant could develop a
serious bleed. In order to prevent this, the American Academy of
Pediatrics recommends that all infants be given a Vitamin K shot shortly
after birth in order to raise Vitamin K levels. As with any shot, there
is typically discomfort to the infant, which can be very discomforting
to new parents.

Kangaroo Care is a method of holding a baby which emphasizes
skin-to-skin contact. The child, who is dressed only in a diaper, is
placed upright on the parent's bare chest; a light blanket is draped
over the child. The method was initially recognized for its benefits in
caring for pre-term infants. Suppose suppose we are interested in
determining if utilizing the method while giving the child a Vitamin K
shot reduces the discomfort in the infant, as measured by the total
amount of time the child cries following the shot. Contrast the
following two potential study designs:

\begin{enumerate}
\def\labelenumi{(\Alph{enumi})}
\tightlist
\item
  We allow the attending nurse to determine whether Kangaroo Care is
  initiated prior to giving the Vitamin K shot. Following the shot, we
  record the total time (in seconds) the child cries.
\item
  We flip a coin. If it comes up heads, the nurse should have the
  parents implement Kangaroo Care prior to giving the Vitamin K shot; if
  it comes up tails, the nurse should give the Vitamin K shot without
  implementing Kagaroo Care. Following the shot, we record the total
  time (in seconds) the child cries.
\end{enumerate}

Note, in both study designs (A) and (B), we only consider term births
which have no complications to avoid potential complications that might
alter the timing of the Vitamin K shot or the ability to implement
Kangaroo Care.
\EndKnitrBlock{example}

Note that there are some similarities in the two study designs:

\begin{itemize}
\tightlist
\item
  The underlying population is the same for both designs --- infants
  born at term with no complications.
\item
  There are two treatment groups in both designs --- the ``Kangaroo
  Care'' group and the ``no Kangaroo Care'' group.
\item
  The response (variable of interest) is the same in both designs ---
  the time (in seconds) the infant cries.
\item
  There is action taken by the researcher in both designs --- a Vitamin
  K shot is given to the child.
\end{itemize}

There is one prominent difference between the two study designs:

\begin{itemize}
\tightlist
\item
  For design (A), the choice of Kangaroo Care is left up to the nurse
  (self-selected); for design (B), the choice of Kangaroo is
  \emph{assigned} to the nurse by the researcher, and this selection is
  made \emph{at random}.
\end{itemize}

Design (A) is an example of an \textbf{observational study}; design (B)
is a \textbf{controlled experiment}.

\BeginKnitrBlock{definition}[Observational Study]
\protect\hypertarget{def:defn-observational-study}{}{\label{def:defn-observational-study}
\iffalse (Observational Study) \fi{} }A study in which the subjects
self-select into the treatment groups under study.
\EndKnitrBlock{definition}

\BeginKnitrBlock{definition}[Controlled Experiment]
\protect\hypertarget{def:defn-controlled-experiment}{}{\label{def:defn-controlled-experiment}
\iffalse (Controlled Experiment) \fi{} }A study in which the subjects
are \emph{randomly} assigned to the treatment groups under study.
\EndKnitrBlock{definition}

It is common to think that anytime the environment is ``controlled'' by
the researcher that an experiment is taking place, but the defining
characteristic is the random assignment to treatment groups (sometimes
referred to as the \emph{factor} under study). In the example above,
both study designs involved a controlled setting (the delivery room of a
hospital) in which trained staff (the nurse) delivered the shot.
However, only design (B) is a controlled experiment because the
researchers randomly determined into which group the infant would be
placed.

To understand the impact of random allocation, suppose that we had
conducted a study as in design (A); further, the results suggest that
those infants who were given a shot while using Kangaroo Care cried for
a shorter time period, on average. Can we conclude that it was the
Kangaroo Care that led to the shorter crying time? Maybe. Consider the
following two potential explanations for the resulting data:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Kangaroo Care is very effective; as a result, those children who were
  given Kangaroo Care had reduced crying time following the Vitamin K
  shot.
\item
  It turns out that those nurses who chose to implement Kangaroo Care
  (remember, they have a choice under design (A) whether they implement
  the method) were also the nurses with a gentler bedside manner.
  Therefore, these nurses tended to be very gentle when giving the
  Vitamin K shot whereas the nurses who chose not to implement Kangaroo
  Care tended to just jab the needle in when giving the shot. As a
  result, the reduced crying time is not a result of the Kangaroo Care
  but the manner in which the shot was given.
\end{enumerate}

The problem is that we are unable to determine which of the explanations
is true. Given the data we have collected, we are unable to tease out
the effect of the Kangaroo Care from that of the nurse's bedside manner.
As a result, we are able to say we observed a \emph{relationship}
between the use of Kangaroo Care and reduced crying time, but we are
unable to conclude that Kangaroo Care \emph{caused} a reduction in the
crying time. In this hypothetical scenario, the nurse's bedside manner
is called a \textbf{confounder}.

\BeginKnitrBlock{definition}[Confounding]
\protect\hypertarget{def:defn-confounding}{}{\label{def:defn-confounding}
\iffalse (Confounding) \fi{} }When the effect of a variable on the
response is mis-represented due to the presence of a third, potentially
unobserved, variable known as a confounder.
\EndKnitrBlock{definition}

Confounders can mask the relationship between the factor under study and
the response. There is a documented relationship between ice cream sales
and the risk of shark attacks. As ice cream sales increase, the risk of
a shark attack also increases. This does not mean that if a small city
in the Midwest increases its ice cream sales that the citizens are at
higher risk of being attacked by a shark. As Figure
\ref{fig:data-confounding} illustrates, there is a confounder ---
temperature. As the temperatures increase, people tend to buy more ice
cream; as the temperature increases, people tend to go to the beach
increasing the risk of a shark attack. Two variables can appear to be
related as a result of a confounder.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./images/Data-Confounding} 

}

\caption{Illustration of a confounding variable. The confounder, related to both the factor and the treatment can make it appear as though there is a causal relationship when none exists.}\label{fig:data-confounding}
\end{figure}

\BeginKnitrBlock{rmdtip}
Confounders are variables that influence \emph{both} the factor of
interest and the response.
\EndKnitrBlock{rmdtip}

Observational studies are subject to confounding; thus, controlled
experiments are often considered the gold standard in research because
they allow us to infer cause-and-effect relationships from the data. Why
does the random allocation make such an impact? Because it removes the
impact of confounders. Let's return to the hypothetical study. Suppose
there are nurses with a gentle bedside manner and those who are a little
less gentle. If the infants are randomly assigned to one of the two
treatment groups, then for every gentle nurse who is told to implement
Kangaroo Care while giving the shot, there is a gentle nurse who is told
to not implement Kangaroo Care. Similarly, for every mean nurse who is
told to implement Kangaroo Care while giving a shot, there is a mean
nurse who is told to not implement Kangaroo Care. This is illustrated in
Figure \ref{fig:data-randomization}. For an observational study, the
treatment groups are unbalanced; there is a higher fraction (11/12
compared to 1/4) of friendly nurses in the Kangaroo Care group compared
to the No Kangaroo Care group. For the controlled experiment however,
the treatment groups are balanced; there is approximately the same
fraction of friendly nurses in both groups. Random assignment is the
great equalizer; it tends to result in groups which are similar in all
respects; therefore, any differences we observe between the groups
\emph{must} be due to the grouping and not an underlying confounding
variable.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./images/Data-Randomization} 

}

\caption{Illustration of the impact of random assignment in study design. For the observational study, the treatment groups are unbalanced.  For the controlled experiment, the treatment groups are balanced.}\label{fig:data-randomization}
\end{figure}

\BeginKnitrBlock{rmdkeyidea}
Randomly assigning treatment groups balances the groups with respect to
the confounders; that is, the treatment groups are similar. Therefore,
any differences between the two groups can be attributed to the grouping
factor itself.
\EndKnitrBlock{rmdkeyidea}

While controlled experiments are a fantastic study design, we should not
discount the use of observational studies. Consider the
\protect\hyperlink{CaseDeepwater}{Deepwater Horizon Case Study}; suppose
we are interested in the following question:

\begin{quote}
Is there evidence that volunteers who are directly exposed to oil have
an increased risk of developing adverse respiratory symptoms compared to
those who are not directly exposed to oil?
\end{quote}

The response is whether a volunteer develops adverse respiratory
symptoms; the factor of interest is whether the volunteer has direct
exposure to oil. We could conduct a controlled experiment by randomly
determining which volunteers are assigned to wildlife clean up and which
are assigned to administrative tasks, for example. However, it may be
that volunteer tasks need to be determined by skillset or by greatest
need at the time of the person volunteers. It may not be feasible to
randomly assign volunteers to specific positions. Or, it could be that
the data was obtained after the fact; that is, the data is not the
result of a planned study in which case random assignment is not
possible because volunteers self-selected into positions in the past. If
random assignment is not possible, it does not mean the data is useless.
But, it does mean we will need to be sure we address the potential
confounding when performing the analysis and discussing the results. The
latter half of the text will discuss methods for addressing confounding.

The big idea is that in order to make causal conclusions, we must be
able to state that the two treatment groups are balanced with respect to
any potential confounders; random assignment is one technique for
accomplishing this.

\chapter{Presenting the Evidence (Summarizing Data)}\label{Summaries}

If you open any search engine and look up ``data visualization,'' you
will be quickly overwhelmed by a host of pages, texts, and software
filled with tools for summarizing your data. Here is the bottom line: a
good visualization is one that helps you answer your question of
interest. It is both that simple and that complicated.

\BeginKnitrBlock{rmdfivefund}
\textbf{Fundamental Idea III}: The use of data for decision making
requires that the data be summarized and presented in ways that address
the question of interest.
\EndKnitrBlock{rmdfivefund}

Whether simple or complex, all graphical and numerical summaries should
help turn the data into usable information. Pretty pictures for the sake
of pretty pictures are not helpful. In this section, we will consider
various simple graphical and numerical summaries to help build a case
for addressing the question of interest.

\section{Characteristics of a Distribution (Summarizing a Single
Variable)}\label{characteristics-of-a-distribution-summarizing-a-single-variable}

Remember that because of \emph{variability}, the key to asking good
questions is to not ask questions about individual values but to
characterize the underlying \emph{distribution} (see Definition
\ref{def:defn-distribution}). Therefore, characterizing the underlying
distribution is also the key to a good visualization or numeric summary.
For the \protect\hyperlink{CaseDeepwater}{Deepwater Horizon Case Study},
the response (whether a volunteer experienced adverse respiratory
symptoms) is categorical. As we stated previously, summarizing the
distribution of a categorical variable reduces to showing how individual
subjects fall into the various groups. Figure
\ref{fig:summaries-deepwater-barchart} displays a \emph{bar chart}
summarizing the rate of respiratory symptoms for volunteers cleaning
wildlife.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/summaries-deepwater-barchart-1} 

}

\caption{Frequency of adverse respiratory symptoms for volunteers cleaning wildlife following the Deepwater Horizon oil spill.}\label{fig:summaries-deepwater-barchart}
\end{figure}

In general, it does not matter whether the frequency or the relative
frequencies are reported; however, if the relative frequencies are
plotted, some indication of the sample size should be provided with the
figure either as an annotation or within the caption. Statisticians tend
to agree that bar charts are preferable to pie charts (see
\href{https://www.google.com/url?sa=t\&rct=j\&q=\&esrc=s\&source=web\&cd=32\&cad=rja\&uact=8\&ved=0ahUKEwjk6Lf42sfVAhVl64MKHaTdAFY4HhAWCC4wAQ\&url=https\%3A\%2F\%2Fwww.perceptualedge.com\%2Farticles\%2Fvisual_business_intelligence\%2Fsave_the_pies_for_dessert.pdf\&usg=AFQjCNFkS-sogmLsZIOheWAPBZSNcqjzkg}{this
whitepaper} and
\href{http://www.storytellingwithdata.com/blog/2014/06/alternatives-to-pies}{this
blog} for further explanation). More importantly, we all agree that the
graphic should help address the question. From the above graphic, we see
that nearly 28\% of volunteers assigned to wildlife experienced adverse
respiratory symptoms.

Summarizing the distribution of a numeric variable requires a bit more
thought. Consider the following example.

\BeginKnitrBlock{example}[Paper Strength]
\protect\hypertarget{ex:summaries-paper}{}{\label{ex:summaries-paper}
\iffalse (Paper Strength) \fi{} }While electronic records have become
the predominant means of storing information, we do not live in a
paperless society. Paper products are still used in a variety of
applications ranging from printing reports and photography to packaging
and bathroom tissue. In manufacturing paper for a particular
application, the strength of the resulting paper product is a key
characteristic.

There are several metrics for the strength of paper. A conventional
metric for assessing the inherent (not dependent upon the physical
characteristics, such as the weight of the paper, which might have an
effect) strength of paper is the \emph{breaking length}. This is the
length of a paper strip, if suspended vertically from one end, that
would break under its own weight. Typically reported in kilometers, the
breaking length is computed from other common measurements. For more
information on paper strength measurements and standards, see the
following website: \url{http://www.paperonweb.com}

A study was conducted at the University of Toronto to investigate the
relationship between pulp fiber properties and the resulting paper
properties (Lee \protect\hyperlink{ref-Lee1992}{1992}). The breaking
length was obtained for each of the 62 paper specimens, the first 5
measurements of which are shown in Table
\ref{tab:summaries-paper-table}. The complete data is available online
at the following website:
\url{https://vincentarelbundock.github.io/Rdatasets/doc/robustbase/pulpfiber.html}

While there are several questions one might ask with the available data,
here we are primarily interested in characterizing the breaking length
of these paper specimens.
\EndKnitrBlock{example}

\begin{table}

\caption{\label{tab:summaries-paper-table}Breaking length (km) for first 5 specimens in the Paper Strength study.}
\centering
\begin{tabular}[t]{r|r}
\hline
Specimen & Breaking Length\\
\hline
1 & 21.312\\
\hline
2 & 21.206\\
\hline
3 & 20.709\\
\hline
4 & 19.542\\
\hline
5 & 20.449\\
\hline
\end{tabular}
\end{table}

Figure \ref{fig:summaries-paper-dotplot} presents the breaking length
for all 62 paper specimens in the sample through a \emph{dot plot}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/summaries-paper-dotplot-1} 

}

\caption{Breaking Length (km) for 62 paper specimens.}\label{fig:summaries-paper-dotplot}
\end{figure}

With any graphic, we tend to be drawn to three components:

\begin{itemize}
\tightlist
\item
  \emph{where} the values tend to be,
\item
  \emph{how tightly} the values tend to be clustered there, and
\item
  \emph{the way} the values tend to cluster.
\end{itemize}

Notice that about half of the paper specimens in the sample had a
breaking length longer than 21.26 km. Only about 25\% of paper specimens
had a breaking length less than 19.33 km. These are measures of
\emph{location}. In particular, these are known as \textbf{percentiles},
of which the \textbf{median}, \textbf{first quartile} and \textbf{third
quartile} are commonly used examples.

\BeginKnitrBlock{definition}[Percentile]
\protect\hypertarget{def:defn-percentile}{}{\label{def:defn-percentile}
\iffalse (Percentile) \fi{} }The value \(q\) such that \(k\)\% of the
values in the distribution are less than or equal to \(q\). For example,

\begin{itemize}
\tightlist
\item
  25\% of values in a distribution are less than or equal to the 25-th
  percentile (known as the first quartile).
\item
  50\% of values in a distribution are less than or equal to the 50-th
  percentile (known as the median).
\item
  75\% of values in a distribution are less than or equal to the 75-th
  percentile (known as the third quartile).
\end{itemize}
\EndKnitrBlock{definition}

The \textbf{average} is also a common measure of location. The breaking
length of a paper specimen is 21.72 km, on average. In this case, the
average breaking length and median breaking length are very close; this
need not be the case. The average is not describing the ``center'' of
the data in the same way as the median; they capture different
properties.

\BeginKnitrBlock{definition}[Average]
\protect\hypertarget{def:defn-average}{}{\label{def:defn-average}
\iffalse (Average) \fi{} }Also known as the ``mean,'' this measure of
location represents the balance point for the distribution. It is
denoted by \(\bar{x}\).

For a sample of size \(n\), it is computed by
\[\bar{x} = \frac{1}{n}\sum_{i=1}^{n} x_i\]

where \(x_i\) rerpesents the \(i\)-th value in the sample.

When referencing the average for a population, it can also be called the
``Expected Value,'' and is often denoted by \(\mu\).
\EndKnitrBlock{definition}

Clearly, the breaking length is not equivalent for all paper specimens;
that is, there is variability in the measurements. Measures of
\emph{spread} quantify the variability of values within a distribution.
Common examples include the \textbf{standard deviation} (related to
\textbf{variance}) and \textbf{interquartile range}. For the Paper
Strength example, the breaking length varies with a standard deviation
of 2.88 km; the interquartile range for the breaking length was 5.2 km.
Neither of these values has a natural interpretation; instead, larger
values of these measures simply indicate a higher degree of variability
in the data. The standard deviation is often reported more often than
the variance since it is on the same scale as the original data;
however, as we will see later, the variance is useful from a
mathematical perspective for derivations.

\BeginKnitrBlock{definition}[Variance]
\protect\hypertarget{def:defn-variance}{}{\label{def:defn-variance}
\iffalse (Variance) \fi{} }A measure of spread, this roughly captures
the average distance values in the distribution are from the mean.

For a sample of size \(n\), it is computed by
\[s^2 = \frac{1}{n-1}\sum_{i=1}^{n} \left(x_i - \bar{x}\right)^2\]

where \(\bar{x}\) is the sample mean and \(x_i\) is the \(i\)-th value
in the sample. The division by \(n-1\) instead of \(n\) reduces the bias
in the statistic.
\EndKnitrBlock{definition}

\BeginKnitrBlock{definition}[Standard Deviation]
\protect\hypertarget{def:defn-standard-deviation}{}{\label{def:defn-standard-deviation}
\iffalse (Standard Deviation) \fi{} }A measure of spread, this is the
square root of the variance.
\EndKnitrBlock{definition}

\BeginKnitrBlock{definition}[Interquartile Range]
\protect\hypertarget{def:defn-interquartile-range}{}{\label{def:defn-interquartile-range}
\iffalse (Interquartile Range) \fi{} }The distance between the first and
third quartiles. This measure of spread indicates the range over which
the middle 50\% of the data is spread.
\EndKnitrBlock{definition}

The measures we have discussed so far are illustrated in Figure
\ref{fig:summaries-summaries}. While some authors suggest that which
values are reported depend on the shape of the distribution, we argue
that it is best to report the values that align with the question of
interest. It is the question that should be shaped by the beliefs about
the underlying distribution.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./images/Summaries-Summaries} 

}

\caption{Illustration of measures of location and spread for a distribution of values.}\label{fig:summaries-summaries}
\end{figure}

Finally, consider the \emph{shape} of the distribution of breaking
length we have observed. The breaking length tends to be clustered in
two locations; we call this \emph{bimodal} (each mode is a ``hump'' in
the distribution). Other terms used to describe the shape of a
distribution are \emph{symmetric} and \emph{skewed}. Symmetry refers to
cutting a distribution in half (at the median) and the lower half being
a mirror image of the upper half; skewed distributions are those which
are not symmetric.

Observe then that the dot plot above gives us some idea of the location,
spread, and shape of the distribution, in a way that the table of values
could not. This makes it a useful graphic as it is characterizing the
\textbf{distribution of the sample} we have observed. This is one of the
distributions in what we call the \emph{Distributional Quartet}.

\BeginKnitrBlock{definition}[Distribution of the Sample]
\protect\hypertarget{def:defn-distribution-sample}{}{\label{def:defn-distribution-sample}
\iffalse (Distribution of the Sample) \fi{} }The pattern of variability
in the observed values of a variable.
\EndKnitrBlock{definition}

When the sample is not large, a dot plot is reasonable. Other common
visualizations for a single variable include a \emph{jitter plot},
\emph{box plot}, \emph{histogram}, or \emph{density plot} (smoothed
histogram). To illustrate, the breaking length for the Paper Strength
example is summarized using various methods in Figure
\ref{fig:summaries-univariate}. The latter three visualizations are more
helpful when the dataset is very large and plotting the raw values
actually hides the distribution. There is no right or wrong graphic; it
is about choosing the graphic which addresses the question and
adequately portrays the distribution.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/summaries-univariate-1} 

}

\caption{Four graphical summaries of the breaking length for the Paper Strength example.}\label{fig:summaries-univariate}
\end{figure}

The numeric summaries of a distribution are known as
\textbf{statistics}. While parameters characterize a variable at the
population level, statistics characterize a variable at the sample
level.

\BeginKnitrBlock{definition}[Statistic]
\protect\hypertarget{def:defn-statistic}{}{\label{def:defn-statistic}
\iffalse (Statistic) \fi{} }Numeric quantity which summarizes the
distribution of a variable within a \emph{sample}.
\EndKnitrBlock{definition}

Why would we compute numerical summaries in the sample if we are
interested in the population? Remember the goal of this discipline is to
use the sample to say something about the underlying population. As long
as the sample is representative, the distribution of the sample should
reflect the \textbf{distribution of the population}; therefore,
summaries of the sample should roughly equate to the analogous summaries
of the population. Now we see the real importance of having a
representative sample; it allows us to say that what we observe in the
sample is a good proxy for what is happening in the population.

\BeginKnitrBlock{definition}[Distribution of the Population]
\protect\hypertarget{def:defn-distribution-population}{}{\label{def:defn-distribution-population}
\iffalse (Distribution of the Population) \fi{} }The pattern of
variability in values of a variable at the population level. Generally,
this is impossible to know, but we might model it.
\EndKnitrBlock{definition}

That is, the mean in the sample should approximate (estimate) the mean
in the population; the standard deviation of the sample should estimate
the standard deviation in the population; and, the shape of the sample
should approximate the shape of the population, etc. The sample is
acting as a representation in all possible ways of the population.

\BeginKnitrBlock{rmdkeyidea}
A representative sample reflects the population; therefore, we can use
statistics as estimates of the population parameters.
\EndKnitrBlock{rmdkeyidea}

\section{Summarizing Relationships}\label{summarizing-relationships}

The summaries discussed above are nice for examining a single variable.
In general, research questions of interest typically involve the
relationship between two or more variables. Most graphics are
two-dimensional (though 3-dimensional graphics and even virtual reality
are being utilized now); therefore, summarizing a rich set of
relationships may require the use of both axes, color, shape, size, and
even multiple plots in order to tell the right story. We will explore
these various features in upcoming units of the text. Here, we focus on
the need to tell a story that answers the question of interest instead
of getting lost in making a graphic. Consider the following question
from the \protect\hyperlink{CaseDeepwater}{Deepwater Horizon Case
Study}:

\begin{quote}
What is the increased risk of developing adverse respiratory symptoms
for volunteers cleaning wildlife compared to those volunteers who do not
have direct exposure to oil?
\end{quote}

Consider the graphic in Figure \ref{fig:summaries-bad-barchart}; this is
\emph{not} a useful graphic. While it compares the number of volunteers
with symptoms in each group, we cannot adequatly address the question
because the research question involves comparing the rates for the two
groups.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/summaries-bad-barchart-1} 

}

\caption{Illustration of a poor graphic; the graphic does not give us a sense of the rate within each group in order to make a comparison.}\label{fig:summaries-bad-barchart}
\end{figure}

Instead, Figure \ref{fig:summaries-good-barchart} compares the rates
within each group. Notice that since we are reporting relative
frequencies for comparison, we report the sample size for each group.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/summaries-good-barchart-1} 

}

\caption{Comparison of the rate of adverse respiratory symptoms among volunteers assigned to different tasks.}\label{fig:summaries-good-barchart}
\end{figure}

From the graphic, it becomes clear that a higher fraction of volunteers
cleaning wildlife experienced adverse symptoms compared with those
without oil exposure. In fact, volunteers cleaning wildlife were 1.79
times more likely to experience adverse respiratory symptoms.

The key to a good summary is understanding the question of interest and
addressing this question through a useful characterization of the
variability.

\chapter{Assessing the Evidence (Quantifying the Variability in
Estimates)}\label{SamplingDistns}

Again, the goal of statistical inference is to use the sample as a
snapshot of the underlying population (Figure
\ref{fig:samplingdistns-statistical-process}). There are generally three
reasons people distrust this process:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Fear that the sample does not represent what is going on in the
  population.
\item
  Fear that we cannot make a conclusion with a sample of size \(n\)
  (wanting more data).
\item
  Fear that one study is not enough to make a conclusion.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/Basics-Stat-Process} 

}

\caption{Illustration of the statistical process (reprinted from Chapter 1).}\label{fig:samplingdistns-statistical-process}
\end{figure}

We have already tackled the first reason in Chapter @ref(\#Data); if we
are to trust statistical results, we must collect data that is
representative of the underlying population. The second and third fears
above are tied together, though maybe not obviously. Before launching
into a slightly more formal discussion, consider the following thought
experiment.

\BeginKnitrBlock{example}[Free Throws]
\protect\hypertarget{ex:samplingdistns-free-throws}{}{\label{ex:samplingdistns-free-throws}
\iffalse (Free Throws) \fi{} }Your friends Dave lives for his Wednesday
``pick-up'' basketball games at the gym. One afternoon, while waiting
for a few more players to arrive Dave shoots 10 free throws, of which he
makes 3.
\EndKnitrBlock{example}

I imagine no one is ready to claim \emph{definitively} that Dave has a
30\% success rate from the free throw line. So, what can we say? Well,
if this set of 10 free throws is representative of Dave's free throw
performance, then we would say that 30\% is an estimate for his success
rate; that is, the statistic 30\% is a good guess at the unknown
parameter (overall success rate). There are two ways we might impove our
confidence in this estimate. First, we might consider a larger sample
size (make Dave shoot more free throws).

\BeginKnitrBlock{example}[Free Throws (cont.)]
\protect\hypertarget{ex:samplingdistns-free-throws2}{}{\label{ex:samplingdistns-free-throws2}
\iffalse (Free Throws (cont.)) \fi{} }Joe has also been waiting for a
few more players to arrive; however, Joe shoots 100 free throws (clearly
he has more time on his hands) of which he makes 30.
\EndKnitrBlock{example}

Again, we probably wouldn't claim \emph{definitively} that Joe has a
30\% success rate from the free throw line. But, assuming this set of
100 free throws is representative of his overall performance, then we
would be more confident in our guess for Joe's overall performance
compared with our guess for Dave's. The more shots we observe, the more
confidence we have in our estimate. This idea is known as the
\textbf{Law of Large Numbers}.

\BeginKnitrBlock{definition}[Law of Large Numbers]
\protect\hypertarget{def:defn-lln}{}{\label{def:defn-lln} \iffalse (Law of
Large Numbers) \fi{} }For our purposes, this essentially says that as a
sample size gets really large, a statistic will become arbitrarily close
(extremely good guess) of the parameter it estimates.
\EndKnitrBlock{definition}

Unfortunately, we may not be able to take a really large sample. It is
probably not feasible to have Dave or Joe shoot thousands of free
throws, for example. Our goal then becomes to somehow quantify the
confidence we have in our estimates \emph{given the sample size we have
available}. That is, given that we only saw Dave shoot 10 free throws,
can we quantify our confidence in that 30\% estimate of his free throw
success? Our ``confidence'' in an estimate is tied to the estimate's
repeatability --- ``if we were to repeat the study, how much would we
expect our estimate to change?'' This gets at the last fear; we know
that if we repeat a study, the results will change. Our job is to
quantify (keeping the sample size in mind) the degree to which the
results will change. That is, we need to quantify the \emph{variability}
in the estimate across repeated studies (known as sampling variability;
we told you statistics was all about variability). This is known as a
\textbf{sampling distribution}.

\BeginKnitrBlock{definition}[Sampling Distribution]
\protect\hypertarget{def:defn-sampling-distribution}{}{\label{def:defn-sampling-distribution}
\iffalse (Sampling Distribution) \fi{} }The distribution of a
\emph{statistic} across repeated samples.
\EndKnitrBlock{definition}

This is perhaps the most important of the \emph{Distributional Quartet};
it is the holy grail of statistical inference. Once we have the sampling
distribution, inference is straight-forward.

\BeginKnitrBlock{rmdfivefund}
\textbf{Fundamental Idea IV}: Variability is inherent in any process,
and as a result, our estimates are subject to sampling variability.
However, these estimates often vary across samples in a predictable way;
that is, they have a distribution that can be modeled.
\EndKnitrBlock{rmdfivefund}

\section{Conceptualizing the Sampling
Distribution}\label{conceptualizing-the-sampling-distribution}

The sampling distribution of a statistic is one of the most fundamental,
and yet one of the most abstract, concepts in statistics. It's name is
even confusing; the ``distribution of the sample'' (Definition
\ref{def:defn-distribution-sample}) and the ``sampling distribution''
(Definition \ref{def:defn-sampling-distribution}) are two different
things. In this section, we develop the idea of a sampling distribution;
then, we turn toward actually constructing it.

For the \protect\hyperlink{DeepwaterCase}{Deepwater Horizon Case Study},
consider the following question:

\begin{quote}
What proportion of volunteers assigned to clean wildlife will develop
adverse respiratory symptoms?
\end{quote}

In the sample, we observed 15 out of 54 such volunteers (27.8\% or a
proportion of 0.278). This proportion is a good estimate of the rate of
adverse symptoms in the population (assuming the sample is
representative, of course). Now, imagine randomly selecting 54 new
volunteers from the population (repeating the study). We could determine
what fraction of volunteers in this new sample experienced adverse
symptoms, expecting this value to be a bit different than what we
obtained in the first sample. Since this second sample is also
representative, it provides a good estimate of the parameter. Now, we
could take a third random sample of 54 volunteers and compute the
fraction in this third sample which experienced adverse symptoms. This
third sample also provides a good (and potentially unique) estimate of
the parameter. We could continue this process \(m\) times, for some
large number \(m\). This process is illustrated in Figure
\ref{fig:samplingdistns-sampling-distribution}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./images/SamplingDistns-Sampling-Distribution} 

}

\caption{Illustration of repeatedly sampling from a population.}\label{fig:samplingdistns-sampling-distribution}
\end{figure}

Consider what we are describing. With each representative sample, we
have constructed an estimate of the parameter. What we have kept from
each repetion is \emph{not} the values of the variables themselves
(whether the volunteers experienced adverse respiratory symptoms) but
rather we have retained the \emph{statistic} from each of \(m\) whole
new studies. So, which of these \(m\) estimates do we trust? All of
them. Since each sample is representative of the population, each
estimate is a good (not perfect) estimate of the parameter. Since we
have all these estimates, we could think about pooling the information
from all of them; describing the way in which they change from one
sample to another is the sampling distribution.

Notice that the sampling distribution is not describing a variable, it
is describing a \emph{statistic}. In order to construct a sampling
distribution, we would go through the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take a sample; record variables of interest.
\item
  Compute the statistic which estimates the parameter.
\item
  Repeat steps 1 and 2 a large number of times.
\item
  Examine the statistics collected.
\end{enumerate}

So, the sampling distribution is not a plot of the raw values of a
variable on individual subjects but a plot of statistics which summarize
entire samples. That is, the unit of observation has changed. While a
sample consists of individual subjects from the population, the sampling
distribution consists of individual samples from the population.

\BeginKnitrBlock{rmdtip}
Re-read the description of a sampling distribution several times, and
return to it often as you read through the text. It takes a while for
this to sink in, but if you truly grasp this one concept, the remainder
of statistical inference becomes much more accessible.
\EndKnitrBlock{rmdtip}

\section{Example of a Sampling
Distribution}\label{example-of-a-sampling-distribution}

Since this idea is so critical to grasping statistical inference, we are
going to walk through the process of generating a sampling distribution
for a known data generating process.

\BeginKnitrBlock{example}[Dice Experiment]
\protect\hypertarget{ex:samplingdistns-dice}{}{\label{ex:samplingdistns-dice}
\iffalse (Dice Experiment) \fi{} }Consider an ordinary six-sided die; we
are interested in the proportion of times that rolling the die will
result in a 1. Putting this in the language of the statistics, we have
the following:

\begin{itemize}
\tightlist
\item
  The \emph{population} of interest is all rolls of the die. Notice that
  this population is infinitely large as we could roll the die forever.
\item
  The \emph{variable} is the resulting value from the roll. Since this
  can take on only one of six values, this is a categorical variable.
\item
  The \emph{parameter} of interest is the proportion of rolls that
  result in a 1.
\end{itemize}

Our goal is to construct the sampling distribution of the proportion of
rolls that result in a 1 when the die is rolled 20 times.
\EndKnitrBlock{example}

What makes this example unique is that we know the value of the
parameter. Because of the physical properties of a die, we know that the
probability a roll results in a 1 is \(\theta = 1/6\). So, statistical
inference is not needed here. This example simply provides a simple
vehicle for studying sampling distributions. Going back to the steps for
creating a sampling distribution described in the previous section, we
have the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Roll a die 20 times, each time recording the resulting value.
\item
  Compute the proportion of times (out of the 20) the resulting value
  was a 1.
\item
  Repeat steps 1 and 2 a large number of times (let's say 500).
\item
  Plot the resulting values; there should be 500 proportions that we are
  keeping.
\end{enumerate}

Notice that we are actually rolling a die 10000 times (20 rolls repeated
500 times); we only keep 500 values (one proportion for each set of 20
rolls). This is something you could physically do at home. For example,
the first sample might look like that in Figure
\ref{fig:samplingdistns-dice-example}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./images/SamplingDistns-Dice-Example} 

}

\caption{Potential sample of rolling a cie 20 times.}\label{fig:samplingdistns-dice-example}
\end{figure}

For this particular sample, the proportion in the sample (our statistic
of interest) would be 0.25 (\(5/20\)). That is the value we would
record. We then repeat this 499 more times. You could try a few out
yourself using \href{https://www.random.org/dice/?num=20}{an online
simulator}. Figure \ref{fig:samplingdistns-dice-dotplot} shows the
resulting proportions for 500 samples of size 20 each.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/samplingdistns-dice-dotplot-1} 

}

\caption{Sampling distribution for the proportion of 20 dice rolls which result in a 1.  The distribution is based on repeating the sampling process 500 times.}\label{fig:samplingdistns-dice-dotplot}
\end{figure}

With modern computing power, there is no need to restrain ourselves to
repeating the study 500 times. A simple computer program could replicate
rolling the dice thousands of times. Figure
\ref{fig:samplingdistns-dice-histogram} is the sampling distribution for
the proportion of rolls that result in a 1 based on a sample of size 20
repeating the study 50000 times.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/samplingdistns-dice-histogram-1} 

}

\caption{Sampling distribution for the proportion of 20 dice rolls which result in a 1.  The distribution is based on repeating the sampling process 50000 times.}\label{fig:samplingdistns-dice-histogram}
\end{figure}

Notice that the sampling distribution is centered around the true value
of the parameter (\(\theta = 1/6\)). In general, the sampling
distribution of statistics, when taken from a random sample, center on
the true value of the parameter. This is the unbiased nature of the data
coming out; random samples are representative of the population.
Similarly, note that while no one sample (remember, each value in the
distribution represents a statistic from a sample of 20 values) is
perfect, no samples produce values which are far from the true
parameter. That is, a representative sample may not be perfect, but it
will give a \emph{reasonable} estimate of the parameter. Notice that
these properties hold even though we had a relatively small sample size
(\(n = 20\) coin flips).

\BeginKnitrBlock{rmdkeyidea}
The size of the sample is not as important as whether it is
representative. A small representative sample is better for making
inference than a large sample which is biased.
\EndKnitrBlock{rmdkeyidea}

One of the most useful things about the sampling distribution is that it
gives us an idea of how much we might expect our statistic to change
from one sample to another. Based on Figure
\ref{fig:samplingdistns-dice-histogram}, we could say that if we roll a
die 20 times, the proportion of rolls which result in a 1 is most likely
to be between 0.05 and 0.30 (so somewhere between 1 and 6 ones out of
the 20 rolls). It would be \emph{extremely} rare to have 12 of the 20
rolls result in a 1 (notice how small the bar is on the 0.6 proportion).
The sampling distribution is therefore giving us an idea of the
variability in our statistic.

Remember, our goal was to account for the variability in the statistic
(how much it changes from one sample to another) \emph{while accounting
for the sample size}. How is this done? When forming the sampling
distribution, we repeated the study. For each replication, we obtained a
new sample that \emph{had the same size as the original}. So, the sample
size is baked into the sampling distribution. To see the impact of
taking a larger sample, consider rolling a six-sided die 60 times
instead of 20 times. When we build the sampling distribution, each
replication will then involve repeating the process with 40 new rolls.
Figure \ref{fig:samplingdistns-dice-histogram2} shows the sampling
distribution of the proportion of 60 rolls which result in a 1 using
50000 replications. Notice that the distribution is still centered on
the true parameter \(\theta = 1/6\). The primary difference between this
figure and the last is that when we increased the sample size, the
sampling distribution narrowed.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/samplingdistns-dice-histogram2-1} 

}

\caption{Sampling distribution for the proportion of 60 dice rolls which result in a 1.  The distribution is based on repeating the sampling process 50000 times.}\label{fig:samplingdistns-dice-histogram2}
\end{figure}

We all have this intuition that ``more data is better.'' In truth, we
should say ``more \emph{good} data is better.'' By ``better,'' we mean
that the statistic is less variable. Notice that we have to be careful
here. We are not saying that the \emph{sample} has less variability; we
are saying the \emph{statistic} has less variability. That is, we are
more confident in our estimate because we do not expect it to change as
much from one sample to the next. From Figure
\ref{fig:samplingdistns-dice-histogram2}, we have that if we roll the
die 60 times, we expect the proportion of 1's to be somewhere between
0.1 and 0.25 (somewhere between 6 and 15 ones out of the 60 show up).
The proportion is varying much less from one sample to the next.

\BeginKnitrBlock{rmdkeyidea}
Larger samples result in \emph{statistics} which are less variable. This
shows itself in the sense that the sampling distribuiton is narrower.
\EndKnitrBlock{rmdkeyidea}

\BeginKnitrBlock{rmdtip}
Students often believe that a large sample reduces the variability in
the data. That is not true; a large sample reduces the variability in
the \emph{statistic}.
\EndKnitrBlock{rmdtip}

\section{Modeling the Sampling
Distribution}\label{modeling-the-sampling-distribution}

Let's return to the \protect\hyperlink{CaseDeepwater}{Deepwater Horizon
Case Study}. In particular, suppose we are tyring to address the
following question:

\begin{quote}
What proportion of volunteers assigned to clean wildlife will develop
adverse respiratory symptoms?
\end{quote}

We have an estimate for this proportion (\(p = 0.278\)) based on the
observed sample. Based on the discussion in the previous section, we
know the sampling distribution of this proportion can help us quantify
the variability in the estimate. Figure
\ref{fig:samplingdistns-deepwater-histogram} represents the sampling
distribution of this proportion. From the graphic, we would not expect
the proportion of volunteers who experience adverse respiratory symptoms
to move much beyond 0.15 and 0.4 if we were to repeat the study; it
would almost certainly not move beyond 0.1 and 0.5 if we were to repeat
the study.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/samplingdistns-deepwater-histogram-1} 

}

\caption{Sampling distribution for the proportion of volunteers assigned to wildlife who will develop adverse symptoms based on a sample of 54 volunteers.}\label{fig:samplingdistns-deepwater-histogram}
\end{figure}

Now, you might ask ``wait, where did this sampling distribution come
from? There is no way you actually repeated the study 50000 times,
right?'' Right. In the previous section, we described building the
sampling distribution through repeated sampling. But, in practice, this
is never practical; if it were, we would have just conducted a bigger
sample to begin with. Generally, cost is the limiting factor in choosing
a sample size; so, we only have a limited set of data to work with. So,
the sampling distribution is critical to making inference, but we cannot
take multiple samples to make it. Where does that leave us? The
answer\ldots{}modeling. Our goal is to construct a model of the sampling
distribution that we can use to make inference.

There are three general techniques for modeling the sampling
distribution of a statistic:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Build an empirical model.
\item
  Build an analytical model using probability theory.
\item
  Build an analytical model appealing to approximations.
\end{enumerate}

We will focus on the first approach; the latter two approaches are
discussed in the last unit of the text. The idea in constructing an
empirical model is to mimic the discussion above regarding the
construction of a sampling distribution. Our description references
Figure \ref{fig:samplingdistns-bootstrap} often. We are limited by our
resources; because of time and money constraints, we cannot resample
from the population (crossed off resamples). So, we pretend for a moment
that our original sample (colored in green in the figure) is the
population for a moment. Our idea is to randomly sample from this
original sample, creating a \emph{bootstrap resample} (colored in orange
in the figure). Forgive the non-technical terms here, but since the
orange ``blob'' is a random sample from the green ``blob,'' then it is
representative of the green blob. Therefore, if we construct an estimate
\(\widehat{\theta}^*\) from the orange blob (the star denotes a
statistic from a resample), then it should be close to the statistic
\(\widehat{\theta}\) from the green blob; but, since this green blob is
representative of the population, \(\widehat{\theta}\) should be close
to the true parameter \(\theta\). Therefore, we have that \[
\widehat{\theta}^* \approx \widehat{\theta} \approx \theta \Rightarrow \widehat{\theta}^* \approx \theta
\]

That is, the bootstrap resamples produce statistics which are good
estimates of the parameter from the underlying population. The benefit
here is that the bootstrap resamples are constructed in the computer.
And, given today's computing power, we are not limited by time or money
(10000 bootstrap resamples can often be taken in a matter of seconds).
If you want to see this process in action, we encourage you to check out
the free online app located at
\url{http://www.lock5stat.com/StatKey/bootstrap_1_cat/bootstrap_1_cat.html}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./images/SamplingDistns-Bootstrap} 

}

\caption{Illustration of modeling the sampling distribution via the Bootstrap.}\label{fig:samplingdistns-bootstrap}
\end{figure}

Again, the idea is to mimic in the computer the resampling that we were
unable to do in real life. This process is known as the
\textbf{bootstrap} procedure.

\BeginKnitrBlock{definition}[Bootstrap]
\protect\hypertarget{def:defn-bootstrap}{}{\label{def:defn-bootstrap}
\iffalse (Bootstrap) \fi{} }A method of modeling the sampling
distribution by repeatedly resampling from the original data.
\EndKnitrBlock{definition}

A couple of notes on the actual implementation of a bootstrap procedure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each resample is the same size as the original sample.
\item
  Each resample is taken \emph{with replacement}; that means that values
  from the original sample can show up multiple times. This is like
  ``catch and release'' fishing.
\item
  Typically, between 3000 and 10000 bootstrap resamples are taken.
\end{enumerate}

We will avoid actual computation throughout the text, but several
resources are available for implementing the bootstrap procedure (and
its many variants) in various computer programming languages and
software packages.

\BeginKnitrBlock{rmdtip}
Students often believe that the bootstrap ``creates more data.'' This is
not true. Instead, the boostrap resamples from the existing data. This
highlights the need to have a representative sample when performing
analysis.
\EndKnitrBlock{rmdtip}

As an example, for the \protect\hyperlink{CaseDeepwater}{Deepwater
Horizon Case Study}, we performed the following steps to create Figure
\ref{fig:samplingdistns-deepwater-histogram}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Select 54 volunteers at random (with replacement) from the original
  sample of 54 volunteers who had been assigned to clean wildlife.
\item
  For our resample, we computed the proportion of those individuals who
  had experienced adverse respiratory symptoms.
\item
  We repeated steps 1 and 2 several thousand times, retaining the
  bootstrap statistics from each bootstrap resample.
\item
  We plotted the distribution of the bootstrap statistics.
\end{enumerate}

\section{Using a Model for the Sampling Distributions (Confidence
Intervals)}\label{using-a-model-for-the-sampling-distributions-confidence-intervals}

From Figure \ref{fig:samplingdistns-deepwater-histogram}, we observed
that we would not expect the proportion of volunteers who had
experienced adverse symptoms to move much beyond 0.15 to 0.4 if we were
to repeat the study. How does this help us in performing inference?
Remember that each value in the bootstrap model for the sampling
distribution is an estimate of the underlying parameter. So, we can
think of the above model as showing us what good estimates of the
parameter look like. Another way of saying it: the model for the
sampling distribution shows us the \emph{reasonable} (or
\emph{plausbile}) values of the parameter. Here, by ``reasonable,'' we
mean values of the parameter for which the data is \emph{consistent}.
Consider the following statements (which are equivalent):

\begin{itemize}
\tightlist
\item
  Based on our sample of 54 volunteers, it is reasonable that the
  proportion of volunteers assigned to clean wildlife who would
  experience adverse respiratory symptoms is between 0.15 and 0.4.
\item
  Our sample of 54 volunteers is consistent with between 15\% and 40\%
  of all volunteers assigned to clean wildlife experiencing adverse
  respiratory symptoms.
\end{itemize}

We have just conducted inference for ``estimation'' type questions. We
are able to provide an estimate for the parameter which acknowledges
that the data is not perfect and there is variability in sampling
procedures. That variability incorporated itself into constructing an
estimate that is an interval instead of a single point.

The above interval was chosen arbitrarily by just looking at the
sampling distribution and capturing the peak of the distribution. If we
want to be more formal, we might try to capture the middle 95\% of
values. This is known as a \textbf{confidence interval}.

\BeginKnitrBlock{definition}[Confidence Interval]
\protect\hypertarget{def:defn-confidence-interval}{}{\label{def:defn-confidence-interval}
\iffalse (Confidence Interval) \fi{} }An interval (range of values)
estimate of a parameter that incorporates the variability in the
statistic. A k\% confidence interval will contain the parameter of
interest in k\% of repeated studies.
\EndKnitrBlock{definition}

If we were to capture the middle 95\% of statistics, a 95\% confidence
interval, we would obtain an interval of (0.167, 0.407), as shown in
Figure \ref{fig:samplingdistns-deepwater-95ci}.

\textbackslash{}begin\{figure\}

\{\centering \includegraphics[width=0.8\linewidth]{./Images/samplingdistns-deepwater-95ci-1}

\}

\textbackslash{}caption\{Construction of a 95\% confidence interval via
bootstrapping for the proportion of volunteers assigned to wildlife who
will develop adverse symptoms based on a sample of 54
volunteers.\}\label{fig:samplingdistns-deepwater-95ci}
\textbackslash{}end\{figure\}

Confidence intervals are often misinterpreted; this comes from their
dependence on repeated sampling. When thinking about confidence
intervals, think about playing a game of ring toss: you toss a ring in
hopes of landing on top of a target. The target is the parameter
characterizing the population. The confidence interval is like a ring.
Since the confidence interval is constructed from a model of the
sampling distribution, it changes with each sample; that is, the
confidence interval itself is a statistic. Just like in ring toss, the
ring moves with each toss, the confidence interval moves with each
sample. However, the target stays fixed. Because of this, the following
interpretations are \emph{incorrect}:

\begin{itemize}
\tightlist
\item
  There is a 95\% chance that the proportion of volunteers assigned to
  clean wildlife who will experience adverse symptoms is between 0.167
  and 0.407.
\item
  95\% of volunteers assigned to clean wildlife in our sample had a
  value between 0.167 and 0.407.
\end{itemize}

The first statement is incorrect because it treats the parameter as the
thing that is moving. Once the data has been collected, the confidence
interval is a fixed quantity; neither the estimate or the parameter is
moving; so, there is no probability left. Again, think about tossing a
ring; once the ring is tossed, you either captured the target or you did
not. There is no ``I captured the target with 95\% probability.''

The second statement is absurd in this case. A volunteer either had
respiratory symptoms or they did not; so, saying they had a value
between 0.167 and 0.407 is ridiculous. However, this is a common
misconception with confidence intervals. They are describing reasonable
values of the parameter, not values of the variable in the sample or
population. We recommend sticking to interpreting a confidence interval
as specifying reasonable values for the parameter.

\BeginKnitrBlock{rmdtip}
Confidence intervals \emph{do not} provide a probability that the
parameter is inside. Nor do they tell you anything about the individual
values in a sample or population. They describe reasonable values of the
parameter.
\EndKnitrBlock{rmdtip}

\BeginKnitrBlock{rmdkeyidea}
Confidence intervals specify \emph{reaonable} values of the parameter
based on the data observed.
\EndKnitrBlock{rmdkeyidea}

It may seem like a good idea to make a 100\% confidence interval to be
sure we always capture the parameter. But, such intervals are not
helpful in practice. For example, a 100\% confidence interval for the
proportion of volunteers experiencing adverse symptoms would be (0, 1).
But, this is useless; it essentially says that the proportion has to be
a number between 0 and 1, but we already knew that. Therefore, we must
balance the confidence we desire with the amount of information the
interval conveys.

\BeginKnitrBlock{rmdtip}
If you want both a high level of confidence but also a narrow interval,
increase the sample size. As the sample size increases, the variability
in the statistic decreases leading to a narrower interval.
\EndKnitrBlock{rmdtip}

\BeginKnitrBlock{rmdtip}
95\% confidence intervals are the most common in practice; however,
90\%, 98\%, and 99\% intervals are also used. It is extremely rare to
use less than a 90\% CI.
\EndKnitrBlock{rmdtip}

\section{Bringing it All Together}\label{bringing-it-all-together}

Consider the following question:

\begin{quote}
Is there evidence that more than 1 in 5 volunteers assigned to clean
wildlife will develop adverse respiratory symptoms?
\end{quote}

Let's answer this question using a confidence interval. Based on the
data obtained, we found that the 95\% confidence interval (CI) for the
proportion of volunteers experiencing adverse symptoms to be (0.167,
0.407). Is this data consistent with more than 1 in 5 volunteers
developing adverse symptoms? Yes, since there are proportions within
this interval which are larger than 0.2. But, \emph{consistency} is not
the same as \emph{evidence}; remember, evidence is the idea of ``beyond
a reasonable doubt.'' After all, is this data \emph{consistent} with
less than 1 in 5 volunteers developing adverse symptoms? Yes, since
there are proportions within this interval which are less than 0.2.

Confidence intervals specify reasonable values --- those values of the
parameter which are consistent with the data. This data is then
consistent with proportions that are both less than 0.2 and greater than
0.2. So, what can we say then? We can say that there is \emph{not}
evidence that more than 1 in 5 volunteers assigned to clean wildlife
will develop adverse respiratory symptoms, but the data is consistent
with this claim.

More, we can say that there \emph{is evidence} that the proportion of
volunteers who will develop symptoms is less than 0.5; further, the
proportion of volunteers who will develop symptoms is larger than 0.1.
That is, the data provides evidence that more than 10\% of volunteers
will develop adverse symptoms, but this percentage will not be larger
than 50\%. How do we know? Because values less than 10\% are not
reasonble values of the parameter based on the 95\% CI. Values like 0.1
are outside of the confidence interval and are therefore not reasonable.
Similarly, values above 0.5 are outside the confidence interval and are
therefore not reasonable.

The power of a model for the sampling distribution is that it allows us
to determine which values of a parameter are reasonable and which values
are not.

\chapter{Quantifying the Evidence (Rejecting Bad
Models)}\label{NullDistns}

Again, the goal of statistical inference is to use the sample as a
snapshot of the underlying population (Figure
\ref{fig:nulldistns-statistical-process}). Recall that there are
essentially two categories of questions we ask when trying to perform
inference:

\begin{itemize}
\tightlist
\item
  Estimation: what \emph{proportion} of volunteers who clean wildlife
  following an oil spill will experience adverse respiratory symptoms?
\item
  Model Consistency: is it reasonable that no more than 1 in 5
  volunteers who clean wildlife following an oil spill will experience
  adverse respiratory symptoms?
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/Basics-Stat-Process} 

}

\caption{Illustration of the statistical process (reprinted from Chapter 1).}\label{fig:nulldistns-statistical-process}
\end{figure}

In the previous chapter we addressed these questions through the use of
confidence intervals --- by specifying reasonable values of the
parameters through a model of the sampling distribution. However, when
working with questions of the second type (model consistency), there is
a second approach; this latter approach is useful when confidence
intervals cannot be constructed for the particular question of interest
(see Unit 2).

Remember, assessing model consistency is similar to performing a trial
in a court of law. After gathering the evidence, the jury is left with
the following decision:

\begin{itemize}
\tightlist
\item
  Assuming the defendant is innocent, if the evidence is unlikely to
  have occurred (so is not consistent with innocence), then they vote
  ``guilty.''
\item
  Assuming the defendant is innocent, if the evidence is reasonably
  likely to have occurred (so is consistent with innocence), then they
  vote ``not guilty.''
\end{itemize}

The goal in this section is to somehow quantify the evidence against a
particular model to determine if we can say that the data is not
consistent with the given model.

\section{Some Subtleties}\label{some-subtleties}

In a U.S. trial, there are some subtleties that we should be aware of,
as they also creep up in statistical analyses and have implications for
how we interpret statistical results. First, the jury weighs the
evidence \emph{under the assumption of innocence}. That is, they first
develop a working hypothesis (the defendant is innocent). Then, the
likelihood of the evidence \emph{under this assumption} is determined.
For example, if a defendant were innocent of murder, it is unlikely to
have five eye witnesses stating the defendant was seen standing over the
victim, holding the murder weapon, and screaming ``I killed him!'' Since
that evidence does not jive with innocence, the jury convicts. If,
however, the only evidence is that five eye witnesses place the
defendant in the same city as the victim and the defendant matches the
description of someone seen fleeing the crime scene, then the jury would
not convict. Why? Because the evidence, while pointing toward guilt, is
not overwhelming; these things could have happened by chance alone.
Therefore, the evidence, while consistent with guilt does not provide
evidence for guilt.

Also notice that a jury saying ``not guilty'' is not the same as saying
``innocent.'' That is, a lack of evidence to convict does not imply the
defendant is innocent. A lack of evidence is simply a lack of evidence.
The defendant may still be guilty, but the evidence has just not proven
it.

Similarly, when assessing model consistency, we will weigh the data
\emph{under the null hypothesis} (our working assumption). Then, the
likelihood of our data occurring by chance alone \emph{under this
hypothesis} is determined. If that likelihood is small (data is not
consistent with the null hypothesis), we can conclude the data supports
the alternative hypothesis (guilty). If, however, that likelihood is
large (data is consistent with the null hypothesis), we can only
conclude that the data is consistent with the hypotheses. We are
\emph{not} able to say ``supports the null'' because that would be like
saying a defendant is innocent. We can't prove innocence because we
started by assuming it!

\section{Assuming the Null
Hypothesis}\label{assuming-the-null-hypothesis}

Consider the question we have been asking regarding the
\protect\hyperlink{CaseDeepwater}{Deepwater Horizon Case Study}:

\begin{quote}
Is there evidence that more than 1 in 5 volunteers assigned to clean
wildlife will develop adverse respiratory conditions?
\end{quote}

Remember, we framed this question through statements about a parameter
in Chapter \ref{Questions}:

\begin{quote}
\(H_0:\) the proportion of volunteers assigned to clean wildlife who
develop adverse respiratory symptoms is no more than 0.20.\\
\(H_1:\) the proportion of volunteers assigned to clean wildlife who
develop adverse respiratory symptoms exceeds 0.20.
\end{quote}

Within the sample we observed that 27.8\% of volunteers experienced
adverse symptoms, which is certainly more than the 0.20 in the claim;
therefore, the data is at least trending toward the alternative
hypothesis. However, it is possible that we just have a strange sample.
Remember in our discussion of sampling distributions in the previous
chapter, however, that we expect the estimate to vary from one sample to
another. Essentially, we need to know whether 27.8\% of volunteers
experiencing symptoms is a strong signal that the rate within the
popoulation is larger than 0.2 (1 in 5) or 27.8\% is simply a fluke that
might happen due to sampling variability. While we are going to be
attacking the question differently in this chapter than the previous, we
see that the key is still variability in the estimate. That is, we are
back to the \emph{Fourth Fundamental Idea of Inference}. As stated
above, in order to determine evidence for one statement (captured by the
alternative hypothesis), we begin by assuming the opposite statement
(captured by the null hypothesis) as our working assumption. That is, if
we want to know if 27.8\% of volunteers experiencing adverse symptoms is
``evidence,'' we need to figure out what we \emph{expect} to happen
\emph{if only 1 in 5 volunteers actually develop adverse respiratory
symptoms}.

Consider this last statement. It is equivalent to saying ``what type of
evidence would we expect for an innocent person?'' Only if we know what
to expect can we determine if the evidence in front of us is extreme
enough to convict. Only if we know what to expect can we determine if
the observed sample provides evidence in favor of the alternative. So,
we enter a fake world\ldots{}a world in which exactly 1 in 5 volunteers
actually develop respiratory symptoms. That is, we enter a world in
which the null hypothesis is true. Now, in this world, how do we know
what to expect? We construct the sampling distribution for the
proportion under this assumption that the null hypothesis is true; this
is known as the \textbf{null distribution}.

\BeginKnitrBlock{definition}[Null Distribution]
\protect\hypertarget{def:defn-null-distribution}{}{\label{def:defn-null-distribution}
\iffalse (Null Distribution) \fi{} }The sampling distribution of a
statistic \emph{if} the null hypothesis is true.
\EndKnitrBlock{definition}

To construct the null distribution, we do the following steps
(illustrated in Figure \ref{fig:nulldistns-null-distribution}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sample randomly from a fake population where the null hypothesis is
  true.
\item
  For each sample, compute the statistic of interest.
\item
  Repeat steps 1 and 2 several thousand times.
\item
  Plot the statistics retained from each sample.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./images/NullDistns-Null-Distribution} 

}

\caption{Illustration of constructing a null distribution.  Notice the similarity to constructing the sampling distributon.}\label{fig:nulldistns-null-distribution}
\end{figure}

Notice that these are the same steps as in constructing a sampling
distribution with the exception that instead of sampling from the
population of interest, we sample from a hypothetical population in
which the null distribution is true. With today's computational power,
we are able to make such samples possible similar to bootstrapping since
we can make the null population in a virtual world and sample from it.
That is, we are simulating what would happen if the null hypothesis were
true. Figure \ref{fig:nulldistns-deepwater-null} represents the null
distribution for the proportion of volunteers in a sample of 54 assigned
to clean wildlife which would develop adverse sympoms when the null
hypothesis is that the proportion is 0.20.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/nulldistns-deepwater-null-1} 

}

\caption{Null distribution for the proportion of volunteers assigned to clean wildlife experiencing adverse respiratory symptoms.  Null hypothesis is that the proportion is 0.20; this is based on a sample of size 54.}\label{fig:nulldistns-deepwater-null}
\end{figure}

\section{Using the Null Distribution}\label{using-the-null-distribution}

From the figure, we see that \emph{if the null hypothesis were true} ---
if only 1 in 5 volunteers assigned to clean wildlife experienced
symptoms --- then in a sample of 54 individuals, we would expect the
proportion who experienced symptoms to be somewhere between 0.1 and 0.3.
\emph{If the null hypothesis were true}, it would be nearly impossible
that half of the individuals experienced symptoms (since 0.5 is way off
in the tail of the distribution). The further in the tail region, the
more extreme the sample. The question is then how extreme is our sample?
Again, the null distribution is just setting up expectations; now, we
have to weigh the evidence against those expectations.

In our sample, we observed 27.8\% of volunteers who experienced
symptoms. Since 0.278 is towards the center of the distribution, we
would say that it is not an extreme sample. In order to quantify how
extreme (or not extreme) its, we find out what fraction of values are
more extreme (larger than in this case) than the value observed; that
is, what fraction of values appear in the right tail of the
distribution. Figure \ref{fig:nulldistns-deepwater-pvalue} illustrates
this computation. Based on the null distribution, there is a 10.6\%
chance that \emph{if the null hypothesis were true} --- only 1 in 5
volunteers actually experienced symptoms --- that in a random sample of
54 volunteers we would obtain data this extreme or moreso by chance
alone. Essentially, this tail area is quantifying the strength of the
evidence. The smaller this area, the further in the tail region our data
is; that is, our data is more unexpected. Therefore, small areas
indicate that the data (our evidence) does not jive with our
expectations under the null (innocence), forcing us to conclude the data
provides evidence \emph{against} the null hypothesis. In our case, since
the area is relatively large, our data is completely consistent with
what we might expect if the null were true. Therefore, in this case, we
conclude that there is no evidence that the rate of those experiencing
symptoms exceeds 1 in 5. This area is known as the \textbf{p-value}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/nulldistns-deepwater-pvalue-1} 

}

\caption{Likelihood of obtaining a sample as extreme or moreso as that of the original sample when the parameter of interest is the proportion of volunteers assigned to clean wildlife experiencing adverse respiratory symptoms.  Null hypothesis is that the proportion is 0.20; this is based on a sample of size 54.}\label{fig:nulldistns-deepwater-pvalue}
\end{figure}

\BeginKnitrBlock{definition}[P-Value]
\protect\hypertarget{def:defn-pvalue}{}{\label{def:defn-pvalue}
\iffalse (P-Value) \fi{} }The probability, assuming the null hypothesis
is true, that we would observe a statistic, by chance alone, as extreme
or moreso as that observed in our sample. This quantifies the strength
of evidence against the null hypothesis. Smaller values indicate
stronger evidence.
\EndKnitrBlock{definition}

It is natural to ask ``how small does the p-value need to be to prove a
statement?'' Like a trial, the weight of the evidence presented depends
on the context. In some studies, a p-value less than 0.01 may be strong
evidence while in other studies a p-value less than \(10^{-6}\) is
required. And, as in a trial, it is not only the strength of the
evidence but the type of evidence presented (DNA evidence may be
stronger than fingerprint evidence). In statistics, it is important to
consider the effect size as well as the p-value. That is, consider
whether the difference between the estimate and the null value is
actually large; this is always based on subject-matter expertise. It is
often helpful to report a confidence interval alongside a p-value.

\BeginKnitrBlock{rmdtip}
While what constitutes ``significant'' may vary from discipline to
discipline, the list below is a good rule of thumb:

\begin{itemize}
\tightlist
\item
  \(p \geq 0.1\): no evidence against the null hypothesis.
\item
  \(0.05 \leq p < 0.1\): weak evidence against the null hypothesis.
\item
  \(0.01 \leq p < 0.05\): some evidence against the null hypothesis.
\item
  \(0.001 \leq p < 0.01\): evidence against the null hypothesis.
\item
  \(p < 0.001\): strong evidence against the null hypothesis.
\end{itemize}

As with any rule of thumb, this should not be considered binding and may
vary depending on the application.
\EndKnitrBlock{rmdtip}

Like confidence intervals, p-values are often misinterpreted. In fact,
they have become so abused that some researchers argue against their
use. It is our opinion that the p-value can be a useful tool once it is
appropriately understood; so, let's dispell some of these
misconceptions. Consider these \emph{incorrect} statements regarding the
p-value obtained for the \protect\hyperlink{CaseDeepwater}{Deewater
Horizon Case Study} computed above:

\begin{itemize}
\tightlist
\item
  There is a 10.6\% chance that only 1 in 5 volunteers assigned to clean
  wildlife will experience adverse symptoms.
\item
  Since the p-value is large, there is evidence (or the data supports
  the claim) that 1 in 5 volunteers assigned to clean wildlife will
  experience adverse symptoms.
\end{itemize}

The first statement incorrectly assumes that there is some chance that
the null hypothesis is true. Remember, our two hypotheses are statements
about the parameter. One is true and other is not. Our ignorance does
not change this; therefore, it does not make sense to talk about the
probability of the null being true or false. Instead, our job is to
quantify the likelihood of the data \emph{assuming the null is true}.
The p-value is about the likelihood of the data under a particular model
(the null hypothesis).

The second statement makes the common mistake that a lack of evidence
for the alternative is evidence in favor of the null. A lack of evidence
is like a ``not guilty'' verdict. It simply means we were not convinced.
However, it does not mean that the defendant is innocent. All we are
saying with the large p-value in this case is that the data is
\emph{consistent} with only 1 in 5 volunteers getting adverse symptoms;
unfortunately, it is also \emph{consistent} with more than 1 in 5
volunteers getting adverse symptoms. This may be an unsatisfying
conclusion, but it is still a conclusion nonetheless. Our conclusion was
based on assessing the variability of a the statistic under a particular
model. This is captured in our last of the \emph{Five Fundamental Ideas
of Inference}:

\BeginKnitrBlock{rmdfivefund}
\textbf{Fundamental Idea V}: With a model for the distribution of a
statistic, we can quantify the error in our estimate and the likelihood
of a sample under a proposed model. This allows us to draw conclusions
about the corresponding parameter, and therefore the population, of
interest.
\EndKnitrBlock{rmdfivefund}

\section{Sampling Distributions vs.~Null
Distributions}\label{sampling-distributions-vs.null-distributions}

Clearly the sampling distribution and null distribution of a statistic
are closely related. The difference is that the null distribution is
created under a proposed model while the sampling distribution lets the
data speak for itself. It is worth taking just a moment to highlight the
differences in the use of these two components of the
\emph{Distributional Quartet}.

The sampling distribution is centered on the true value of the
parameter; the null distribution is centered on the null value. Once we
assume the null hypothesis is true, we have a value for the parameter;
as a result, we expect the sampling distribution under this assumption
(that is, the null distribution) to be centered on this hypothesized
value. So, null distributions are \emph{always} centered on the null
value.

Sampling distributions lead to confidence intervals by specifying
reasonable values of the parameter.

Null distributions lead to p-values by quantifying the likelihood of our
data under a proposed model.

\BeginKnitrBlock{rmdtip}
Model the sampling distribution to construct a confidence interval; to
assess a hypothesis the null value is overlayed on the sampling
distribution. Extreme values of the distribution are unreasonable values
for the parameter.

Model the null distribution to compute a p-value; to assess a
hypothesis, the statistic from the sample is overlayed on the null
distribution. Extreme values of the distribution are values which
provide evidence against the null hypothesis.
\EndKnitrBlock{rmdtip}

\chapter{Using the Tools Together}\label{RecapLanguage}

In this unit, we have introduced the key components in both the language
and logic of statistical inference. In fact, with a firm grasp of the
concepts in this unit, you should be able to read and interpret key
statistical findings. All statistical analyses make use of the
\emph{Five Fundamental Ideas of Inference} and alternate between the
members of the \emph{Distributional Quartet}. The context of each
problem differs, but the logic remains the same. In this chapter, we
present another analysis based on the
\protect\hyperlink{CaseDeepwater}{Deepwater Horizon Case Study},
annotating it along the way to see how these elements work together
fluidly to reach a conclusion. Specifically, we are interested in the
following question:

\begin{quote}
Are volunteers assigned to clean wildlife at higher risk of developing
adverse respiratory symptoms compared to those volunteers who do not
come into direct contact with oil? If so, estimate the increased risk.
\end{quote}

\section{Framing the Question (Fundamental Idea
I)}\label{framing-the-question-fundamental-idea-i}

We are really interested in whether the rate of respiratory symptoms in
one group of volunteers is larger than that in a second group.
Therefore, our working assumption is that there is no difference in the
rate of respiratory symptoms between these two groups. That is, we have

\begin{quote}
\(H_0:\) the rate of adverse respiratory symptoms is similar between
volunteers assigned to clean wildlife and those assigned to tasks which
do not involve direct exposure to oil.\\
\(H_1:\) the rate of adverse respiratory symptoms is greater for
volunteers assigned to clean wildlife and those assigned to tasks which
do not involve direct exposure to oil.
\end{quote}

We can also state this more formally with mathematical notation as
follows:

\begin{quote}
Let \(\theta_1\) be the rate of developing adverse respiratory symptoms
for volunteers assigned to clean wildlife.\\
Let \(\theta_2\) be the rate of developing adverse respiratory symptoms
for volunteers assigned to tasks without direct exposure to oil.\\
\(H_0: \theta_1/\theta_2 \leq 1\)\\
\(H_1: \theta_1/\theta_2 > 1\)
\end{quote}

The ratio \(\theta_1/\theta_2\) is known as the \emph{risk ratio} as it
captures the increased risk for one group compared to another.

Notice that this is a well-posed question as it centers on parameters
which characterize the population. Therefore, it can be answered with
appropriate data.

\begin{quote}
Distribution of the Population: Our questions of interest are about the
population and therefore focus on characterizing this distribution.
\end{quote}

\section{Getting Good Data (Fundamental Idea
II)}\label{getting-good-data-fundamental-idea-ii}

As we are working with previously collected data, we are unable to
design a good sampling scheme. The only thing we can do at this point is
critique the sample we have. The key question to ask ourselves is
whether there is any reason that these group of volunteers differs
systematically from other volunteers working oil spills. For example,
this oil spill occurred in the Gulf of Mexico; the majority of
volunteers were then naturally residents of Gulf states. It is possible
that these residents are somehow fundamentally different with respect to
their risk of developing adverse respiratory symptoms compared to the
remainder of the United States. If that is the case, the results of this
study would not generalize to oil spills occuring in the Atlantic.
However, it is probably reasonable to say that these results would apply
to future oil spills in the Gulf.

Also note that this was not a controlled experiment. Volunteers were not
randomly allocated to their assignments that we know of. Therefore, our
results could be somewhat limited. The two groups should be compared
regarding other attributes (this data is unavailable to us currently) in
order to determine if they are similar with respect to other variables
which may potentially confound the results.

\section{Presenting the Data (Fundamental Idea
III)}\label{presenting-the-data-fundamental-idea-iii}

The heart of this question is comparing the rate of adverse events in
each group. Figure \ref{fig:recaplanguage-deepwater-plot} makes this
comparison.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/recaplanguage-deepwater-plot-1} 

}

\caption{The risk of developing adverse respiratory symptoms for volunteers assigned to clean wildlife and those volunteers assigned to tasks which do not have direct exposure to oil.}\label{fig:recaplanguage-deepwater-plot}
\end{figure}

As seen in the figure, the rate of adverse respiratory symptoms is
larger in the group of volunteers assigned to wildlife cleanup. The rate
of respiratory symptoms is 1.79 times higher in the volunteers assigned
to clean wildlife compared to those assigned to tasks with no direct oil
exposure.

Notice that we reported the relative risk comparing the two groups as it
is directly tied to how we specified the hypotheses above.

\begin{quote}
Distribution of the Sample: graphics and numerical summaries
characterize this distribution, informing us about the underlying
population. This is possible as long as the sample is representative of
the population.
\end{quote}

\section{Quantifying the Variability in the Estimate (Fundamental Idea
IV)}\label{quantifying-the-variability-in-the-estimate-fundamental-idea-iv}

While we have an estimate for the increased risk of adverse respiratory
symptoms for those volunteers assigned to clean wildlife, the estimate
has not taken into account the variability in the sample. In order to
quantify this variability, we use a bootstrap procedure to model the
sampling distribution of the risk ratio. Observe that we focus on the
sampling distribution of the statistic that estimates the parameter of
interest.

Recall that the bootstrap mimics the process for generating a sampling
distribution. In this case, ``repeating the study'' involves collecting
data from not one, but two groups. So, we must resample both from the 54
volunteers who were assigned to clean wildlife and the 103 volunteers
assigned to tasks not involving direct oil exposure. Each time we
resample, we ensure that we select 54 volunteers who clean wildlife and
103 who do not. We need the process of the original study to be
maintained. Each time we resample from these groups, we compue the
relative risk and retain this value. Figure
\ref{fig:recaplanguage-sampling-distribution} shows the sampling
distribution for the relative risk comparing these two groups. Again, it
is important to note that we are not generating \emph{new} data; we are
\emph{resampling}/\emph{reusing} the original sample.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/recaplanguage-sampling-distribution-1} 

}

\caption{Model of the sampling distribution for the relative risk comparing volunteers assigned to clean wildlife to volunteers assigned to tasks not involving oil exposure.  The model was developed via bootstrapping using 50000 replications.}\label{fig:recaplanguage-sampling-distribution}
\end{figure}

Volunteers assigned to clean wildlife have are 1.79 times (95\% CI =
(0.92, 3.47)) more likely to experience adverse respiratory symptoms
compared to those volunteers assigned to tasks not requiring direct
exposure to oil. Our data suggests that our data is consistent with the
two groups having a similar risk but tends toward volunteers assigned to
clean wildlife being at increased risk..

\begin{quote}
Sampling Distribution: allows us to quantify the variability in the
statistic and provide an interval estimate for the paraemter which
incorporates this variability.
\end{quote}

\section{Quantifying the Evidence (Fundamental Idea
V)}\label{quantifying-the-evidence-fundamental-idea-v}

In order to quantify the departure of the data from our working
assumption that the risk is similar between the two groups, we rely on
the null distribution and compute a p-value.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/recaplanguage-null-distribution-1} 

}

\caption{Null distribution for the relative risk comparing volunteers assigned to clean wildlife to volunteers assigned to tasks not involving oil exposure.  The null hypothesis assumed the two groups of volunteers had a similar risk.  The nul distribution was developed via bootstrapping using 50000 replications.}\label{fig:recaplanguage-null-distribution}
\end{figure}

There is some evidence (p = 0.04) to suggest that volunteers exposed to
oil have an increased risk of developing adverse respiratory symptoms.
Given the estimated level of this increased risk, we believe this is
something health officials should investigate further. It would be worth
investigating further what aspects of the oil exposure may have caused
the increased risk to determine if it can be avoided in the future.

Note we are careful to not claim that the assignments have caused an
increase in the risk as this data is not from a controlled experiment.
This is one of the limitations of this analysis. However, if we are able
to assume the two groups are fairly similar with respect to other
attributes --- that is, there is no reason why people prone to
respiratory symptoms would become assigned to wildlife cleaning --- then
we may have some reason to believe the results are causal. We will
wrestle more with these types of conclusions in the next unit.

\begin{quote}
Null Distribution: allows us to quantify the level of evidence against a
particular claim; how strongly do the data disagree with the working
assumption.
\end{quote}

\section{Summary}\label{summary}

Notice that our analysis moved through the \emph{Five Fundamental
Ideas}, and in doing so made use or referenced each of the four
components of the \emph{Distributional Quartet}. As we move through the
remainder of the text, we will explore how these frameworks are used in
various other analysis scanarios. As we do, we reveal additional
concepts that underly statistical modeling.

We admit that there are several other questions that may be raised by
the above analysis. This unit is meant to introduce the big concepts of
inference. We will concern ourselves more with the details as we
progress through the text.

\part{Unit II: Comparing the Average Response Across
Groups}\label{part-unit-ii-comparing-the-average-response-across-groups}

\hypertarget{CaseOrganic}{\chapter{Case Study: Organic Foods and
Superior Morals}\label{CaseOrganic}}

``You are what you eat'' is a common phrase dating back to at least the
1820's used to suggest that if you want to be fit, you must eat healthy
foods. However, does the phrase extend to our personality as well as our
physique? Recent research has suggested that specific tastes (sweet
vs.~disgusting, for example) can influence moral processing. That is,
certain foods may lead us to be nicer to those around us or lead us to
be more judgemental. Organic foods are often marketed using phrases like
``pure'' or ``honest'' (Jessica Alba's
\href{https://www.honest.com/}{Honest Company}, for example); is there
some relationship between the consumption of organic foods and moral
behavior?

Dr.~Eskine of the Department of Psychological Sciences at Loyola
University sought to answer this question (Eskine
\protect\hyperlink{ref-Eskine2013}{2013}). He conducted a study to
investigate whether exposure to certain types of food had an effect on a
person's moral processing. Specifically, he randomized 62 Loyola
University undergradates to one of three food types: organic, comfort,
and control. Each participant received a packet containing pictures of
four food items from the assigned category:

\begin{itemize}
\tightlist
\item
  Organic Foods: apple, spinach, tomato, carrot
\item
  Comfort Foods: ice cream, cookie, chocolate, brownie
\item
  Control Foods: oatmeal, rice, mustard, beans
\end{itemize}

The control foods are those which are pre-packaged and are generally
considered staple items; organic foods are those which are associated
with a healthy diet; and, comfort foods were sweets. After viewing the
images for a set period of time, each participant received a packet
containing six counter-balanced moral transgressions. An example of such
a transgression is produced below:

\begin{quote}
Bob was at a family gathering when he met Ellen, a second cousin of his
that he had seen once or twice before. Bob found Ellen very attractive
and he asked her out on a date. Ellen accepted and they began to have a
romantic and sexual relationship. They often go on weekend trips to
romantic hotels in the mountains.
\end{quote}

Participants were then asked to rate the morality of the scenario on a
7-point scale (1 = ``not at all morally wrong'' to 7 = ``very morally
wrong''). The average of the morality scores across the six scenarios
was used as an overall measure of their moral expectations. A higher
value indicates high moral expectations (very strict) and a lower value
indicates lower moral expectations (very lenient).

Dr.~Eskine's analysis revealed that there was strong evidence
(\(p = 0.001\)) that participants' moral judgments differed, on average,
across the various food exposure groups. In particular, those exposed to
organic foods had higher moral expectations (an average mean moral
judgment of 5.58) compared to those experiencing comfort foods (average
mean moral judgment of 4.89) or control foods (average mean moral
judgment of 5.08). He therefore concluded that exposure to organic food
did lead to higher moral expectations.

Understandably, Dr.~Eskine's work caught the interest of various media
outlets and researchers. Two researchers within the Department of
Psychology at Domincan University in Illinois sought to replicate
Dr.~Eskine's work (Moery and Calin-Jageman
\protect\hyperlink{ref-Moery2016}{2016}). There were several components
to their research, but the first phase included a replication of
Dr.~Eskine's initial study with minor variants. They enrolled 124
college students into their study. The participants were presented with
the same food images as in Eskine's study with the exception that celery
was used instead of an apple for organic food. The same moral dilemmas
were given to participants. As in the original study, the average score
from the six moral dilemmas was the primary response for this study. A
subset of the collected data, showing three participants from each
treatment group (type of food shown), is presented below. The full
dataset\footnote{There were multiple phases to their research. The
  direct replication of Dr.~Eskine's work was Study 1, which is the
  dataset being considered in this text.} has been made available by the
researchers at the following website:
\url{https://osf.io/atkn7/wiki/home/}

\begin{table}

\caption{\label{tab:caseorganic-table}Subset of data from study characterizing moral behavior following exposure to various food categories.}
\centering
\begin{tabular}[t]{r|l|r}
\hline
Participant & Food Condition & Response (Avg of Moral Questions)\\
\hline
18 & organic & 5.500\\
\hline
20 & organic & 5.500\\
\hline
21 & organic & 6.333\\
\hline
1 & comfort & 6.000\\
\hline
2 & comfort & 3.500\\
\hline
3 & comfort & 6.167\\
\hline
4 & control & 5.167\\
\hline
10 & control & 7.000\\
\hline
12 & control & 6.833\\
\hline
\end{tabular}
\end{table}

\chapter{Framing the Question}\label{ANOVAquestions}

``Does exposure to various food types lead to different moral
expectations?'' The primary question from the
\protect\hyperlink{CaseOrganic}{Organic Food Case Study} is primarily
about the relationship between two variables: the response (moral
expectations; see Definition \ref{def:defn-response}) and the
\textbf{factor} of interest (food type).

\BeginKnitrBlock{definition}[Factor]
\protect\hypertarget{def:defn-factor}{}{\label{def:defn-factor}
\iffalse (Factor) \fi{} }Also referred to as the ``treamtent,'' a
categorical variable used to explain/predict a response.
\EndKnitrBlock{definition}

The majority of interesting research questions involve identifying or
quantifying the relationship between two variables. Despite the
complexity of the analyses sometimes employed to address these
questions, the basic principles are the same as those studied in Unit 1.
To begin, asking good questions involves defining the population of
interest and characterizing the variable(s) at the population level
through well-defined parameters.

The question of the \protect\hyperlink{CaseOrganic}{Organic Food Case
Study}, as stated above, is ill-posed. Almost certainly, there are
individuals for which exposure to organic foods may result in higher
moral expectations compared to exposure to comfort foods. However, there
are almost certainly individuals for which the effect is reversed ---
higher moral expectations are expected following exposure to comfort
foods compared with organic foods. That is, we expect there to be
\emph{variability} in the effect of food types on the resulting moral
expectations. The question needs to be refined.

While the study was conducted using college students, the original
question seems quite broad (we discuss this discrepancy in more detail
in the next chapter). Notice that the original question is not
predicated on \emph{consuming} various foods but simply \emph{exposure}
to various foods. The question itself is not limited to only those
individuals which purchase a specific type of food but concerns all
individuals. More, we really see that there are three groups of interest
--- those which are exposed to organic foods, those exposed to comfort
foods, and those exposed to the control foods. We can think of actually
three distinct populations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  All individuals exposed to organic foods.
\item
  All individuals exposed to comfort foods.
\item
  All individuals exposed to control foods.
\end{enumerate}

We now work to characterize the response within each of these three
populations. Since the response of interest is a numeric variable
(taking values between 1 and 7 with higher values indicating higher
moral expectations), summarizing the variable using the mean is
reasonable. That is, we might ask ``does exposure to various food types
lead to different moral expectations, \emph{on average}?'' Our question
now compares the mean response across the groups. In particular, our
question is looking for some type of difference in this mean response
across the groups; our working hypothesis is then that the groups are
all equivalent, on average. This could be framed in the following
hypotheses:

\begin{quote}
\(H_0:\) the average moral expectations are the same following exposure
to each of the three types of food.\\
\(H_1:\) the average moral expectations following exposure to food
differ for at least one of the three types.
\end{quote}

This is equivalent to expressing the hypotheses in terms of a relation
between the two variables:

\begin{quote}
\(H_0:\) there is no association between the type of food an individual
is exposed to and their moral expectations, on average.\\
\(H_1:\) there is is an association between the type of food an
individual is exposed to and their moral expectations, on average.
\end{quote}

We can represent these hypotheses mathematically as

\begin{quote}
\(H_0: \mu_{\text{comfort}} = \mu_{\text{control}} = \mu_{\text{organic}}\)\\
\(H_1:\) At least one \(\mu\) differs from the others
\end{quote}

where \(\mu_{\text{comfort}}\) is the mean moral expectations for
individuals exposed to comfort foods, etc. The question is now
well-posed --- it is centered on the population and captured through
parameters.

For this particular setting, there is an alternative way of thinking
about the population. You might argue that there are not three distinct
populations; instead, there is only a single population (all
individuals) and three different exposures (organic, comfort and control
foods). This is a reasonable way of characterizing the population. The
hypotheses remain the same:

\begin{quote}
\(H_0: \mu_{\text{comfort}} = \mu_{\text{control}} = \mu_{\text{organic}}\)\\
\(H_1:\) At least one \(\mu\) differs from the others
\end{quote}

The difference is in our interpretation of the parameters. We would
describe \(\mu_{\text{comfort}}\) as the mean moral expectations when an
individual is exposed to comfort foods. The distinction, while subtle is
to place emphasis on switching an individual from one group to another
instead of the groups being completely distinct. In fact, this latter
way of thinking is more in line with how the study was conducted.
Individuals were allocated to one of the exposure groups, suggesting
that exposure is something that could be changed for an individual.

From an analysis perspective, there is little difference between these
two ways of describing the population. The difference is primarily in
our interpretation. In many cases, we can envision the population either
way; however, there are a few instances where that is not possible.
Suppose we were comparing the average number of offspring of mice
compared to rats (a lovely thought, I know). It does not make sense to
think about changing a mouse into a rat; here, it only makes sense to
think about two distinct populations being compared on some metric. How
we describe the population is often related to the question we are
asking.

\BeginKnitrBlock{rmdtip}
How we describe the population is often connected to the study design we
implement. In a controlled experiment, we envision a single population
under various conditions. For an observational study, we generally
consider distinct populations.
\EndKnitrBlock{rmdtip}

\section{General Setting}\label{general-setting}

This unit is concerned with comparing the mean response of a numeric
variable across \(k\) groups. Let \(\mu_1, \mu_2, \dotsc, \mu_k\)
represent the mean response for each of the \(k\) groups. Then, we are
primarily interested in the following hypotheses:

\begin{quote}
\(H_0: \mu_1 = \mu_2 = \dotsb = \mu_k\)\\
\(H_1:\) At least one \(\mu\) differs from the others
\end{quote}

When there are only two groups (\(k = 2\)), then this can be written as

\begin{quote}
\(H_0: \mu_1 = \mu_2\)\\
\(H_1: \mu_1 \neq \mu_2\)
\end{quote}

\BeginKnitrBlock{rmdtip}
When there are two groups, it makes sense to say the means are equal or
not. While tempting to do something when there are more than two groups,
it is not possible. The opposite of ``all groups equal'' is \emph{not}
``all groups differ.'' The opposite of ``all groups equal'' is ``at
least one differs,'' which is what we are capturing with the above
hypotheses. Keep it simple and do not try to get fancy with the
notation.
\EndKnitrBlock{rmdtip}

Here we are writing things in the mathematical notation, but let's not
forget that every hypothesis has a context. Throughout this unit, we are
looking for some signal in the \emph{location} of the response across
the groups. Our working assumption then states that the groups are all
similar, \emph{on average}. This may not be the only comparison of
interest to make in practice. For example, it may not be the location
that is of interest but the spread of a process. In some applications,
managers would prefer to choose the process that is the most precise.
These questions are beyond the scope of this unit, but the concepts are
similar to what we discuss here.

\chapter{Study Design}\label{ANOVAdata}

Chapter \ref{Data} discussed the impact that the design of the study has
on interpreting the results. Recall that the goal of any statistical
analysis is to use the sample to say something about the underlying
population. Observational studies are subject to confounding. In order
to use the available data in order to make causal statements that apply
within the population, we need to address the confounding. There are two
ways of doing this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Conduct a controlled experiment. While we do not limit our discussion
  to controlled experiments in this unit, our discussion will emphasize
  the elements of a well designed experiment.
\item
  Use observational data and account for confounders. This will be the
  emphasis of the discussion in the subsequent unit.
\end{enumerate}

As discussed in Chapter \ref{Data}, controlled experiments balance the
groups being compared relative to the potential confounders. As a
result, such studies permit causal conclusions to be drawn.

\section{Aspects of a Well Designed
Experiment}\label{aspects-of-a-well-designed-experiment}

Generally speaking, there are three components to a well-designed study:
replication, randomization, and comparative groups.

As we have stated repeatedly, variability is inherit in any process. We
know there is variability in the population; not every subject will
respond exactly the same to each treatment. Therefore, our questions do
not seek to answer statements about individuals but about general trends
in the population. In order to establish these general trends, we must
allow that subject-to-subject variability be present within the study
itself. This is accomplished through \textbf{replication}, obtaining
data on multiple subjects from each group. Each subject's response would
be expected to be similar, with variability within the group due to the
inherit variability in the data-generating process.

\BeginKnitrBlock{definition}[Replication]
\protect\hypertarget{def:defn-replication}{}{\label{def:defn-replication}
\iffalse (Replication) \fi{} }Taking measurements on different subjects,
for which you expect the results to be similar. That is, any variability
is due to nautral variability within the population.
\EndKnitrBlock{definition}

When we talk about gathering ``more data,'' we typically mean obtaining
a larger number of replicates. Ideally, replicates will be obtained
through \emph{randomly selecting} from the underlying population to
ensure they are representative. The subjects are then \emph{randomly
allocated} to a particular level of the factor under study (randomly
allocated to a group). This random allocation breaks the link between
the factor and any potential confounders, allowing for causal
interpretations. However, if a link exists between the factor and the
response, that is preserved. These are the two aspects of
\textbf{randomization}.

\BeginKnitrBlock{definition}[Randomization]
\protect\hypertarget{def:defn-randomization}{}{\label{def:defn-randomization}
\iffalse (Randomization) \fi{} }Refers to the random \emph{selection} of
subjects which minimizes bias and random \emph{allocation} of subjects
which permits causal interpretation.
\EndKnitrBlock{definition}

\BeginKnitrBlock{rmdtip}
While students can typically describe random selection vs.~random
allocation, they often confuse their purpose. Random selection is to
ensure the sample is representative. Random allocation balances the
groups with respect to confounders.
\EndKnitrBlock{rmdtip}

We now have two sources of variability. That is, we have two reasons the
response will differ from one subject to another. Subjects assigned to
different groups may differ because of an effect due to the group; this
is a signal that we are tyring to identify with our hypotheses. Subjects
within the same group will differ due to natural variability.

Random allocation ensures the groups are balanced with respect to
confounders. However, there may still be a lot of variability within
each group. The more variability present, the more difficult it is to
detect a signal. The study will have more \textbf{power} to detect the
signal if the groups are similar. This is the idea of having
\textbf{comparative groups}.

\BeginKnitrBlock{definition}[Power]
\protect\hypertarget{def:defn-power}{}{\label{def:defn-power}
\iffalse (Power) \fi{} }Refers to the probability that a study will find
a signal when one really exists in the data generating process. This is
like saying ``the probability a jury will declare a defendant guilty
when he actually committed the crime.''
\EndKnitrBlock{definition}

\BeginKnitrBlock{definition}[Comparative Groups]
\protect\hypertarget{def:defn-comparative-groups}{}{\label{def:defn-comparative-groups}
\iffalse (Comparative Groups) \fi{} }The idea that the treatment groups
(levels of the factor under study) should be as similar as possible to
reduce external variability in the process.
\EndKnitrBlock{definition}

It is tempting to manually adjust the treatment groups to achieve what
the researcher views as balance. This temptation should be avoided as
balancing one feature of the subjects may lead to an imbalance in other
features. We want to rely on randomization. However, when there is a
particular feature which we would like to balance, we can employ
specialized randomization techniques. For example, if we would like an
equal number of males and females in a study, we can use stratified
random sampling (see Definition \ref{def:defn-stratified-random-sample})
to ensure equal representation. During the random allocation, we can
employ \textbf{blocking}, in which the random allocation to treatments
happens within a secondary feature.

\BeginKnitrBlock{definition}[Blocking]
\protect\hypertarget{def:defn-blocking}{}{\label{def:defn-blocking}
\iffalse (Blocking) \fi{} }One way of minimizing variability contributed
by an inherit characteristic. All observations that are linked through
the characteristic are grouped together and random allocation occurs
\emph{within} the block.
\EndKnitrBlock{definition}

\BeginKnitrBlock{example}[Overseeding Golf Greens]
\protect\hypertarget{ex:anovadata-golf}{}{\label{ex:anovadata-golf}
\iffalse (Overseeding Golf Greens) \fi{} }Golf is a major pasttime,
especially in southern states. Each winter, the putting greens need to
be overseeded with grasses that will thrive in cooler weather. This can
affect how the ball rolls along the green. Dudeck and Peeacock
(\protect\hyperlink{ref-Dudeck1981}{1981}) reports on an experiment that
involved comparing the ball roll for greens seeded with one of five
varieties of rye grass. Ball roll was measured by the mean distance (in
meters) that five balls traveled on the green. In order to induce a
constant initial velocity, each ball was rolled down an inclined plane.

Because the distance a ball rolls is influenced by the slope of the
green, 20 greens were placed into four groups in such a way that the
five greens in the same group had a similar slope. Then, within each of
these four groups, one green was randomly assigned to be overseeded with
one of the five types of Rye grass. The average ball roll was recorded
for each of the 20 greens.
\EndKnitrBlock{example}

The data for Example \ref{ex:anovadata-golf} is shown in Table
\ref{tab:anovadata-golf-table}.

\begin{table}

\caption{\label{tab:anovadata-golf-table}Data from Overseeding Golf Greens example.}
\centering
\begin{tabular}[t]{l|l|r}
\hline
Rye Grass Variety & Slope of Green Grouping & Mean Distance Traveled (m)\\
\hline
A & 1 & 2.764\\
\hline
B & 1 & 2.568\\
\hline
C & 1 & 2.506\\
\hline
D & 1 & 2.612\\
\hline
E & 1 & 2.238\\
\hline
A & 2 & 3.043\\
\hline
B & 2 & 2.977\\
\hline
C & 2 & 2.533\\
\hline
D & 2 & 2.675\\
\hline
E & 2 & 2.616\\
\hline
A & 3 & 2.600\\
\hline
B & 3 & 2.183\\
\hline
C & 3 & 2.334\\
\hline
D & 3 & 2.164\\
\hline
E & 3 & 2.127\\
\hline
A & 4 & 3.049\\
\hline
B & 4 & 3.028\\
\hline
C & 4 & 2.895\\
\hline
D & 4 & 2.724\\
\hline
E & 4 & 2.697\\
\hline
\end{tabular}
\end{table}

It would have been easy to simply assign 4 greens to each of the Rye
grass varieties; the random allocation would have balanced the slope of
the greens across the five varieties. However, an additional layer was
added to the design in order to control some of that additional
variability. In particular, greens with similar slopes were grouped
together; then, the random allocation to Rye grass varieties happened
\emph{within} groups of greens. As a result, what we see is that there
is one green of each type of slope for each Rye grass variety. This has
the effect of reducing variability due to nuisance characteristics of
the subjects.

\BeginKnitrBlock{rmdtip}
Blocking is often a way of gaining additional power when limited
resources require your study to have a small sample size.
\EndKnitrBlock{rmdtip}

The extreme case of blocking occurs when you have repeatedly measure the
response on the same subject under different treatment conditions. For
example, a pre-test/post-test study is an example of a study which
incorporates blocking. In this case, the blocks are the individual
subjects. The subjects then undergo each of the possible treatment
options. The rationale here is to use every subject as his or her own
control. The treatment groups are then as similar as possible.

We do note that blocking, while a powerful aspect of a design, has an
impact on the type of analysis that can be conducted. Specifically, we
must account for the blocking when conducting the analysis. We will
discuss this in Chapter \ref{ANOVAblocking}.

How did the design of the \protect\hyperlink{CaseOrganic}{Organic Food
Case Study} incorporate these aspects? First, we notice that random
allocation was utilized. Each of the 124 participants was randomly
assigned to one of three treatment groups (type of food to which the
particpant was exposed). The random allocation allows us to make causal
conclusions from the data as any confounder should be balanced across
the three foods. For example, subjects who adhere to a strict diet for
religious purposes would naturally tend toward organic foods and higher
moral expectations. However, for each subject like this exposed to
organic foods, there is someone like this (on average) who was assigned
to the comfort foods (on average). We also note that there is
replication. Instead of assigning only one subject to each of the three
treatment groups, we have several subjects within each group. This
allows us to evaluate the degree to which the results vary within a
particular treatment group.

The study does not make use of blocking. There are a couple of potential
reasons for this; first, with such a large sample size, the researchers
may not thought it necessary. Second, it could be that there was a
restriction on timee. For example, researchers may have considered
having students be exposed to each of the three types of food and
answering different scenarios after each. However, this would take a
longer amount of time to collect data. Third, it could be that
researchers were not concerned about any identifiable characteristics
that would generate additional variability. Regardless, the study is not
worse off because it did not use blocking; the design is still a very
reliable design.

While it is clear that random allocation was utilized in the design,
random selection was not. Students participating in the study are those
from a particular lecture hall. As a result, these students were not
randomly sampled from all college students (or even from the university
student body). As a result, we must really consider whether the
conclusions drawn from this study would apply to all college students
within the United States. Having additional information on their
demographics may help determine this, but in general, this is not
something that can be definitively answered. It is an assumption we are
either willing to make or not. More, notice that the original question
was not focused on college students; however, the sample consists only
of college students. This can impact the broader generalization of our
results. It is quite possible that we observe an effect in college
students that is not present in the larger population. We should always
be careful to ensure that the sample we are using adequately represents
the population.

\section{Collecting Observational
Data}\label{collecting-observational-data}

An inability to conduct a controlled experiment does not mean we neglect
study design. Random sampling is still critical to ensuring that the
data is representative of the population. Similarly, ensuring there are
a sufficient number of replications to capture the variability within
the data is an important aspect of conducting an observational study.
When collecting observational data, one of the most important steps is
constructing a list of potential confounders and then collecting data on
these variables. This will allow us to account for these confounders in
our analyses, as we will discuss in the next unit.

\chapter{Presenting the Data}\label{ANOVAsummaries}

When a research question involves the relationship between two or more
variables, such as comparing the mean response across levels of a
factor, successful presentations of the data which address the question
of interest \emph{partition the variability}. This key idea is essential
to both the data presentation and the data analysis.

We have already argued that variability makes addressing questions
difficult. If every subject had the same response to a particular
exposure, there would be no need for statistics. We would simply
evaluate one subject and determine which treatment to give. Statistics
exists because of the ambiguity created by variability in the responses.
In response to this variability, our statistical graphics and models
distinguish (partition) the various sources of variability. That is,
with any analysis, we try to answer the question ``why aren't all the
values the same? What are the reason(s) for the difference we are
observing?''

From the \protect\hyperlink{CaseOrganic}{Organic Food Case Study},
consider the primary question of interest:

\begin{quote}
Is there evidence of a relationship between the type of food a person is
exposed to and their moral expectations, on average, following exposure?
\end{quote}

What we are really asking is ``does the food exposure help explain the
differences in the moral expectations of individuals?'' We know that
there are differences in moral expectations between individuals. But,
are these differences solely due to natural variability (some people are
just inherently, possibly due to how they were raised, more or less
liberal with moral beliefs); or, is there some systematic component that
explains at least a portion of the differences between individuals. We
are thinking about partitioning the ``why the responses differ'' (the
variability).

A good graphic must then tease out how much of the differences in the
moral expectations is from subject-to-subject variability and how much
is due to the food exposure. First, consider a common graphic which is
\textbf{not} useful in this situation (Figure
\ref{fig:anovasummaries-bad-bar}).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/anovasummaries-bad-bar-1} 

}

\caption{Illustration of a poor graphic using the Organic Food Case Study; the graphic does not give us a sense of variability.  As a result, it is not clear how different these means really are.}\label{fig:anovasummaries-bad-bar}
\end{figure}

To determine an appropriate graphic, we need to remember that we want to
partition the variability. So, we must not only compare the differences
between the groups but also allow the viewer to get a sense of the
variability within the group. A common way of doing this within the
engineering and sciences is to construct side-by-side boxplots, as
illustrated in Figure \ref{fig:anovasummaries-organic-boxplot}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/anovasummaries-organic-boxplot-1} 

}

\caption{Comparison of the moral expectations for college students exposed to different types of food.}\label{fig:anovasummaries-organic-boxplot}
\end{figure}

From the graphic, we see that the moral expectation scores seem to have
nearly the same pattern in each of the exposure groups. More, the center
of each of the groups is roughly the same. That is, there does not
appear to be any evidence that the type of food to which a subject is
exposed is associated with moral expectations, on average.

Side-by-side boxplots can be helpful in comparing large samples as they
summarize the location and spread of the data. When the sample is
smaller, it can be helpful to overlay the raw data on the graphic in
addition to the summary provided by the boxplot. We might also consider
adding additional information, like the mean within each group. An
alternative to boxplots is to use violin plots which emphasize the shape
of the distribution instead of summarizing it like boxplots. Yet another
option is to construct density plots which are overlayed on one another.
This works when there are only a small number of groups; if the number
of groups is large, then placing the distributions side-by-side is much
more effective. A comparison of these approaches is in Figure
\ref{fig:anovasummaries-organic-comparison}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/anovasummaries-organic-comparison-1} 

}

\caption{Multiple ways to effectively compare the response across multiple groups.}\label{fig:anovasummaries-organic-comparison}
\end{figure}

Each of these plots is reasonable. What makes them useful in addressing
the question is that in each plot, we can compare the degree to which
the groups differ relative to the variability within a group. That is,
we partition the variability. With each plot, we can say that one of the
reasons the groups differ is because of exposure to different food
types; however, this difference is extremely small relative to the fact
that regardless of which food group you were exposed to, the variability
in moral expectations with that group is quite large. Since the
predominant variability in the moral exposure is the variability within
the groups, we would say there is no signal here. That is, there is no
evidence that the average scores differ across food exposure groups.

The key to a good summary is understanding the question of interest and
building a graphic which addresses this question through a useful
characterization of the variability.

\chapter{Quantifying the Evidence}\label{ANOVAteststat}

Figure \ref{fig:anovateststat-boxplots} displays a numeric response
across three groups for two different datasets. Consider the following
question:

\begin{quote}
For which dataset is there \emph{stronger} evidence that the response is
associated with the grouping variable?
\end{quote}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/anovateststat-boxplots-1} 

}

\caption{Simulated data illustrating that signal strength is determined by partitioning variability. There is a clear signal (difference in the location across groups) for Dataset A but not for Dataset B.}\label{fig:anovateststat-boxplots}
\end{figure}

Nearly everyone will say that Dataset A provides stronger evidence of a
relationship between the grouping variable and the response. We
generated these data such that the mean for Groups I, II and II are 5, 6
and 7, respectively, \emph{for both Datasets A and B}. While there is a
difference, on average, in the response across the groups in both cases,
it is correct that Dataset A provides stronger evidence for that
relationship. The real question is ``what is it that leads everyone to
make the same conclusion when we have not yet discussed how to analyze
this data?'' When we ask students why they feel Dataset A provides
stronger evidence, we typically hear that it is because the ``gaps''
between the groups ``look bigger.'' In essence, that is exactly right!

\section{Partitioning Variability}\label{partitioning-variability}

Subconsciously, when we are deciding whether there is a difference
between the groups, we are partitioning the variability in the response.
We are essentially describing two sources of variability: the
variability in the response caused by subjects belonging to different
groups and the variability in the response within a group (Figure
\ref{fig:anovateststat-partition-variability}). In both Datasets A and B
from Figure \ref{fig:anovateststat-boxplots}, the \textbf{between-group
variability} is the same; the difference in the means from one group to
another is the same in both cases. However, the \textbf{within-group
variability} is much smaller for Dataset A compared to Dataset B.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./images/ANOVATestStat-Partition-Variability} 

}

\caption{Illustration of partitioning the variability in the response to assess the strength of a signal.}\label{fig:anovateststat-partition-variability}
\end{figure}

\BeginKnitrBlock{definition}[Between Group Variability]
\protect\hypertarget{def:defn-between-group-variability}{}{\label{def:defn-between-group-variability}
\iffalse (Between Group Variability) \fi{} }The variability in the
average response from one group to another.
\EndKnitrBlock{definition}

\BeginKnitrBlock{definition}[Within Group Variability]
\protect\hypertarget{def:defn-within-group-variability}{}{\label{def:defn-within-group-variability}
\iffalse (Within Group Variability) \fi{} }The variability in the
response within a particular group.
\EndKnitrBlock{definition}

Figure \ref{fig:anovateststat-boxplots} then illustrates the larger the
variability between groups \emph{relative to} the variability within
groups, the stronger the signal. Quantifying the strength of a signal is
then about quantifying the ratio of these two sources of variability.
Let this sink in because it is completely counter-intuitive. We are
saying that in order to determine if there is a difference in the mean
response across groups, we have to examine variability. Further, a
signal in data is measured by the variability it produces. For this
reason, comparing a quantitative response across a categorical variable
is often referred to as Analysis of Variance (ANOVA).

\BeginKnitrBlock{rmdkeyidea}
Consider the ratio of the variability between groups to the variability
within groups. The larger this ratio, the stronger the evidence of a
signal provided by the data.
\EndKnitrBlock{rmdkeyidea}

\section{Forming a Standardized Test
Statistic}\label{forming-a-standardized-test-statistic}

As we stated above, quantifying the strength of a signal is equivalent
to quantifying the ratio of two sources of variability. Such ratios are
known as \textbf{standardized test statistics}.

\BeginKnitrBlock{definition}[Standardized Test Statistic]
\protect\hypertarget{def:defn-standardized-test-statistic}{}{\label{def:defn-standardized-test-statistic}
\iffalse (Standardized Test Statistic) \fi{} }A ratio of two sources of
variability, or a signal-to-noise ratio. The larger the test statistic,
the stronger the evidence of a signal; said another way, the larger the
test statistic, the stronger the evidence against the null hypothesis.
\EndKnitrBlock{definition}

Based on our observations above, the standardized test statistic for
comparing the mean response across multiple groups has the general form

\begin{equation}
  T = \frac{(\text{Between Group Variability})}{(\text{Within Group Variability})}
  \label{eq:general-test-stat}
\end{equation}

The question we then have before us is the following: how do we measure
these sources of variability? Consider again the hypothesis of interest
for the \protect\hyperlink{CaseOrganic}{Oranic Food Case Study}:

\begin{quote}
\(H_0: \mu_{\text{comfort}} = \mu_{\text{control}} = \mu_{\text{organic}}\)\\
\(H_1:\) At least one \(\mu\) differs from the others
\end{quote}

In order to form the standardized test statistic, let's again think
about what constitutes evidence \emph{against} the null hypothesis. The
more the means differ from one another, the stronger the evidence. But,
in the previous unit, we had a measure for how different values were
from one another --- variance. That is, the \emph{between-group}
variability can be measured by the variance of the means; we call this
the \textbf{Mean Square for Treatment (MSTrt)}.

\BeginKnitrBlock{definition}[Mean Square for Treatment (MSTrt)]
\protect\hypertarget{def:defn-mstrt}{}{\label{def:defn-mstrt} \iffalse (Mean
Square for Treatment (MSTrt)) \fi{} }This captures the between-group
variability in an Analysis of Variance; it is a weighted variance among
the sample means from the various groups. It represents the signal.
\EndKnitrBlock{definition}

Since we do not know the means for each groups (remember, each \(\mu\)
is a parameter), we assess the between group variability within the
sample using the estimates for these parameters --- the sample means.
This is our signal. The larger this variance, the further apart the
means are from one another (agreeing with the alternative hypothesis);
the smaller this variance, the closer the means are (agreeing with the
null hypothesis).

While the numerator provides some measure of the size of the signal, we
need again need to consider how much noise is within the data. Again, in
Figure \ref{fig:anovateststat-boxplots}, the variability between the
means is identical for the two datasets; the signal is stronger for
Dataset A because this variability is larger \emph{with respect to the
noise}. In order to capture the \emph{within-group} variability, we pool
the variances for each group; this is called the \textbf{Mean Square for
Error (MSE)}.

\BeginKnitrBlock{definition}[Mean Square for Error (MSE)]
\protect\hypertarget{def:defn-mse}{}{\label{def:defn-mse} \iffalse (Mean
Square for Error (MSE)) \fi{} }This captures the within-group
variability; it is a pooled estimate of the variance within the groups.
It represents the noise.
\EndKnitrBlock{definition}

Our test statistic in Equation \eqref{eq:general-test-stat} is then
refined to

\begin{equation}
  T = \frac{MSTrt}{MSE}
  \label{eq:anova-test-stat}
\end{equation}

\BeginKnitrBlock{rmdtip}
Consider testing the hypotheses \textgreater{}
\(H_0: \mu_1 = \mu_2 = \dotsb = \mu_k\)\\
\textgreater{} \(H_1:\) At least one \(\mu\) differs from the others

The standardized test statistic of interest is

\[
  T = \frac{MSTrt}{MSE}
\]

where

\[
\begin{aligned}
  MSTrt &= \frac{1}{k-1} \sum_{j=1}^{k} n_j \left(\bar{y}_j - \bar{y}\right)^2 \\
  MSE &= \frac{1}{n-k} \sum_{j=1}^{k} \left(n_j - 1\right) s_j^2
\end{aligned}
\]

and \(n_j\) represents the sample size for the \(j\)-th group,
\(\bar{y}_j\) represents the sample mean for the \(j\)-th group,
\(\bar{y}\) represents the overall mean response across all groups, and
\(s_j^2\) represents the sample variance for the \(j\)-th group.
\EndKnitrBlock{rmdtip}

We note that while mathematical formulas have been provided to add some
clarity to those who think algebraically, our emphasis is \emph{not} on
the computational formulas as much as the idea that we are comparing two
sources of variability.

\section{Obtaining a P-value}\label{obtaining-a-p-value}

Standardized test statistics quantify the strength of a signal, but they
do not allow for easy interpretation. However, with a standardized test
statistic, we are able to compute a p-value to quantify how unlikely our
particular sample is. That is, we need to construct the null
distribution for the standardized test statistic. We need to know what
type of signal we would expect if the null hypothesis were true.
Conceptually, this is no different than it was in Unit I. We consider
running the study again in a world in which all the groups are the same;
for the \protect\hyperlink{CaseOrganic}{Organic Food Case Study}, this
would involve - Obtaining a new sample of students. - Randomizing each
student to one of the three groups at random, all showing the same
foods. - Having each student answer a questionnaire regarding moral
dilemmas. - Summarize the data by computing a standardized test
statistic.

Notice the difference in step 2 above compared to what actually happened
in the real study. In the real study, each group had a different set of
foods. This was to answer the question about whether there is a
difference in the groups. However, in order to construct the \emph{null
distribution}, we need to force all groups to be the same. This could be
accomplished by showing every group the same set of foods. The primary
difference in this unit is that the strength of the signal is measured
through a standardized test statistic. After repeating the above steps
over and over again, we determine how often the recorded standardized
test statistics exceeded the value we obtained in our actual sample.

Figure \ref{fig:anovateststat-pvalue} represents the null distribution
of the standardized test statistic. Again, these are values of the
standardized test statistic we would expect if there were no
relationship between the food categories to which the students were
exposed and their moral score. We are then interested in finding out if
the observed dataset is consistent with these expectations.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/anovateststat-pvalue-1} 

}

\caption{Computation of the p-value for the Organic Food Case Study by simulating the null distribution.  The null distribution is based on 5000 replications.}\label{fig:anovateststat-pvalue}
\end{figure}

Notice that in our data, we observed a standardized test statistic of
0.41; based on the null distribution, we would expect a signal this
strong or stronger about 66.9\% of the time \emph{when no signal existed
at the population} (by chance alone). That is, our data is quite
consistent with what we would expect under the null hypothesis. There is
no evidence of a relationship between the type of food a student is
exposed to and their moral expectations, on average.

Again, conceptually, this is similar to what we saw in the previous
unit. We are simply determining how likely our data is under the null
hypothesis. However, unlike the previous unit, it may not be clear how
we actually model this null distribution. If we cannot physically redo
the study, how can we construct this model? In order to understand this,
we must consider a different model --- that for the data generating
process. This is the topic of the next chapter.

\chapter{Building the Statistical Model}\label{ANOVAmodel}

The numerical summaries of any study are subject to sampling
variability. That is, if we were to repeat the study with new subjects,
the statistics we compute would almost certainly change to some degree.
The key to feeling confident in our results is to quantify the
variability in our estimates; this was the argument made in Chapters
\ref{SamplingDistns} and \ref{NullDistns}. The goal of any statistical
analysis is then to develop a model for the sampling (or null)
distribution of a statistic. Often times, this requires modeling the
data-generating process as a precursor. As in any other discipline,
statistical models simplify the process being modeled by making certain
assumptions. In this chapter, we develop a model that will help us make
inference about the mean of several populations.

\section{General Formulation}\label{general-formulation}

Consider dropping a tennis ball from the top of a 50-meter building and
recording the time required before the ball hits the ground. Applying
the principles learned in a first course in physics, we would be able to
compute the time precisely using the formula
\[\text{time} = \sqrt{\frac{2(\text{distance})}{9.8}}\]

where \(9.8 m/s^2\) is the acceleration due to gravity; further, this
formula works regardless of the mass of the object. Plugging 50 meters
into the equation yields a time of 10.2 seconds. If we were to drop a
second tennis ball from the same building, the formula tells us that it
will also take 10.2 seconds to hit the ground below. This is known as a
\textbf{deterministic} system since entering a constant input always
results in the same output.

\BeginKnitrBlock{definition}[Deterministic Process]
\protect\hypertarget{def:defn-deterministic-process}{}{\label{def:defn-deterministic-process}
\iffalse (Deterministic Process) \fi{} }One which is completely
determined by the inputs. That is, entering the same input twice will
always result in the same output with certainty.
\EndKnitrBlock{definition}

This is a model; it simplifies extremely complex processes involving the
gravitational pull between objects and works reasonably well. However,
it does not always match reality. If we were to repeatedly drop tennis
balls from the same 50-meter building and record the time before hitting
the ground, we might find that the time differs slightly from one ball
to the next. There are several reasons why our observed responses do not
line up directly with those predicted by the above equation; for
example, our device for measuring time may be subject to some
measurement error, a strong gust of wind could alter the results (while
the above equation assumes no air resistance), or the person dropping
the ball may have inadvertantly increased the initial velocity of the
ball. These reasons, and others, contribute to the observations not
lining up with the model. That is, there is associated noise in the
resulting measurements. A model which incorporates this noise might be
written as
\[\text{time} = \sqrt{\frac{2(\text{distance})}{9.8}} + \text{noise}\]

where the noise is not a known quantity. As a result, this is a
\textbf{stochastic} model as the same value for distance may result in
different outputs each time.

\BeginKnitrBlock{definition}[Stochastic Process]
\protect\hypertarget{def:defn-stochastic-process}{}{\label{def:defn-stochastic-process}
\iffalse (Stochastic Process) \fi{} }One which has an element of
randomness. That is, the resulting output of the system cannot be
predicted with certainty.
\EndKnitrBlock{definition}

This leads us to our general formulation for a statistical model:

\begin{equation}
  \text{Response} = f(\text{variables, parameters}) + \text{noise}
  \label{eq:general-model}
\end{equation}

The response we observe is the result of two components:

\begin{itemize}
\tightlist
\item
  A deterministic component which takes the form of a function of
  variables and unknown parameters. It is often this component on which
  we would like to make inference.
\item
  A stochastic component which captures the unexplained variability in
  the data generating process.
\end{itemize}

Since the noise is a random element, it has a distribution. We often
make additional assumptions on the structure of this distribution to
enable inference on the deterministic component of the model. We discuss
this later in the chapter.

This general model adheres to the idea of partitioning the variability
in the response. It says that a part of the reason the responses differ
between subjects is because they have different variables (remember,
parameters are fixed for all subjects in a population); part of the
reason is unexplained noise. The overall goal of a statistical model is
to give an explanation for why the data is what it is. How did it come
to be? What process generated the values I have observed? Our
statistical model says that these values have some deterministic
component plus some additional noise we cannot explain. We now turn
towards employing this model in the case of comparing the mean response
for multiple groups.

\section{Statistical Model for A Quantitative Response and a Categorical
Predictor}\label{statistical-model-for-a-quantitative-response-and-a-categorical-predictor}

For the \protect\hyperlink{CaseOrganic}{Organic Food Case Study}, we are
comparing the moral expectations (quantitative response) for different
food exposures (levels of a categorical variable). Our model for the
data-generating process is best understood in light of the graphic we
used to display the data (see Figure \ref{fig:anovamodel-organic-plot}).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/anovamodel-organic-plot-1} 

}

\caption{Moral expectation scores for students following exposure to various food types.}\label{fig:anovamodel-organic-plot}
\end{figure}

Let's consider how the value 3.67, highlighted red in Figure
\ref{fig:anovamodel-organic-plot}, was generated. As discussed
previously, there are two sources of variability in the moral
expectation scores (two reasons that the values are not all the same).
One source is the fact that different subjects had different exposures.
That is, one reason the value 3.67 differs from others observed is
because this subject belongs to the organic group and not the comfort or
control exposure groups. As this is something we can explain, it goes
into the deterministic portion of the model; it is a function of known
variables (group exposure). Let the function \(f(\cdot)\) be such that
the input is the group exposure for the \(i\)-th subject and the output
is the mean moral expectation score for that group; this can be
represented as a piecewise function: \[
f\left((\text{Food Exposure Group})_i\right) = \begin{cases}
  \mu_1 & \text{if i-th subject exposed to organic foods} \\
  \mu_2 & \text{if i-th subject exposed to control foods} \\
  \mu_3 & \text{if i-th subject exposed to comfort foods} \end{cases}
\]

Notice that \(f(\cdot)\) involves both a variable of interest as well as
parameters of interest --- the mean response \(\mu_1, \mu_2, \mu_3\) for
each of the three groups. This function is perfectly acceptable, but it
is cumbersome to write in a shortened form. Notice how the function
works: it receives an input regarding which group, and it directs you to
the appropriate parameter as an output. We can write this in a compact
way as \[
f\left((\text{Food Exposure Group})_i\right) = \sum_{j=1}^{3} \mu_j \mathbb{I}\left(\text{i-th subject in food exposure group j}\right)
\]

where \(\mathbb{I}(\cdot)\) is the indicator function taking value 1 if
the event occurs and 0 otherwise.

\BeginKnitrBlock{rmdkeyidea}
The deterministic component of a statistical model incorporates the
parameters which govern the question of interest. It is built to explain
differences in the response based on differences in group membership or
other characteristics of the subjects.
\EndKnitrBlock{rmdkeyidea}

This is the deterministic part of the model, as inputing the same group
always results in the same output --- the unknown parameter
characterizing the mean response for the group. This, however, only
captures one reason we feel the responses differ across subject. This
deterministic component says that every single person exposed to the
same food group should have the same moral expectations. It does not
explain why subjects within the organic group do not all share the
average moral expectation score. This source of variability is something
we cannot fully explain but attribute to natural variability in this
group or measurement error in how we obtained the response. In order to
capture this, we add noise to the system, and we allow this noise to be
a random variable which is unique to each subject within the population.
Letting \(\epsilon_i\) represent the noise accompanying the response of
the \(i\)-th subject, we can now extend the model in Equation
\eqref{eq:general-model} to accommodate these two sourses of variability
and obtain \[
\text{(Moral Expectation Score)}_i = \sum_{j=1}^3 \mu_j \mathbb{I}(\text{i-th subject in food exposure group j}) + \epsilon_i
\]

This may be written in shorthand (suppressing the parameters and noise)
as \[ \text{Moral Expectation Score} \sim \text{Food Exposure Group}\]

\BeginKnitrBlock{rmdkeyidea}
The stochastic component of a statistical model captures the unexplained
variability due to natural variability in the population or measurement
error in the response.
\EndKnitrBlock{rmdkeyidea}

\BeginKnitrBlock{rmdtip}
In general, given a quantitative response variable \(y\), our model for
the data generating process comparing this variable across several
levels of a factor is
\[y_i = \sum_{j=1}^k \mu_j \mathbb{I}(\text{i-th subject in factor level j}) + \epsilon_i\]
\EndKnitrBlock{rmdtip}

In general, students struggle with the fact that we have two different
models floating around. Currently, we are modeling the data-generating
process. This model is used to develop a secondary model of the sampling
distribution (or null distribution) of a statistic of interest. It is
this secondary model that is actually necessary in order to conduct
inference; the model for the data-generating process is simply a
stepping stone to the model of interest.

\section{Conditions on the Error
Distribution}\label{conditions-on-the-error-distribution}

In our model for the data-generating process we incorporated a component
\(\epsilon\) to capture the noise within each group. Since the error is
a random variable (stochastic element), we know it has a distribution.
We typically assume a certain structure to this distribution. The more
assumptions we are willing to make, the easier the analysis, but the
less likely our model is to be applicable to the actual data-generating
process we have observed. The conditions we make dictate how we conduct
inference (the computation of a p-value or confidence interval).

The first condition we consider is that the noise attributed to one
observed individual is \textbf{independent} of the noise attributed to
any other individual observed. That is, the amount of error in any one
individual's response is unrelated to the error in any other response
observed. It is easiest to understand this condition by examining a case
when the condition would not hold.

\BeginKnitrBlock{definition}[Independence]
\protect\hypertarget{def:defn-independence}{}{\label{def:defn-independence}
\iffalse (Independence) \fi{} }Two variables are said to be independent
when the likelihood that one variable takes on a particular value does
not depend on the value of the other variable.
\EndKnitrBlock{definition}

\BeginKnitrBlock{example}[Programming Speed]
\protect\hypertarget{ex:ex-programming}{}{\label{ex:ex-programming}
\iffalse (Programming Speed) \fi{} }Suppose we are conducting a study to
compare the speed required to complete a particular programming task in
two different languages: Python and R. We obtain a sample of 100
programmers previously exposed to Java but neither Python nor R. We ask
each programmer to complete a programming exercise in Python and record
the time required to successfully complete the task. Then, we ask each
programmer to perform the same task in R and record the time required to
successfully complete the task.

The model for the data generating process would be \[
(\text{Time})_i = \mu_1 \mathbb{I}(\text{i-th task programmed in Python}) + \mu_2 \mathbb{I}(\text{i-th task programmed in R}) + \epsilon_i
\]

Given the method in which the data was collected, it would not be
reasonable to assume the errors are independent of one another. Some
programmers are naturally faster than others. A programmer with a below
average (negative \(\epsilon\)) time in Python will most likely have a
below average (negative \(\epsilon\)) time in R on the same task.
Therefore, there is a relationship between the errors for some of the
observations taken. This violates the independence condition.
\EndKnitrBlock{example}

The second condition that is typically placed on the distribution of the
errors is that the variability of the responses is similar within each
group. This assumption is known as \textbf{homoskedasticity}.

\BeginKnitrBlock{definition}[Homoskedasticity]
\protect\hypertarget{def:defn-homoskedasticity}{}{\label{def:defn-homoskedasticity}
\iffalse (Homoskedasticity) \fi{} }Also known as ``constant variance,''
this assumption states that the variability of error terms for
individuals within a group is the same across all groups.
\EndKnitrBlock{definition}

Practically, this means that the responses in one group are not
dramatically more variable than any other group (the width of the box
portion of a boxplot should be roughly the same across groups). This
condition ensures that the precision of the measurements is roughly
similar. In fact, we made use of this assumption in the construction of
our standardized test statistic \[T = \frac{MSTrt}{MSE}\]

since MSE was a pooled estimate of the variability. If we were not
willing to assume that the variabilities were similar, we would not
construct a pooled estimate. This also highlights that the MSE is an
estimate of the variability of observations within any group when this
condition is satisfied.

\section{Simulating the Null
Distribution}\label{simulating-the-null-distribution}

We note that this section is a bit more technical than other sections.
We want to give the reader a feel for the computational aspect of
simulating the null distribution. However, understanding conceptually
that we are repeating the study in a world in which the null hypothesis
is true is sufficient for interpreting a p-value.

Under the above conditions, we can model the null distribution of our
standardized test statistic. The key here is to lean on our data
generating process. Consider the \protect\hyperlink{CaseOrganic}{Organic
Food Case Study}. \emph{If the null hypothesis is true}, then we have
that
\[\mu_{\text{organic}} = \mu_{\text{comfort}} = \mu_{\text{control}}\]

Let's define this common mean to be \(\mu\); we do not know what this
value is, but it is common to all groups. Therefore, \emph{if the null
hypothesis is true}, we have that the data generating process reduces to

\begin{equation}
  \text{(Moral Expectation Score)}_i = \mu + \epsilon_i
  \label{eq:null-model}
\end{equation}

Therefore, we can generate data according to this model. We can replace
\(\mu\) by our best estimate --- the sample mean response across all
observations regardless of their group. It simply remains to determine
how to approximate a random variable from the noise distribution. In
order to do this, we need estimates of the errors, known as
\textbf{residuals}.

\BeginKnitrBlock{definition}[Residual]
\protect\hypertarget{def:defn-residual}{}{\label{def:defn-residual}
\iffalse (Residual) \fi{} }The difference between the observed response
and the predicted response (estimated deterministic portion of the
model). Residuals approximate the noise in the data-generating process.
\EndKnitrBlock{definition}

The deterministic component of the model gives a way of predicting the
response. For example, consider the
\protect\hyperlink{CaseOrganic}{Organic Food Case Study}; the data is
reproduced in Figure \ref{fig:anovamodel-organic-boxplot}. Based on the
data available, if a subject were to be exposed to organic foods, we
would expect their moral expectation score to be 5.66; this is the
average observed among individuals randomized to this treatment within
our study.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/anovamodel-organic-boxplot-1} 

}

\caption{Comparison of the moral expectations for college students exposed to different types of food.}\label{fig:anovamodel-organic-boxplot}
\end{figure}

That is, we can define the \textbf{predicted value} for \(i\)-th
observation in our study as \[
\widehat{y}_i = \sum_{j=1}^{3} \bar{y}_j \mathbb{I}(\text{i-th subject in food exposure group j})
\]

and the corresponding residual as \[
e_i = y_i - \widehat{y}_i
\]

Let's not get lost in the mathematical notation; the residual here is
simply the difference between the response of the subject and the
average response for their corresponding group.

The key idea here is that residuals approximate the unseen error.
Therefore, if we take this error and perturb it (the details of which
are beyond the scope of this course), we can generate new data. A new
dataset, generate under the null hypothesis, can then be constructed as
\[
y_i^* = \bar{y} + e_i^*
\]

where \(y_i^*\) is then a new observation constructed by taking a mean
and adding a perturbed version of the residual for that observation.
Notice that each newly generated response has the same mean (so that the
null is true). We then take this new dataset and compute the
standardized test statistic as before and record it. Then, we repeat
this process over and over again until we have constructed the null
distribution. This gives us a sense of the p-value.

\section{Recap}\label{recap}

We have covered a lot of ground in this chapter, and it is worth taking
a moment to summarize the big ideas. In order to construct a model for
the null distribution of the standardized test statistic, we took a step
back and modeled the data generating process. Such a model consists of
two components: a deterministic component explaining the differences
between groups and a stochastic component capturing the noise in the
system.

Certain conditions are placed on the distribution of the noise in our
model. Using these assumptions, we can generate data which adheres to
the null hypothesis. Therefore, we can obtain an empirical model that
suggests what values of a test statistic we might expect.

\chapter{Classical ANOVA Model}\label{ANOVAclassical}

In the previous chapter, we developed a model for a quantitative
response as a function of a categorical predictor. Specifically, suppose
we are comparing a quantitative response across \(k\) levels of a factor
of interest. Our model has the form
\[(\text{Response})_i = \sum_{j=1}^{k} \mu_j \mathbb{I}\left(\text{i-th subject belongs to group j}\right) + \epsilon_i\]

where \(\epsilon\) is a random variable capturing the noise in the
data-generating process. In order to perform inference, we made two
assumptions on this error term:

\begin{itemize}
\tightlist
\item
  The error in one observation is independent of the error in any other
  observation.
\item
  The variability in the error is constant across groups.
\end{itemize}

This allowed us to empirically model the null distribution of our
standardized test statistic \[T = \frac{MSTrt}{MSE}\] through
simulation. This approach is completely valid; however, it has been
shown that such empirical models can be unstable in small sample sizes.
When we do not have the resources to obtain a large sample, we can
improve our model of the sampling distribution (or null distribution) of
a statistic through additional modeling assumptions. In this unit, we
discuss an additional modeling assumption that is common in the
engineering and scientific disciplines.

\section{Modeling the Population}\label{modeling-the-population}

Before we delve into the details, let's set the stage for the bigger
story being told. Recall that our goal is to say something about the
population using a sample. We have developed a process to address this
goal:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Frame our question through a parameter of interest.
\item
  Collect data that allows us to estimate the parameter using the
  analogous statistic within the sample.
\item
  Summarize the variability in the data graphically.
\item
  Quantify the variability in the statistic through modeling the
  sampling distribution (or null distribution).
\item
  Using the sampling distribution (or null distribution), quantify the
  evidence in the sample.
\end{enumerate}

This process is wrapped up in our \emph{Five Fundamental Ideas of
Inference} and the \emph{Distributional Quartet}. The key step in this
process is quantifying the variability by modeling the \emph{sampling
distribution} (or \emph{null distribution}). We have described the
construction of these models empirically, through repeating the study by
appropriately resampling the data available.

Our goal is still to model the sampling distribution (or null
distribution); that is the key inferential step. Instead of building an
empirical model, we can construct an analytical (exact) model through an
additional step: modeling the population directly.

\BeginKnitrBlock{rmdkeyidea}
A model for the sampling distribution of a statistic can often be
obtained by placing a model on the distribution of the population.
\EndKnitrBlock{rmdkeyidea}

So, we have two models; the model for the distribution of the population
is simply a stepping stone to what we really need, a model for the
sampling distribution of the statistic. It is important to separate
these steps. We are not interested in directly modeling the population;
we do it in order to construct a model for the sampling distribution.

There is one other distinction to make: a model for the population is
\emph{always} an assumption. We hope that the data is consistent with
this assumption in order to apply the resulting model for the sampling
distribution.

\section{Adding the Assumption of
Normality}\label{adding-the-assumption-of-normality}

The sub-field of mathematics known as probability is the discipline of
modeling randomness. In particular, we make use of probability to model
a distribution. In order to get a feel for probability models, consider
the following example.

\BeginKnitrBlock{example}[Iris Characteristics]
\protect\hypertarget{ex:ex-iris}{}{\label{ex:ex-iris} \iffalse (Iris
Characteristics) \fi{} }The discipline of statistics began in the early
1900's primarily within the context of agricultural research. Edgar
Anderson was a researcher investigating the characteristics of the iris.
He had collected measurements on over one hundred iris flowers,
including their petal length and width and their sepal length and width.
The sepal is the area (typically green) beneath the petal of a flower.
It offers protection while the flower is budding and then support for
the petals after the flower blooms.
\EndKnitrBlock{example}

Figure \ref{fig:anovaclassical-iris-histogram} is a histogram of the
sepal width for the iris plants observed by Edgar Anderson; overlayed is
the density estimate for the same dataset which we have described as a
smoothed histogram. This smoothed histogram is estimated from the data;
it is an empirical model of the distribution.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/anovaclassical-iris-histogram-1} 

}

\caption{Summary of the distribution of sepal widths for a sample of irises.}\label{fig:anovaclassical-iris-histogram}
\end{figure}

Probability models are analytical models for the distribution of a
variable. Instead of constructing a density using data, we probability
theory posits a functional form for the density (subject to certain
constraints that are beyond the scope of this course). For example,
Figure \ref{fig:anovaclassical-iris-normal} overlays the following
function on top of the the iris data:

\[f(x) = \frac{1}{\sqrt{0.380\pi}} e^{-\frac{1}{0.380}(x - 3.057)^2}\]

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/anovaclassical-iris-normal-1} 

}

\caption{Summary of distribution  the sepal widths for a sample of irises with a probability model overlayed.}\label{fig:anovaclassical-iris-normal}
\end{figure}

While this model for the density is not perfect, we note that it does
capture many of the characteristics present in the data. This particular
model, characterized by the bell-shape density, is known as the
\textbf{Normal Distribution}.

\BeginKnitrBlock{definition}[Normal Distribution]
\protect\hypertarget{def:defn-normal-distribution}{}{\label{def:defn-normal-distribution}
\iffalse (Normal Distribution) \fi{} }Also called the Gaussian
Distribution, this probability model is popular for modeling noise
within a data-generating process. It has the following characteristics:

\begin{itemize}
\tightlist
\item
  It is bell-shaped.
\item
  It is symmetric, meaning the mean is directly at its center.
\item
  Often useful for modeling natural phenomena or sums of measurements.
\end{itemize}

The functional form of the Normal distribution is
\[f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2}(x - \mu)^2}\]

where \(\mu\) is the mean of the distribution and \(\sigma^2\) is the
variance of the distribution.
\EndKnitrBlock{definition}

While there are several nice properties of the Normal Distribution, we
are primarily interested in the fact that if we assume the error in a
data generating process follows a Normal Distribution, then we have an
analytical model for the sampling distribution of our standardized test
statistic. What does this get us? We avoid simulating in order to build
a model for the sampling distribution; so, computationally it is faster.
If the errors really are from a Normal Distribution, then we also gain
power in our study. Finally, such a model does not rely on sufficient
data to construct; it is valid for any sample size (of course, large
samples will always decrease variability which is a plus).

So, in addition to the two assumptions we have been willing to make so
far, if we also include the assumption that the errors follow a Normal
Distribution, we have the ``Classical ANOVA Model.''

\BeginKnitrBlock{rmdtip}
``Classical ANOVA Model'': For a quantitative response and categorical
variable, the classical ANOVA model assumes the following
data-generating process:

\[(\text{Response})_i = \sum_{j=1}^{k} \mu_j \mathbb{I}(\text{i-th observation belongs to j-th group}) + \epsilon_i\]

where

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The errors are independent of one another.
\item
  The errors from one group have the same variability as all other
  groups.
\item
  The errors follow a Normal Distribution.
\end{enumerate}

It is possible to relax these assumptions; however, this is the default
``ANOVA'' analysis implemented in the majority of statistical packages.
\EndKnitrBlock{rmdtip}

\section{Impact of Normality
Assumption}\label{impact-of-normality-assumption}

If all three of the classical ANOVA conditions on the error hold, then
we have an analytical model for the distribution of our standardized
test statistic \[T = \frac{MSTrt}{MSE}\]

under the null hypothesis. That is, we can model the null distribution.
Figure \ref{fig:anovaclassical-organic-model} compares the empirical
model for the null distribution of the standardized test statistic using
the data from the \protect\hyperlink{CaseOrganic}{Organic Food Case
Study} with that implied by assuming the errors follow a Normal
Distribution. The two models line up quite nicely; we present this as a
proof of concept that these assumptions can often be reasonable.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/anovaclassical-organic-model-1} 

}

\caption{Comparison of the empirical model for the null distribution of our standardized test statistic with the probability model under the classical ANOVA model.}\label{fig:anovaclassical-organic-model}
\end{figure}

Thus, if we are willing to make the assumption that all three conditions
on the error hold, we could rely on the analytical probability model to
compute a p-value instead of the empirical model we get from simulation.
This probability model is called the F-distribution, and as a result,
the standardized test statistic \(T = MSTrt/MSE\) is often referred to
as the F-statistic.

Many software packages contain a function for conducting an analysis
assuming the classicial ANOVA model is valid. When this is the case, the
output has the form in Figure \ref{fig:anovaclassical-table}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./images/ANOVAclassical-Table} 

}

\caption{Layout of an ANOVA table which summarizes the analysis conducted.  Emphasis is on partitioning the variability.}\label{fig:anovaclassical-table}
\end{figure}

The table is a way of summarizing the output from the analysis; the
table itself is not very interesting, but we present it because it has
the same emphasis we have in this unit --- partitioning variability. The
key to separating a signal from a noise is to partition the variability
in the data. The total variability is partitioned into that resulting
from the groups (the factor), this is the deterministic portion of the
model that we can explain, and the error, the stochastic portion of the
model that we cannot explain. By partitioning this variability, we are
able to compute the standardized test statistic and the corresponding
p-value. Primarily, the only component we examine in such a table is the
p-value. However, it is worth noting that the mean square for error
(MSE) also provides an estimate of the variance of the errors within a
group, the residual variance. That is, the MSE provides an estimate of
the variance in the response within a group.

\BeginKnitrBlock{rmdtip}
The mean square for error (MSE) is an estimate of the variability in the
response within a particular group.
\EndKnitrBlock{rmdtip}

\section{Analysis of Organic Food Case
Study}\label{analysis-of-organic-food-case-study}

Let's consider the \protect\hyperlink{CaseOrganic}{Organic Food Case
Study} data. We will continue working with the model we have developed
for the moral expectation score:
\[\text{Moral Expectation Score} \sim \text{Food Exposure Group}\]

Further, let's suppose that the data is consistent with all three
classical ANOVA conditions. The results from the corresponding analysis
comparing the average moral expectation score across the three food
conditions are given in Table
\ref{tab:anovaclassical-organic-anova-table}.

\begin{table}

\caption{\label{tab:anovaclassical-organic-anova-table}ANOVA table for the Organic Food Case Study.}
\centering
\begin{tabular}[t]{l|r|r|r|r|r}
\hline
Source & DF & SS & MS & F & P-value\\
\hline
Food Exposure Group & 2 & 0.562 & 0.281 & 0.406 & 0.667\\
\hline
Residuals & 120 & 82.951 & 0.691 &  & \\
\hline
Total & 122 & 83.513 &  &  & \\
\hline
\end{tabular}
\end{table}

As long as the conditions on the error term are reasonable, then we can
interpret the above p-value. Based on these results, there is no
evidence that the moral expectations differ, on average, across the
various food exposure groups. That is, there is no evidence of a
relationship between the type of food to which we are exposed and our
resulting moral expectations, on average.

\section{Recap}\label{recap-1}

By placing an additional condition on the distribution of the error
term, we are able to construct an analytical model for the null
distribution of the standardized test statistic, instead of requiring an
empirical model. Of course, this model for the distribution of the error
terms is an assumption. In fact, we have made several assumptions in
order to compute the p-value given above. It is unwise to place
assumptions on a process without performing some type of assessment to
ensure those assumptions are reasonable --- that the data is consistent
with the assumptions. That is the focus of the next chapter.

\chapter{Assessing Modeling Assumptions}\label{ANOVAassessment}

In the previous chapter, we introduced a model for how a quantitative
response being generated across multiple groups. For the
\protect\hyperlink{CaseOrganic}{Organic Food Case Study}, this is
essentially \[
\text{(Moral Expectation Score)}_i = \sum_{j=1}^3 \mu_j \mathbb{I}(\text{i-th subject in food exposure group j}) + \epsilon_i
\]

Further, we added two conditions to the distribution of the error term:
1. The error in the moral expectation score for one individual is
independent of the error in the moral expectation score for all other
individuals. 2. The variability in the error for the moral expectation
score within a group is similar for any food exposure group.

Unfortunately, we cannot just state that these are the conditions we
hope hold for the data generating process and move on our merry way.
Since the p-value was computed assuming these conditions hold, the
p-value is only meaningful if the data is consistent with these
conditions. If any of these conditions is violated, then the p-value is
meaningless.

\BeginKnitrBlock{rmdkeyidea}
Residuals, since they are estimates of the noise in the data-generating
process, provide a way of assessing the modeling conditions placed on
the distribution of the error term.
\EndKnitrBlock{rmdkeyidea}

In this section, we discuss how to use residuals to assess these
conditions qualitatively.

\section{Assessing Independence}\label{assessing-independence}

Generally, independence is assessed through the context of the data
collection scheme. By carefully considering the manner in which the data
was collected, we can typically determine whether it is reasonable that
the errors in the response are independent of one another. Some key
things to consider when examining the data collection process: - Are
there repeated observations made on the same subject? This often
suggests some type of relationship between the responses and therefore
would not be consistent with errors being independent. - Is the response
measured over time (time-series) such as daily temperature over the
course of a month? Time-series data often exhibits strong
period-to-period relationships suggesting the errors are not
independent. For example, if it is hot today, it will probably be hot
tomorrow as well. - Is there a learning curve in how the data was
collected? Learning curves again suggest some dependence from one
observation to the next.

Random sampling and random assignment allow us to confidently state that
the errors are independent of one another. One additional pitfall to
watch out for when collecting your own data is whether there is some
type of systematic error in the measurement device. - Measurement
devices which are failing over time will introduce a dependence from one
observation to the next. Imagine a bathroom scale that begins to add an
additional pound each day. Then, being above average weight one day will
most likely lead to an above average weight the next, due primarily to
the measurement device.

This last point illustrates a particular deviation from our condition of
independence in which two observations collected close together in time
are related. When we know the order in which the data was collected, we
can assess whether the data is consistent with independence or tends to
deviate in this manner. This is done graphically through a
\textbf{time-series plot} of the \emph{residuals}. If two errors were
unrelated, then the value of one residual should tell us nothing about
the value of the next residual. Therefore, a plot of the residuals over
time should look like noise (since residuals are supposed to be
estimates of noise). If there are any trends, then it suggests the data
is not consistent with independence.

\BeginKnitrBlock{definition}[Time Series Plot]
\protect\hypertarget{def:defn-time-series-plot}{}{\label{def:defn-time-series-plot}
\iffalse (Time Series Plot) \fi{} }Plot of a variable over time. This
plot allows us to assess some deviations from independence. A trend in
the location or spread of the points over time suggests a deviation from
independence.
\EndKnitrBlock{definition}

As an example, consider the time-series plots shown in Figure
\ref{fig:anovaassessment-independence-violations}, both representing
hypothetical datasets. In Panel A, the residuals display a trend in the
location over time. Knowing that a response was below average suggests
the next response will also be below average. In Panel B, the results
deplay a trend in the spread over time. This suggests that measurements
taken later in the study were less precise. Both panels are then
examples of patterns which would suggest the data is not consistent with
the condition of independence.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/anovaassessment-independence-violations-1} 

}

\caption{Examples of trends in a time-series plot of the residuals.  Such trends indicate the data is not consistent with the condition that the errors are independent of one another.}\label{fig:anovaassessment-independence-violations}
\end{figure}

Instead, if the data were consistent with the condition of independence
on the error terms, we would expect to see a plot as in Figure
\ref{fig:anovaassessment-independence-reasonable}. Notice there are no
trends in the location or spread of the residuals.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/anovaassessment-independence-reasonable-1} 

}

\caption{Example of a time-series plot of residuals which shows no trends in location or spread.  This is consistent with what we would expect if the condition of independence among errors were satisfied.}\label{fig:anovaassessment-independence-reasonable}
\end{figure}

For the \protect\hyperlink{CaseOrganic}{Organic Food Case Study},
participants were assessed simultaneously within a large lecture.
Therefore, there is no ordering in time to be concerned about. Further,
since students worked individually on the questionnaire, it is
reasonable to assume that the errors in the moral expectation score are
unrelated to one another.

\section{Assessing Homoskedasticity}\label{assessing-homoskedasticity}

We want the variability in the errors within a group to be the same
across the groups. This corresponds to the spread of the response within
each group is the same. This implication leads to a simple way of
assessing this assumption. Examining the side-by-side boxplots (or
jitter plots, etc.) of the response allows us to get a sense of the
variability within each group. Figure
\ref{fig:anovaassessment-variance-organic} shows the moral expectation
score for each individual across the various groups. Notice that the
boxes for each group are roughly the same size; that is, the
interquartile ranges are similar. This suggests that the variability
within each group is similar from one group to the next. That is, the
data is consistent with this condition.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/anovaassessment-variance-organic-1} 

}

\caption{Comparison of the moral expectations for college students exposed to different types of food.}\label{fig:anovaassessment-variance-organic}
\end{figure}

\section{Assessing Normality}\label{assessing-normality}

Assessing whether observations adhere to a particular distribution is a
large area in statistical research. Many methods have been developed for
this purpose. We emphasize a single graphical summary known as a
\textbf{probability plot}. The construction of the plot is beyond the
scope of this text, but the concepts underlying its construction
actually tie in nicely to the big themes of the course. Recall that if a
sample is representative, then it should be a snapshot of the underlying
population. Therefore, if we believe the underlying population has some
particular distribution, we would expect the properties of this
distribution to be apparent in the sample as well.

If we believe the errors follow a Normal distribution, then it is
reasonable that the residuals should maintain some of those properties.
For example, the 10-th percentile of the residuals should roughly equate
to the 10-th percentile expected from a Normal distribution. Mapping up
the percentiles that we observe to those that we expect is the essence
of a probability plot.

\BeginKnitrBlock{definition}[Probability Plot]
\protect\hypertarget{def:defn-probability-plot}{}{\label{def:defn-probability-plot}
\iffalse (Probability Plot) \fi{} }Graphic for comparing a theoretical
probability model for the distribution an underlying population with the
distribution of the sample. Sample points should follow a straight line.
If points deviate from this linear trend, that suggests the points do
not align with the proposed model.
\EndKnitrBlock{definition}

While a probability plot can be used for a host of probability
distributions, the most common is the normal probability plot. Since we
expect the percentiles to line up directly, we would expect a one-to-one
linear relationship to be exhibited in the plot. Trends away from a
linear relationship suggest the proposed Normal distribution is not a
reasonable model for the distribution of the errors.

Figure \ref{fig:anovaassessment-normal-organic} shows the probability
plot for the residuals from the \protect\hyperlink{CaseOrganic}{Organic
Food Case Study}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/anovaassessment-normal-organic-1} 

}

\caption{Probability plot of the residuals for the Organic Food Case Study.  If the errors follow a Normal distribution, we would expect the residuals to fall along a straight line.}\label{fig:anovaassessment-normal-organic}
\end{figure}

Overall, the points do tend to follow a straight line. There are some
deviations from a linear relationship at each end of the plot, but the
deviations are not extreme. We argue that these residuals are consistent
with the errors having a Normal distribution.

For comparison, Figure \ref{fig:anovaassessment-normal-bad} illustrates
a hypothetical dataset for which the residuals suggest the condition of
the errors following a Normal distribution is violated.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/anovaassessment-normal-bad-1} 

}

\caption{Probability plot of residuals for a hypothetical dataset.  The trend away from a straight line suggests assuming the errors follow a Normal distribution would be unreasonable.}\label{fig:anovaassessment-normal-bad}
\end{figure}

\section{General Tips for Assessing
Assumptions}\label{general-tips-for-assessing-assumptions}

Each of the methods presented here are qualitative assessements, which
means they are subjective. That is okay. As the analyst, it is up to you
to determine which assumptions you are willing to make. You need to
determine whether you feel the data is consistent with the assumptions.
Here are two overall things to keep in mind.

First, do not spend too much time examining residual plots. If you stare
at a plot too long, you can convince yourself there is pattern in
anything. We are looking for glaring evidence that the data is not
consistent with the conditions we have imposed on our model. This is
especially true when we have only a few observations. In these settings,
reading plots can be very difficult. Again, it is about what you are
comfortable assuming; how much faith do you want to place in the
results?

Second, we have chosen the language carefully throughout this chapter.
We have never once stated that a condition was satisfied. When we
perform an analysis, we are making an assumption that the conditions are
satisfied. We can never prove that they are; we can only show that the
data is consistent with a particular condition. We can, however, provide
evidence that a condition is violated. When that is the case, we should
be wary of trusting the resulting p-values and confidence intervals.
This is not unlike hypothesis testing; just as we can never prove the
null hypothesis is true, we cannot prove that a condition is satisfied.

Finally, any conditions required for a particular analysis should be
assessed. If your sample is not consistent with the necessary
conditions, you should choose a different analysis. The inference you
obtain from an analysis is only reliable of the data is consistent with
any necessary conditions.

\BeginKnitrBlock{rmdtip}
The conditions for a model are placed on the error, but the residuals
are used to assess whether a dataset is consistent with these
conditions, allowing us to determine if assuming the conditions are
satisfied is reasonable.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We can never prove a condition is satisfied.
\item
  The assumptions are not on the residuals, but the errors.
\item
  A sample should be consistent with any conditions you impose on your
  model.
\end{enumerate}

If a sample is not consistent with the conditions you impose, you should
consider revising your analysis.
\EndKnitrBlock{rmdtip}

\chapter{Using the Tools Together}\label{ANOVArecap}

This unit introduced a framework for determining if there is an
association between a quantitative response and a categorical predictor.
We formed a standardized test statistic for measuring the signal, and
then we developed a model for the data-generating process which allowed
us to model the null distribution of the standardized statistic. In this
chapter, we pull these tools together once more to answer a research
question.

The primary question we have been addressing in this unit was whether
the moral expectations of students were affected by the type of food to
which they were exposed. We saw that there was little evidence of a
relationship between these two variables. We now use the data from the
\protect\hyperlink{CaseOrganic}{Organic Food Case Study} to answer a
related question:

\begin{quote}
Do the moral expectations of males and females differ?
\end{quote}

\section{Framing the Question (Fundamental Idea
I)}\label{framing-the-question-fundamental-idea-i-1}

As stated, the above question is ill-posed. We have not identified a
variable or parameter of interest. We refine this question to be

\begin{quote}
Does the average moral expectation score of males differ from that of
females?
\end{quote}

This question could also be stated as the following set of hypotheses:

\begin{quote}
Let \(\mu_1\) and \(\mu_2\) represent the average moral expectation
score for males and females, respectively.\\
\(H_0: \mu_1 = \mu_2\)\\
\(H_1: \mu_1 \neq \mu_2\)
\end{quote}

\section{Getting Good Data (Fundamental Idea
II)}\label{getting-good-data-fundamental-idea-ii-1}

As we are working with previously collected data, our goal in this
discussion is not how best to collect the data but making note of the
limitations of the data as a result of how it was collected. We
previously described the \protect\hyperlink{CaseOrganic}{Organic Food
Case Study} as an example of a controlled experiment. This was
true\ldots{}with regard to the primary question of interest (moral
expectations and food exposure). However, the subjects were \emph{not}
randomly assigned to gender here; therefore, with regard to this
question of interest, the data was an observational study.

It is common for young researchers to believe that if initially a
controlled experiment was performed that the data always permits a
causal interpretation. However, we must always examine the data
collection with respect to the question of interest. Such ``secondary
analyses'' (using data collected from a study to answer a question for
which the data was not initially collected) are generally observational
studies. As a result, there may be other factors related to gender and
moral expectations that drive any results we may see.

\section{Presenting the Data (Fundamental Idea
III)}\label{presenting-the-data-fundamental-idea-iii-1}

Our question here is examining the relationship between a quantitative
response (moral expectation score) and a categorical predictor (gender).
Figure \ref{fig:anovarecap-boxplot} compares the distribution of the
moral expectation score for the two groups. Note that 4 students did not
specify their gender; these subjects will be removed from the analysis.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/anovarecap-boxplot-1} 

}

\caption{Comparison of the moral expectations of males and females. The average value is added for each group.  Students who did not specify their gender were removed from the analysis.}\label{fig:anovarecap-boxplot}
\end{figure}

Based on the above graphic, it appears the females tend to have higher
moral expectations by about 1 point, compared to males. We also observe
that there are many more females in our sample compared to males, which
is probably a result of the type of class and the demographic makeup of
the university at which the study was conducted.

\section{Quantifying the Variability in the Estimate (Fundamental Idea
IV)}\label{quantifying-the-variability-in-the-estimate-fundamental-idea-iv-1}

In order to measure the size of the signal, we can compute the
standardized test statistic \[T = \frac{MSTrt}{MSE}\]

which is 6.52 for the sample we have observed. Of course, if we were to
collect a new sample, we would expect this value to change. If we want
to quantify the variability in this statistic, we need a model for its
sampling distribution. More, what we are really interested in is the
sampling distribution of this statistic if the average moral expectation
score were the same for the two genders; that is, we are interested in
the null distribution of this standardized test statistic. With the null
distribution, we could ascertain how unlikely (how strong the evidence)
our sample is.

In order to model the null distribution, we consider the following model
for the data-generating process:
\[\text{Moral Expectation Score} \sim \text{Gender}\] Formally, we write

\begin{equation}
  \begin{split}
    (\text{Moral Expectation Score})_i & = \mu_1\mathbb{I}(\text{i-th subject is a male}) \\
      & \quad + \mu_2\mathbb{I}(\text{i-th subject is a female}) + \epsilon_i
  \end{split}
  \label{eq:anovarecap-model}
\end{equation}

where we make the following assumptions on the error term:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The error in the moral expectation score for one individual is
  independent of the error in the moral expectation score for any other
  individual.
\item
  The variance of the error in the moral expectation scores for males is
  the same as the variance of the error in moral expectation scores for
  females.
\item
  The error in the moral expectation score for individuals follows a
  Normal Distribution.
\end{enumerate}

Under these three assumptions, we are able to construct a model for the
null distribution of the standardized test statistic. Figure
\ref{fig:anovarecap-classical-null-model} illustrates the null
distribution assuming these conditions are satisfied.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/anovarecap-classical-null-model-1} 

}

\caption{Model for the standardized test statistic measuring the signal comparing the moral expectation scores for males and females in the Organic Food Case Study.  This model is constructed assuming the classical ANOVA conditions are satisfied.}\label{fig:anovarecap-classical-null-model}
\end{figure}

Before we can use this model to make any conclusions regarding our
question of interest, we need to address the fact that we have assumed
certain conditions are satisfied. We need to assess whether the data is
consistent with these assumptions. This requires examining the residuals
for the model. That is, we must determine how far away the moral
expectation score for each individual is from the average moral
expectation score for their group.

First, we discuss the assumption of independence. Since the data was
collected at a single point in time, known as a \emph{cross-sectional
study}, constructing a time-series plot of the residuals would not
provide any information regarding this assumption. Instead, we rely on
the context of the problem to make some statements regarding whether the
data is consistent with this condition (whether making this assumption
is reasonable). It is reasonable that whether a student has a slightly
above (or below) moral expectation score is not related to whether any
other student has a slightly above (or below) moral expectation score.
That is, it is reasonable that the errors are independent. One case in
which this might be violated is if students discussed their answers to
the questions as they filled out the survey; then, it is plausible that
one student influenced another's responses. As this is unlikely given
the description of the data collection, we feel it is reasonable to
assume independence.

Again, note that there is a condition of independence; we are simply
saying whether we are willing to assume the condition is satisfied.
There is no way to ensure a condition holds.

In order to assess the condition of constant variance, let us look back
at the box plots given in Figure \ref{fig:anovarecap-boxplot}. As the
spread of the moral expectation score for each of the two genders is
roughly the same, it is reasonable to assume the variability of the
errors in each group is the same.

Finally, to assess the condition that the distribution of the errors is
Normal, we consider a probability plot of the residuals (Figure
\ref{fig:anovarecap-resids-probplot}). Given that the residuals tend to
display a linear relationship, it is reasonable that the residuals
represent a sample from a Normal Distribution. That is, it is reasonable
that the errors follow a Normal Distribution.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/anovarecap-resids-probplot-1} 

}

\caption{Probability plot assessing the assumption that the errors for our model comparing the moral expectation score across gender follow a Normal Distribution.}\label{fig:anovarecap-resids-probplot}
\end{figure}

Given that we are comfortable assuming the conditions on the error term
are reasonable, we can make use of the analytical model for the null
distribution in Figure \ref{fig:anovarecap-classical-null-model}.

\section{Quantifying the Evidence (Fundamental Idea
V)}\label{quantifying-the-evidence-fundamental-idea-v-1}

Now that we have a model for the null distribution, we can determine how
extreme our particular sample was by comparing the standardized test
statistic for our sample with this null distribution. We can measure
this through computation of a p-value, the probability that we would
observe a standardized test statistic of this magnitude or higher by
chance alone if there were no difference in the mean moral expecation
scores of males and females. This is summarized in Table
\ref{tab:anovarecap-anova-table} below.

\begin{table}

\caption{\label{tab:anovarecap-anova-table}ANOVA table summarizing the comparison of the moral expectation score across gender within the Organic Food Case Study.}
\centering
\begin{tabular}[t]{l|r|r|r|r|r}
\hline
Source & DF & SS & MS & F & P-value\\
\hline
Gender & 1 & 4.363 & 4.363 & 6.517 & 0.012\\
\hline
Residuals & 118 & 79.008 & 0.670 &  & \\
\hline
Total & 119 & 83.372 &  &  & \\
\hline
\end{tabular}
\end{table}

Based on the plot of the null distribution above, we should have
expected a small p-value since hardly any of the standardized statistics
exceeded the 6.517 we observed in our sample. From the results, we can
conclude that there is evidence (p = 0.012) of a relationship between
the moral expectations of a student and their gender. Looking back at
Figure \ref{fig:anovarecap-boxplot}, females tend to have higher moral
expectations.

\section{Conclusion}\label{conclusion}

Throughout this unit, we have examined a framework for examining the
association between a quantitative response and a categorical predictor.
This served to introduce a couple of big ideas we will make use of
throughout the remainder of this text:

\begin{itemize}
\tightlist
\item
  The key to measuring a signal is to partition the variability in the
  response.
\item
  A standardized test statistic is a numeric measure of the signal
  strength in the sample.
\item
  Modeling the data-generating process provides us a way of modeling the
  sampling distribution and null distribution of a standardized
  statistic.
\item
  Conditions are often placed on the noise portion of the model for the
  data-generating process; before assuming these conditions are met, we
  should graphically assess whether the data is consistent with these
  conditions.
\end{itemize}

Within this unit, we focused on a categorical predictor. In the next
unit, we consider the case when we have a quantitative predictor or even
a collection of predictors.

\chapter{Analyzing a Design that Incorporates
Blocking}\label{ANOVAblocking}

In Chapter \ref{ANOVAdata} we discussed various characteristics of a
good study design. Prior to this chapter, the study design has
influenced the interpretations of the results (can we assume cause and
effect, for example) but not the analysis itself. That is, whether the
data is from an observational study or a controlled experiment, we used
a model of the form \[\text{Response} \sim \text{Factor}\]

where we had a quantitative response and a categorical predictor (or
factor). However, it is not always the case where the study design and
the analysis are unrelated. When we incorporate blocking into the study
design, it has major implications with regard to the study. This chapter
explores those implications.

\section{What is the Big Deal?}\label{what-is-the-big-deal}

Why does blocking impact our analysis. Consider Example
\ref{ex:anova-golf} from Chapter \ref{ANOVAdata}; briefly, a study was
conducted to determine if the type of seed used on a golf green has an
effect on the distance a ball rolls on the green. This appears to fit
into the framework we have been discussing in this unit --- we have a
quantitative response (the distance the ball rolls) and a categorical
predictor (the type of seed used on the green). Therefore, we might
suggest the following model:
\[\text{Rolling Distance} \sim \text{Seed Variety}\]

Formally, we have that
\[(\text{Rolling Distance})_i = \sum_{j=1}^{5} \mu_j \mathbb{I}(\text{i-th ball rolled on green seeded with variety j}) + \epsilon_i\]

Further, we might assume the following conditions on the distribution of
the error term:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The error in the distance one ball rolls is independent of the error
  in the distance of any other ball.
\item
  The variance of the error in the distance a ball rolls is similar for
  all five seed varieties.
\item
  The error in the distance a ball rolls follows a Normal Distribution.
\end{enumerate}

The problem now arises. The errors cannot possibly be independent; in
fact, we purposely designed a study in which that was not the case!
Remember the goal of blocking is to group subjects which are alike with
respect to some inherent characteristic. In our example here, the greens
with a similar slope were grouped together and the randomization of a
green to a particular seeding variety occured within each block (group
of greens with similar slope). That is, we recognized that the way a
ball rolls within these greens is quite similar. So, if the ball rolls a
little further on average on a green with a steep slope, then we can
expect that it will roll a little further on average for most greens
within that same slope block. This suggests the errors in the rolling
distance are not independent.

Since the data is not consistent with the conditions we have placed on
the model, we are unable to conduct the same types of analyses
(classical or resampling-based) presented in the prior chapters. We need
a new strategy.

\section{Solution: Partition the
Variability}\label{solution-partition-the-variability}

We have seen throughout this unit that the key to measuring a signal is
to partition the sources of variability that contribute to the response.
Again, this is best explained through a graphic. Figure
\ref{fig:anovablocking-golf-raw} presents the rolling distance of the
golf balls on the various greens.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{./Images/anovablocking-golf-raw-1} 

}

\caption{Comparison of the rolling distance of golf balls seeded with one of five types of Rye grass.  Greens with similar slopes were grouped prior to randomization.  The slope groups, of which there are 4, are denoted by a common value.}\label{fig:anovablocking-golf-raw}
\end{figure}

Even the way we present the data must account for the relationship
present in the observed responses. Greens which belong to a similar
group are indicated on the graphic. We see that balls rolled on those
greens in group 3 tended to travel less distance, compared to balls on
other greens. However, for all groups, the ball tended to roll the
furthest when seeded with Variety A.

This discussion helps to tease out the sources of variability:

\begin{itemize}
\tightlist
\item
  Type of Rye Seed: the distance the ball rolled is potentially related
  to the type of Rye grass used on the green. That is, we might expect
  the distance to be different as we move from one variety to another.
  This is reflected in our primary question.
\item
  Slope of Green: the slope of the green will affect the distance the
  ball travels. As a result, greens were grouped. Therefore, the group
  to which the green belongs also contributes to the variability in the
  distance the ball rolls. While we would like to address this, it is
  not primarily part of our research question; this is a nuisance
  variable.
\item
  Error in the Process: balls do not always roll the same distance, even
  on two greens within the same slope group and assigned to the same Rye
  variety. This is the variability we cannot explain. It is also a
  nuisance.
\end{itemize}

The first and third sources of variability mentioned above are what we
have been discussing throughout this unit. The second source of
variability is a result of the blocked design. The solution to our
modeling problem is to simply incorporate this additional component of
the variability. As we will see in the next unit, this is a general
modeling strategy --- whenever we have additional information which
contributes to why there is variability in the response, we incorporate
that information into our model. What is unique here is that we are not
particularly interested in the slope of the green; it is a nuisance.
That is, we are not interested in determining which variety of Rye we
should place on a green with a particular slope; we actually believe
there is probably one superior type of seed regardless of the slope.
Placing the greens into groups was simply to reduce this additional
variability. The fact that the slope group is a nuisance, similar to the
overall noise in the data generating process, gives us an idea on how we
approach this in our model.

Our overall model is still
\[\text{Rolling Distance} \sim \text{Seed Variety}\]

However, we now need to partition the noise a bit further when we
formally write the model; this leads to \[
\begin{aligned}
  (\text{Rolling Distance})_i &= \sum_{j=1}^{5} \mu_j \mathbb{I}(\text{i-th ball rolled on green seeded with variety j}) \\
    &\quad + \sum_{k=1}^{4} \alpha_k\mathbb{I}(\text{i-th ball rolled on green belonging to slope group k}) \\
    &\quad + \epsilon_i
\end{aligned}
\]

We have essentially added an additional set of noise terms
\(\alpha_1, \dotsc, \alpha_4\) which capture the additional ``bump'' we
should expect to the distance the ball rolls as a result of being in a
particular slope group. Now, we can place certain conditions on each of
these error terms; for example,

\begin{itemize}
\tightlist
\item
  The bump to the rolling distance is the same for all balls within the
  same slope group.
\item
  The bump to the rolling distance for a ball in one group is
  independent of the bump to the rolling distance for a ball in any
  other group.
\item
  The bumps to the rolling distance across groups follows a Normal
  Distribution.
\item
  The error in the rolling distance for one ball within a slope group is
  independent of the error in the rolling distance for a ball within the
  same slope group.
\item
  The error in the rolling distance for balls within the same slope
  group is similar across all seeding varieties.
\item
  The error in the rolling distance follows a Normal Distribution.
\item
  The error in the rolling distance is independent of the bump that
  occurs to a ball for being from a particular slope group.
\end{itemize}

Many of these additional conditions (those placed on the ``bumps'')
cannot be assessed. Instead, we determine based on the context of the
problem whether we feel these assumptions are reasonable. Further
investigation of the details of this model and its conditions is beyond
the scope of this course. We focus on the interpretation of the
resulting output.

\section{Interpreting the Analysis}\label{interpreting-the-analysis}

The resulting output looks similar to what we have seen previously
(Table @ref); the only change is that the variability now includes an
additional component to the partition. As this additional component was
a nuisance, we refrain from making any interpretations from this
component. We focus instead on the component of interest: the
variability due to the factor.

\begin{table}

\caption{\label{tab:anovablocking-anova-table}ANOVA table summarizing the comparison of the ball roll for different seeding varieties while accounting for the slope of the green.}
\centering
\begin{tabular}[t]{l|r|r|r|r|r}
\hline
Source & DF & SS & MS & F & P-value\\
\hline
Rye Seed Variety & 4 & 0.452 & 0.113 & 8.345 & 0.002\\
\hline
Slope Group & 3 & 1.051 & 0.350 & 25.867 & 0.000\\
\hline
Residuals & 12 & 0.163 & 0.014 &  & \\
\hline
Total & 19 & 1.666 &  &  & \\
\hline
\end{tabular}
\end{table}

From the analysis, we have strong evidence (p = 0.002) that the distance
a ball rolls, on average, is associated with the type of Rye grass used
on the green. From our initial graphical summary of the data, if you
would like a course which has fast greens, we would recommend seeding
with Variety A.

\hypertarget{refs}{}
\hypertarget{ref-Dudeck1981}{}
Dudeck, A E, and C H Peeacock. 1981. ``Effects of Several Overseeded
Ryegrasses on Turf Quality, Traffic Tolerance and Ball Roll.'' In
\emph{Proceedings of the Fourth International Turfgrass Research
Conference}, edited by R W Sheard, 75--81.

\hypertarget{ref-Eskine2013}{}
Eskine, Kendall J. 2013. ``Wholesome Foods and Wholesome Morals? Organic
Foods Reduce Prosocial Behavior and Harshen Moral Judgments.''
\emph{Social Psychological and Personality Science} 4 (2): 251--54.

\hypertarget{ref-Goldstein2011}{}
Goldstein, Bernard D, Howard J Osofsky, and Maureen Y Lichtveld. 2011.
\emph{The Gulf Oil Spill}. \emph{The New England Journal of Medicine}.
Vol. 364.
doi:\href{https://doi.org/10.1056/NEJMra1007197}{10.1056/NEJMra1007197}.

\hypertarget{ref-Johnson2003}{}
Johnson, Eric J, and Daniel Goldstein. 2003. ``Do Defaults Save Lives?''
\emph{Science} 302: 1338--9.

\hypertarget{ref-Lee1992}{}
Lee, J. 1992. ``Relationships Between Properties of Pulp-Fibre and
Paper.''

\hypertarget{ref-Moery2016}{}
Moery, Eileen, and Robert J Calin-Jageman. 2016. ``Direct and Conceptual
Replications of Eskine (2013): Organic Food Exposure Has Little to No
Effect on Moral Judgments and Prosocial Behavior.'' \emph{Social
Psychological and Personality Science} 7 (4): 312--19.

\hypertarget{ref-Tintle2015}{}
Tintle, Nathan, Beth L Chance, A J Rossman, S Roy, T Swanson, and J
VanderStoep. 2015. \emph{Introduction to Statistical Investigations}.
Wiley.


\end{document}
