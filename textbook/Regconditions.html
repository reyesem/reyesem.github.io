<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Statistical Foundations for Engineers and Scientists</title>
  <meta name="description" content="Course notes for MA223 (Engineering Statistics I) at Rose-Hulman Institute of Technology.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Statistical Foundations for Engineers and Scientists" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course notes for MA223 (Engineering Statistics I) at Rose-Hulman Institute of Technology." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Statistical Foundations for Engineers and Scientists" />
  
  <meta name="twitter:description" content="Course notes for MA223 (Engineering Statistics I) at Rose-Hulman Institute of Technology." />
  

<meta name="author" content="Eric M Reyes">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="Regmodel.html">
<link rel="next" href="Regquality.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistical Modeling</a></li>

<li class="divider"></li>
<li class="part"><span><b>I Unit I: Language and Logic of Inference</b></span></li>
<li class="chapter" data-level="1" data-path="Basics.html"><a href="Basics.html"><i class="fa fa-check"></i><b>1</b> The Statistical Process</a><ul>
<li class="chapter" data-level="1.1" data-path="Basics.html"><a href="Basics.html#overview-of-drawing-inference"><i class="fa fa-check"></i><b>1.1</b> Overview of Drawing Inference</a></li>
<li class="chapter" data-level="1.2" data-path="Basics.html"><a href="Basics.html#anatomy-of-a-dataset"><i class="fa fa-check"></i><b>1.2</b> Anatomy of a Dataset</a></li>
<li class="chapter" data-level="1.3" data-path="Basics.html"><a href="Basics.html#a-note-on-codebooks"><i class="fa fa-check"></i><b>1.3</b> A Note on Codebooks</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="CaseDeepwater.html"><a href="CaseDeepwater.html"><i class="fa fa-check"></i><b>2</b> Case Study: Health Effects of the Deepwater Horizon Oil Spill</a></li>
<li class="chapter" data-level="3" data-path="Questions.html"><a href="Questions.html"><i class="fa fa-check"></i><b>3</b> Asking the Right Questions</a><ul>
<li class="chapter" data-level="3.1" data-path="Questions.html"><a href="Questions.html#characterizing-a-variable"><i class="fa fa-check"></i><b>3.1</b> Characterizing a Variable</a></li>
<li class="chapter" data-level="3.2" data-path="Questions.html"><a href="Questions.html#framing-the-question"><i class="fa fa-check"></i><b>3.2</b> Framing the Question</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Data.html"><a href="Data.html"><i class="fa fa-check"></i><b>4</b> Gathering the Evidence (Data Collection)</a><ul>
<li class="chapter" data-level="4.1" data-path="Data.html"><a href="Data.html#what-makes-a-sample-reliable"><i class="fa fa-check"></i><b>4.1</b> What Makes a Sample Reliable</a></li>
<li class="chapter" data-level="4.2" data-path="Data.html"><a href="Data.html#poor-methods-of-data-collection"><i class="fa fa-check"></i><b>4.2</b> Poor Methods of Data Collection</a></li>
<li class="chapter" data-level="4.3" data-path="Data.html"><a href="Data.html#preferred-methods-of-sampling"><i class="fa fa-check"></i><b>4.3</b> Preferred Methods of Sampling</a></li>
<li class="chapter" data-level="4.4" data-path="Data.html"><a href="Data.html#two-types-of-studies"><i class="fa fa-check"></i><b>4.4</b> Two Types of Studies</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Summaries.html"><a href="Summaries.html"><i class="fa fa-check"></i><b>5</b> Presenting the Evidence (Summarizing Data)</a><ul>
<li class="chapter" data-level="5.1" data-path="Summaries.html"><a href="Summaries.html#characteristics-of-a-distribution-summarizing-a-single-variable"><i class="fa fa-check"></i><b>5.1</b> Characteristics of a Distribution (Summarizing a Single Variable)</a></li>
<li class="chapter" data-level="5.2" data-path="Summaries.html"><a href="Summaries.html#summarizing-relationships"><i class="fa fa-check"></i><b>5.2</b> Summarizing Relationships</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="SamplingDistns.html"><a href="SamplingDistns.html"><i class="fa fa-check"></i><b>6</b> Assessing the Evidence (Quantifying the Variability in Estimates)</a><ul>
<li class="chapter" data-level="6.1" data-path="SamplingDistns.html"><a href="SamplingDistns.html#conceptualizing-the-sampling-distribution"><i class="fa fa-check"></i><b>6.1</b> Conceptualizing the Sampling Distribution</a></li>
<li class="chapter" data-level="6.2" data-path="SamplingDistns.html"><a href="SamplingDistns.html#example-of-a-sampling-distribution"><i class="fa fa-check"></i><b>6.2</b> Example of a Sampling Distribution</a></li>
<li class="chapter" data-level="6.3" data-path="SamplingDistns.html"><a href="SamplingDistns.html#modeling-the-sampling-distribution"><i class="fa fa-check"></i><b>6.3</b> Modeling the Sampling Distribution</a></li>
<li class="chapter" data-level="6.4" data-path="SamplingDistns.html"><a href="SamplingDistns.html#using-a-model-for-the-sampling-distributions-confidence-intervals"><i class="fa fa-check"></i><b>6.4</b> Using a Model for the Sampling Distributions (Confidence Intervals)</a></li>
<li class="chapter" data-level="6.5" data-path="SamplingDistns.html"><a href="SamplingDistns.html#bringing-it-all-together"><i class="fa fa-check"></i><b>6.5</b> Bringing it All Together</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="NullDistns.html"><a href="NullDistns.html"><i class="fa fa-check"></i><b>7</b> Quantifying the Evidence (Rejecting Bad Models)</a><ul>
<li class="chapter" data-level="7.1" data-path="NullDistns.html"><a href="NullDistns.html#some-subtleties"><i class="fa fa-check"></i><b>7.1</b> Some Subtleties</a></li>
<li class="chapter" data-level="7.2" data-path="NullDistns.html"><a href="NullDistns.html#assuming-the-null-hypothesis"><i class="fa fa-check"></i><b>7.2</b> Assuming the Null Hypothesis</a></li>
<li class="chapter" data-level="7.3" data-path="NullDistns.html"><a href="NullDistns.html#using-the-null-distribution"><i class="fa fa-check"></i><b>7.3</b> Using the Null Distribution</a></li>
<li class="chapter" data-level="7.4" data-path="NullDistns.html"><a href="NullDistns.html#sampling-distributions-vs.null-distributions"><i class="fa fa-check"></i><b>7.4</b> Sampling Distributions vs. Null Distributions</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="RecapLanguage.html"><a href="RecapLanguage.html"><i class="fa fa-check"></i><b>8</b> Using the Tools Together</a><ul>
<li class="chapter" data-level="8.1" data-path="RecapLanguage.html"><a href="RecapLanguage.html#framing-the-question-fundamental-idea-i"><i class="fa fa-check"></i><b>8.1</b> Framing the Question (Fundamental Idea I)</a></li>
<li class="chapter" data-level="8.2" data-path="RecapLanguage.html"><a href="RecapLanguage.html#getting-good-data-fundamental-idea-ii"><i class="fa fa-check"></i><b>8.2</b> Getting Good Data (Fundamental Idea II)</a></li>
<li class="chapter" data-level="8.3" data-path="RecapLanguage.html"><a href="RecapLanguage.html#presenting-the-data-fundamental-idea-iii"><i class="fa fa-check"></i><b>8.3</b> Presenting the Data (Fundamental Idea III)</a></li>
<li class="chapter" data-level="8.4" data-path="RecapLanguage.html"><a href="RecapLanguage.html#quantifying-the-variability-in-the-estimate-fundamental-idea-iv"><i class="fa fa-check"></i><b>8.4</b> Quantifying the Variability in the Estimate (Fundamental Idea IV)</a></li>
<li class="chapter" data-level="8.5" data-path="RecapLanguage.html"><a href="RecapLanguage.html#quantifying-the-evidence-fundamental-idea-v"><i class="fa fa-check"></i><b>8.5</b> Quantifying the Evidence (Fundamental Idea V)</a></li>
<li class="chapter" data-level="8.6" data-path="RecapLanguage.html"><a href="RecapLanguage.html#summary"><i class="fa fa-check"></i><b>8.6</b> Summary</a></li>
</ul></li>
<li class="part"><span><b>II Unit II: Implementing the Logic of Inference for a Single Mean</b></span></li>
<li class="chapter" data-level="9" data-path="CaseBabies.html"><a href="CaseBabies.html"><i class="fa fa-check"></i><b>9</b> Case Study: Birth Weights of Babies</a></li>
<li class="chapter" data-level="10" data-path="MeanModels.html"><a href="MeanModels.html"><i class="fa fa-check"></i><b>10</b> Model for the Data Generating Process</a><ul>
<li class="chapter" data-level="10.1" data-path="MeanModels.html"><a href="MeanModels.html#general-formulation"><i class="fa fa-check"></i><b>10.1</b> General Formulation</a></li>
<li class="chapter" data-level="10.2" data-path="MeanModels.html"><a href="MeanModels.html#statistical-model-for-a-quantitative-response-with-no-predictors"><i class="fa fa-check"></i><b>10.2</b> Statistical Model for a Quantitative Response with No Predictors</a></li>
<li class="chapter" data-level="10.3" data-path="MeanModels.html"><a href="MeanModels.html#conditions-on-the-error-distribution"><i class="fa fa-check"></i><b>10.3</b> Conditions on the Error Distribution</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="SingleConfInt.html"><a href="SingleConfInt.html"><i class="fa fa-check"></i><b>11</b> Estimating with Confidence</a></li>
<li class="chapter" data-level="12" data-path="SingleTeststat.html"><a href="SingleTeststat.html"><i class="fa fa-check"></i><b>12</b> Estimating with Confidence</a><ul>
<li class="chapter" data-level="12.1" data-path="SingleTeststat.html"><a href="SingleTeststat.html#standardized-statistics"><i class="fa fa-check"></i><b>12.1</b> Standardized Statistics</a></li>
<li class="chapter" data-level="12.2" data-path="SingleTeststat.html"><a href="SingleTeststat.html#computing-the-p-value"><i class="fa fa-check"></i><b>12.2</b> Computing the P-value</a></li>
</ul></li>
<li class="part"><span><b>III Unit III: Modeling the Average Response as a Function of a Continuous Predictor</b></span></li>
<li class="chapter" data-level="13" data-path="CaseGreece.html"><a href="CaseGreece.html"><i class="fa fa-check"></i><b>13</b> Case Study: Seismic Activity in Greece</a></li>
<li class="chapter" data-level="14" data-path="Regquestions.html"><a href="Regquestions.html"><i class="fa fa-check"></i><b>14</b> Myriad of Potential Questions</a></li>
<li class="chapter" data-level="15" data-path="Regdata.html"><a href="Regdata.html"><i class="fa fa-check"></i><b>15</b> Nature of Collecting Multivariable Data</a></li>
<li class="chapter" data-level="16" data-path="Regsummaries.html"><a href="Regsummaries.html"><i class="fa fa-check"></i><b>16</b> Summarizing Multivariable Data</a><ul>
<li class="chapter" data-level="16.1" data-path="Regsummaries.html"><a href="Regsummaries.html#characterizing-the-marginal-relationship-of-two-quantitative-variables"><i class="fa fa-check"></i><b>16.1</b> Characterizing the Marginal Relationship of Two Quantitative Variables</a></li>
<li class="chapter" data-level="16.2" data-path="Regsummaries.html"><a href="Regsummaries.html#visualizing-the-impact-of-a-third-variable-on-the-marginal-relationship"><i class="fa fa-check"></i><b>16.2</b> Visualizing the Impact of a Third Variable on the Marginal Relationship</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="Regmodel.html"><a href="Regmodel.html"><i class="fa fa-check"></i><b>17</b> Building our Statistical Model</a><ul>
<li class="chapter" data-level="17.1" data-path="Regmodel.html"><a href="Regmodel.html#statistical-model-for-a-quantitative-response-and-quantitative-predictor"><i class="fa fa-check"></i><b>17.1</b> Statistical Model for A Quantitative Response and Quantitative Predictor</a></li>
<li class="chapter" data-level="17.2" data-path="Regmodel.html"><a href="Regmodel.html#using-a-categorical-predictor"><i class="fa fa-check"></i><b>17.2</b> Using a Categorical Predictor</a></li>
<li class="chapter" data-level="17.3" data-path="Regmodel.html"><a href="Regmodel.html#estimating-the-parameters"><i class="fa fa-check"></i><b>17.3</b> Estimating the Parameters</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="Regconditions.html"><a href="Regconditions.html"><i class="fa fa-check"></i><b>18</b> Conditions on the Error Term of a Regression Model</a><ul>
<li class="chapter" data-level="18.1" data-path="Regconditions.html"><a href="Regconditions.html#correctly-specified-model"><i class="fa fa-check"></i><b>18.1</b> Correctly Specified Model</a><ul>
<li class="chapter" data-level="18.1.1" data-path="Regconditions.html"><a href="Regconditions.html#interpreting-the-parameters"><i class="fa fa-check"></i><b>18.1.1</b> Interpreting the Parameters</a></li>
<li class="chapter" data-level="18.1.2" data-path="Regconditions.html"><a href="Regconditions.html#embedding-our-question-in-a-statistical-framework"><i class="fa fa-check"></i><b>18.1.2</b> Embedding our Question in a Statistical Framework</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="Regconditions.html"><a href="Regconditions.html#additional-conditions"><i class="fa fa-check"></i><b>18.2</b> Additional Conditions</a><ul>
<li class="chapter" data-level="18.2.1" data-path="Regconditions.html"><a href="Regconditions.html#modeling-the-population"><i class="fa fa-check"></i><b>18.2.1</b> Modeling the Population</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="Regconditions.html"><a href="Regconditions.html#adding-the-assumption-of-normality"><i class="fa fa-check"></i><b>18.3</b> Adding the Assumption of Normality</a></li>
<li class="chapter" data-level="18.4" data-path="Regconditions.html"><a href="Regconditions.html#classical-regression-model"><i class="fa fa-check"></i><b>18.4</b> Classical Regression Model</a></li>
<li class="chapter" data-level="18.5" data-path="Regconditions.html"><a href="Regconditions.html#imposing-the-conditions"><i class="fa fa-check"></i><b>18.5</b> Imposing the Conditions</a></li>
<li class="chapter" data-level="18.6" data-path="Regconditions.html"><a href="Regconditions.html#recap"><i class="fa fa-check"></i><b>18.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="Regquality.html"><a href="Regquality.html"><i class="fa fa-check"></i><b>19</b> Quantifying the Quality of a Model Fit</a><ul>
<li class="chapter" data-level="19.1" data-path="Regquality.html"><a href="Regquality.html#partitioning-variability"><i class="fa fa-check"></i><b>19.1</b> Partitioning Variability</a></li>
<li class="chapter" data-level="19.2" data-path="Regquality.html"><a href="Regquality.html#hypothesis-testing"><i class="fa fa-check"></i><b>19.2</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="19.3" data-path="Regquality.html"><a href="Regquality.html#r-squared"><i class="fa fa-check"></i><b>19.3</b> R-squared</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="Regassessment.html"><a href="Regassessment.html"><i class="fa fa-check"></i><b>20</b> Assessing Modeling Conditions</a><ul>
<li class="chapter" data-level="20.1" data-path="Regassessment.html"><a href="Regassessment.html#residuals"><i class="fa fa-check"></i><b>20.1</b> Residuals</a></li>
<li class="chapter" data-level="20.2" data-path="Regassessment.html"><a href="Regassessment.html#assessing-mean-0"><i class="fa fa-check"></i><b>20.2</b> Assessing Mean 0</a></li>
<li class="chapter" data-level="20.3" data-path="Regassessment.html"><a href="Regassessment.html#assessing-independence"><i class="fa fa-check"></i><b>20.3</b> Assessing Independence</a></li>
<li class="chapter" data-level="20.4" data-path="Regassessment.html"><a href="Regassessment.html#assessing-homoskedasticity"><i class="fa fa-check"></i><b>20.4</b> Assessing Homoskedasticity</a></li>
<li class="chapter" data-level="20.5" data-path="Regassessment.html"><a href="Regassessment.html#assessing-normality"><i class="fa fa-check"></i><b>20.5</b> Assessing Normality</a></li>
<li class="chapter" data-level="20.6" data-path="Regassessment.html"><a href="Regassessment.html#general-tips-for-assessing-assumptions"><i class="fa fa-check"></i><b>20.6</b> General Tips for Assessing Assumptions</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="Regextensions.html"><a href="Regextensions.html"><i class="fa fa-check"></i><b>21</b> Extending the Regression Model</a><ul>
<li class="chapter" data-level="21.1" data-path="Regextensions.html"><a href="Regextensions.html#including-multiple-precitors"><i class="fa fa-check"></i><b>21.1</b> Including Multiple Precitors</a><ul>
<li class="chapter" data-level="21.1.1" data-path="Regextensions.html"><a href="Regextensions.html#general-model-formulation"><i class="fa fa-check"></i><b>21.1.1</b> General Model Formulation</a></li>
<li class="chapter" data-level="21.1.2" data-path="Regextensions.html"><a href="Regextensions.html#interpretation-of-parameters"><i class="fa fa-check"></i><b>21.1.2</b> Interpretation of Parameters</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="Regextensions.html"><a href="Regextensions.html#modifying-an-effect"><i class="fa fa-check"></i><b>21.2</b> Modifying an Effect</a><ul>
<li class="chapter" data-level="21.2.1" data-path="Regextensions.html"><a href="Regextensions.html#inference-for-effect-modifications"><i class="fa fa-check"></i><b>21.2.1</b> Inference for Effect Modifications</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="Regrecap.html"><a href="Regrecap.html"><i class="fa fa-check"></i><b>22</b> Puting it All Together</a><ul>
<li class="chapter" data-level="22.1" data-path="Regrecap.html"><a href="Regrecap.html#graphical-summary"><i class="fa fa-check"></i><b>22.1</b> Graphical Summary</a></li>
<li class="chapter" data-level="22.2" data-path="Regrecap.html"><a href="Regrecap.html#development-of-statistical-model"><i class="fa fa-check"></i><b>22.2</b> Development of Statistical Model</a></li>
<li class="chapter" data-level="22.3" data-path="Regrecap.html"><a href="Regrecap.html#assessment-of-conditions"><i class="fa fa-check"></i><b>22.3</b> Assessment of Conditions</a></li>
<li class="chapter" data-level="22.4" data-path="Regrecap.html"><a href="Regrecap.html#summary-of-model-fit"><i class="fa fa-check"></i><b>22.4</b> Summary of Model Fit</a></li>
</ul></li>
<li class="part"><span><b>IV Unit IV: Comparing the Average Response Across Groups</b></span></li>
<li class="chapter" data-level="23" data-path="CaseOrganic.html"><a href="CaseOrganic.html"><i class="fa fa-check"></i><b>23</b> Case Study: Organic Foods and Superior Morals</a></li>
<li class="chapter" data-level="24" data-path="ANOVAquestions.html"><a href="ANOVAquestions.html"><i class="fa fa-check"></i><b>24</b> Framing the Question</a><ul>
<li class="chapter" data-level="24.1" data-path="ANOVAquestions.html"><a href="ANOVAquestions.html#general-setting"><i class="fa fa-check"></i><b>24.1</b> General Setting</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="ANOVAdata.html"><a href="ANOVAdata.html"><i class="fa fa-check"></i><b>25</b> Study Design</a><ul>
<li class="chapter" data-level="25.1" data-path="ANOVAdata.html"><a href="ANOVAdata.html#aspects-of-a-well-designed-experiment"><i class="fa fa-check"></i><b>25.1</b> Aspects of a Well Designed Experiment</a></li>
<li class="chapter" data-level="25.2" data-path="ANOVAdata.html"><a href="ANOVAdata.html#collecting-observational-data"><i class="fa fa-check"></i><b>25.2</b> Collecting Observational Data</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="ANOVAsummaries.html"><a href="ANOVAsummaries.html"><i class="fa fa-check"></i><b>26</b> Presenting the Data</a></li>
<li class="chapter" data-level="27" data-path="ANOVAmodel.html"><a href="ANOVAmodel.html"><i class="fa fa-check"></i><b>27</b> Building the Statistical Model</a><ul>
<li class="chapter" data-level="27.1" data-path="ANOVAmodel.html"><a href="ANOVAmodel.html#statistical-model-for-a-quantitative-response-and-a-categorical-predictor"><i class="fa fa-check"></i><b>27.1</b> Statistical Model for A Quantitative Response and a Categorical Predictor</a></li>
<li class="chapter" data-level="27.2" data-path="ANOVAmodel.html"><a href="ANOVAmodel.html#conditions-on-the-error-distribution-1"><i class="fa fa-check"></i><b>27.2</b> Conditions on the Error Distribution</a></li>
<li class="chapter" data-level="27.3" data-path="ANOVAmodel.html"><a href="ANOVAmodel.html#classical-anova-model"><i class="fa fa-check"></i><b>27.3</b> Classical ANOVA Model</a></li>
<li class="chapter" data-level="27.4" data-path="ANOVAmodel.html"><a href="ANOVAmodel.html#imposing-the-conditions-1"><i class="fa fa-check"></i><b>27.4</b> Imposing the Conditions</a></li>
<li class="chapter" data-level="27.5" data-path="ANOVAmodel.html"><a href="ANOVAmodel.html#recap-1"><i class="fa fa-check"></i><b>27.5</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="ANOVAteststat.html"><a href="ANOVAteststat.html"><i class="fa fa-check"></i><b>28</b> Quantifying the Evidence</a><ul>
<li class="chapter" data-level="28.1" data-path="ANOVAteststat.html"><a href="ANOVAteststat.html#partitioning-variability-1"><i class="fa fa-check"></i><b>28.1</b> Partitioning Variability</a></li>
<li class="chapter" data-level="28.2" data-path="ANOVAteststat.html"><a href="ANOVAteststat.html#forming-a-standardized-test-statistic"><i class="fa fa-check"></i><b>28.2</b> Forming a Standardized Test Statistic</a></li>
<li class="chapter" data-level="28.3" data-path="ANOVAteststat.html"><a href="ANOVAteststat.html#link-to-regression-analysis"><i class="fa fa-check"></i><b>28.3</b> Link to Regression Analysis</a></li>
<li class="chapter" data-level="28.4" data-path="ANOVAteststat.html"><a href="ANOVAteststat.html#obtaining-a-p-value"><i class="fa fa-check"></i><b>28.4</b> Obtaining a P-value</a></li>
<li class="chapter" data-level="28.5" data-path="ANOVAteststat.html"><a href="ANOVAteststat.html#anova-table"><i class="fa fa-check"></i><b>28.5</b> ANOVA Table</a></li>
<li class="chapter" data-level="28.6" data-path="ANOVAteststat.html"><a href="ANOVAteststat.html#simulating-the-null-distribution"><i class="fa fa-check"></i><b>28.6</b> Simulating the Null Distribution</a></li>
<li class="chapter" data-level="28.7" data-path="ANOVAteststat.html"><a href="ANOVAteststat.html#recap-2"><i class="fa fa-check"></i><b>28.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="29" data-path="ANOVAassessment.html"><a href="ANOVAassessment.html"><i class="fa fa-check"></i><b>29</b> Assessing Modeling Assumptions</a><ul>
<li class="chapter" data-level="29.1" data-path="ANOVAassessment.html"><a href="ANOVAassessment.html#assessing-independence-1"><i class="fa fa-check"></i><b>29.1</b> Assessing Independence</a></li>
<li class="chapter" data-level="29.2" data-path="ANOVAassessment.html"><a href="ANOVAassessment.html#assessing-homoskedasticity-1"><i class="fa fa-check"></i><b>29.2</b> Assessing Homoskedasticity</a></li>
<li class="chapter" data-level="29.3" data-path="ANOVAassessment.html"><a href="ANOVAassessment.html#assessing-normality-1"><i class="fa fa-check"></i><b>29.3</b> Assessing Normality</a></li>
<li class="chapter" data-level="29.4" data-path="ANOVAassessment.html"><a href="ANOVAassessment.html#general-tips-for-assessing-assumptions-1"><i class="fa fa-check"></i><b>29.4</b> General Tips for Assessing Assumptions</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="ANOVArecap.html"><a href="ANOVArecap.html"><i class="fa fa-check"></i><b>30</b> Using the Tools Together</a><ul>
<li class="chapter" data-level="30.1" data-path="ANOVArecap.html"><a href="ANOVArecap.html#framing-the-question-fundamental-idea-i-1"><i class="fa fa-check"></i><b>30.1</b> Framing the Question (Fundamental Idea I)</a></li>
<li class="chapter" data-level="30.2" data-path="ANOVArecap.html"><a href="ANOVArecap.html#getting-good-data-fundamental-idea-ii-1"><i class="fa fa-check"></i><b>30.2</b> Getting Good Data (Fundamental Idea II)</a></li>
<li class="chapter" data-level="30.3" data-path="ANOVArecap.html"><a href="ANOVArecap.html#presenting-the-data-fundamental-idea-iii-1"><i class="fa fa-check"></i><b>30.3</b> Presenting the Data (Fundamental Idea III)</a></li>
<li class="chapter" data-level="30.4" data-path="ANOVArecap.html"><a href="ANOVArecap.html#quantifying-the-variability-in-the-estimate-fundamental-idea-iv-1"><i class="fa fa-check"></i><b>30.4</b> Quantifying the Variability in the Estimate (Fundamental Idea IV)</a></li>
<li class="chapter" data-level="30.5" data-path="ANOVArecap.html"><a href="ANOVArecap.html#quantifying-the-evidence-fundamental-idea-v-1"><i class="fa fa-check"></i><b>30.5</b> Quantifying the Evidence (Fundamental Idea V)</a></li>
<li class="chapter" data-level="30.6" data-path="ANOVArecap.html"><a href="ANOVArecap.html#conclusion"><i class="fa fa-check"></i><b>30.6</b> Conclusion</a></li>
</ul></li>
<li class="part"><span><b>V Unit V: Comparing the Average Response Across Correlated Groups</b></span></li>
<li class="chapter" data-level="31" data-path="CaseYogurt.html"><a href="CaseYogurt.html"><i class="fa fa-check"></i><b>31</b> Case Study: Paying a Premium for the Experience</a></li>
<li class="chapter" data-level="32" data-path="Blockquestions.html"><a href="Blockquestions.html"><i class="fa fa-check"></i><b>32</b> Framing the Question</a><ul>
<li class="chapter" data-level="32.1" data-path="Blockquestions.html"><a href="Blockquestions.html#general-setting-1"><i class="fa fa-check"></i><b>32.1</b> General Setting</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="Blockdata.html"><a href="Blockdata.html"><i class="fa fa-check"></i><b>33</b> Correlated Data</a></li>
<li class="chapter" data-level="34" data-path="Blocksummaries.html"><a href="Blocksummaries.html"><i class="fa fa-check"></i><b>34</b> Presenting Correlated Data</a></li>
<li class="chapter" data-level="35" data-path="Blockmodel.html"><a href="Blockmodel.html"><i class="fa fa-check"></i><b>35</b> Analyzing Correlated Responses</a><ul>
<li class="chapter" data-level="35.1" data-path="Blockmodel.html"><a href="Blockmodel.html#statistical-model-for-correlated-responses"><i class="fa fa-check"></i><b>35.1</b> Statistical Model for Correlated Responses</a></li>
<li class="chapter" data-level="35.2" data-path="Blockmodel.html"><a href="Blockmodel.html#conditions-on-the-error-distribution-2"><i class="fa fa-check"></i><b>35.2</b> Conditions on the Error Distribution</a></li>
<li class="chapter" data-level="35.3" data-path="Blockmodel.html"><a href="Blockmodel.html#conditions-on-the-random-effects"><i class="fa fa-check"></i><b>35.3</b> Conditions on the Random Effects</a></li>
<li class="chapter" data-level="35.4" data-path="Blockmodel.html"><a href="Blockmodel.html#classical-repeated-measures-anova-model"><i class="fa fa-check"></i><b>35.4</b> Classical Repeated Measures ANOVA Model</a></li>
<li class="chapter" data-level="35.5" data-path="Blockmodel.html"><a href="Blockmodel.html#recap-3"><i class="fa fa-check"></i><b>35.5</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="36" data-path="Blockteststat.html"><a href="Blockteststat.html"><i class="fa fa-check"></i><b>36</b> Quantifying the Evidence</a><ul>
<li class="chapter" data-level="36.1" data-path="Blockteststat.html"><a href="Blockteststat.html#partitioning-variability-2"><i class="fa fa-check"></i><b>36.1</b> Partitioning Variability</a></li>
<li class="chapter" data-level="36.2" data-path="Blockteststat.html"><a href="Blockteststat.html#forming-a-standardized-test-statistic-1"><i class="fa fa-check"></i><b>36.2</b> Forming a Standardized Test Statistic</a></li>
<li class="chapter" data-level="36.3" data-path="Blockteststat.html"><a href="Blockteststat.html#anova-table-1"><i class="fa fa-check"></i><b>36.3</b> ANOVA Table</a></li>
<li class="chapter" data-level="36.4" data-path="Blockteststat.html"><a href="Blockteststat.html#recap-4"><i class="fa fa-check"></i><b>36.4</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="37" data-path="Blockassessment.html"><a href="Blockassessment.html"><i class="fa fa-check"></i><b>37</b> Assessing Modeling Assumptions</a><ul>
<li class="chapter" data-level="37.1" data-path="Blockassessment.html"><a href="Blockassessment.html#assessing-independence-2"><i class="fa fa-check"></i><b>37.1</b> Assessing Independence</a></li>
<li class="chapter" data-level="37.2" data-path="Blockassessment.html"><a href="Blockassessment.html#assessing-identical-distribution"><i class="fa fa-check"></i><b>37.2</b> Assessing Identical Distribution</a></li>
<li class="chapter" data-level="37.3" data-path="Blockassessment.html"><a href="Blockassessment.html#assessing-normality-2"><i class="fa fa-check"></i><b>37.3</b> Assessing Normality</a></li>
<li class="chapter" data-level="37.4" data-path="Blockassessment.html"><a href="Blockassessment.html#general-tips-for-assessing-assumptions-2"><i class="fa fa-check"></i><b>37.4</b> General Tips for Assessing Assumptions</a></li>
</ul></li>
<li class="chapter" data-level="38" data-path="Blockrecap.html"><a href="Blockrecap.html"><i class="fa fa-check"></i><b>38</b> Using the Tools Together</a><ul>
<li class="chapter" data-level="38.1" data-path="Blockrecap.html"><a href="Blockrecap.html#framing-the-question-fundamental-idea-i-2"><i class="fa fa-check"></i><b>38.1</b> Framing the Question (Fundamental Idea I)</a></li>
<li class="chapter" data-level="38.2" data-path="Blockrecap.html"><a href="Blockrecap.html#getting-good-data-fundamental-idea-ii-2"><i class="fa fa-check"></i><b>38.2</b> Getting Good Data (Fundamental Idea II)</a></li>
<li class="chapter" data-level="38.3" data-path="Blockrecap.html"><a href="Blockrecap.html#presenting-the-data-fundamental-idea-iii-2"><i class="fa fa-check"></i><b>38.3</b> Presenting the Data (Fundamental Idea III)</a></li>
<li class="chapter" data-level="38.4" data-path="Blockrecap.html"><a href="Blockrecap.html#quantifying-the-variability-in-the-estimate-fundamental-idea-iv-2"><i class="fa fa-check"></i><b>38.4</b> Quantifying the Variability in the Estimate (Fundamental Idea IV)</a></li>
<li class="chapter" data-level="38.5" data-path="Blockrecap.html"><a href="Blockrecap.html#quantifying-the-evidence-fundamental-idea-v-2"><i class="fa fa-check"></i><b>38.5</b> Quantifying the Evidence (Fundamental Idea V)</a></li>
<li class="chapter" data-level="38.6" data-path="Blockrecap.html"><a href="Blockrecap.html#conclusion-1"><i class="fa fa-check"></i><b>38.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Foundations for Engineers and Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="Regconditions" class="section level1">
<h1><span class="header-section-number">18</span> Conditions on the Error Term of a Regression Model</h1>
<p>In the previous chapter we developed a general model for generating a quantitative response as a linear function of a quantitative predictor:</p>
<p><span class="math display">\[(\text{Response})_i = \beta_0 + \beta_1 (\text{Predictor})_{i} + \epsilon_i\]</span></p>
<p>We also discussed a common method for estimating the parameters of this model from a sample — the least squares method. However, if we are to construct a model for the sampling distribution of these estimates we must add some structure to the stochastic component <span class="math inline">\(\epsilon\)</span> in the model. We will find that the more assumptions we are willing to make, the easier the analysis, but the less likely our model is to be applicable to the actual data-generating process we have observed. The conditions we make dictate how we conduct inference (the computation of a p-value or confidence interval).</p>
<div id="correctly-specified-model" class="section level2">
<h2><span class="header-section-number">18.1</span> Correctly Specified Model</h2>
<p>The first condition we consider is the most important. It states that for every value of the predictor, the average error is 0. This condition implies that the model we have posited for the data generating process is accurate; that is, it implies that the form of the model is appropriate — that the response is linearly related to the predictor. There are two reasons we say that this is the most important condition:</p>
<ol style="list-style-type: decimal">
<li>If this condition is violated, it says your model for the data generating process is incorrect. Generally this is the result of ignoring some curvature or additional feature.</li>
<li>This condition allows us to interpret the parameters of the model.</li>
</ol>
<div id="interpreting-the-parameters" class="section level3">
<h3><span class="header-section-number">18.1.1</span> Interpreting the Parameters</h3>
<p>In the second unit, we were were focused on the mean response. Now, instead of considering the average response overall, we are asking what the average response is for subjects in the population with a specific value of the predictor(s). When we impose the “mean 0 condition,” we are saying the errors are not biasing the average response (since on average, they have a value of 0); therefore, we are able to say that the determinisic portion of our model is giving the <em>average</em> response for a specified value of the predictor(s).</p>

<div class="rmdkeyidea">
The deterministic portion of a regression model specifies the <em>average</em> value of the response given the value(s) of the predictor(s).
</div>

<p>As an example, consider our model for the <a href="CaseGreece.html#CaseGreece">Seismic Activity Case Study</a> which predicted the bracketed duration as a function of the magnitude of the earthquake:</p>
<p><span class="math display">\[(\text{Bracketed Duration})_i = \beta_0 + \beta_1(\text{Magnitude})_i + \epsilon_i\]</span></p>
<p>When the errors have mean 0 for all magnitudes (our first condition), then earthquakes with a 5.0 magnitude have an <em>average</em> bracketed duration of</p>
<p><span class="math display">\[\beta_0 + \beta_1(5)\]</span></p>
<p>Similarly, earthquakes with a 6.0 magnitude have an <em>average</em> bracketed duration of</p>
<p><span class="math display">\[\beta_0 + \beta_1(6)\]</span></p>
<p>As we have mentioned, the deterministic portion of the model does not specify the exact response for any individual but the trend. We are now able to say that the “trend” we are modeling is the average response. Further, we can estimate this average response by plugging in the least squares estimates <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span>. Specifically, using the method of least squares, the line of best fit was estimated as</p>
<p><span class="math display">\[(\text{Bracketed Duration}) = -19.19 + 4.48 (\text{Magnitude})\]</span></p>
<p>Therefore, we estimate the average bracketed duration for locations with 5.0 magnitude earthquakes to be 3.22 seconds (7.71 seconds for locations with 6.0 magnitude earthquakes). While we do not expect every location which has a 5.0 magnitude earthquake to have a bracketed duration of this length, we expect the bracketed duration to vary about this length of time. This is huge; it says that when we use a regression model to predict a response, we are actually predicting the <em>average</em> response. More, we can interpret the parameters themselves.</p>
<p>Let’s begin with the intercept term, <span class="math inline">\(\beta_0\)</span>. Notice that in our model above, if we try to predict the bracketed duration for a location with an earthquake which has a magnitude of 0, then our model returns <span class="math inline">\(\beta_0\)</span>. In fact, for any regression model, the intercept <span class="math inline">\(\beta_0\)</span> is the value of the deterministic portion of the model whenever all predictors in the model are set to 0. And, that deterministic portion is the average response.</p>

<div class="rmdtip">
The intercept in a regression model <span class="math inline">\(\beta_0\)</span> represents the <em>average</em> response when all predictors in the model are set equal to 0. Note that this may not be practically meaningful in all contexts.
</div>

<p>For our particular example, the estimate of the intercept does not make sense — what does it mean to have a duration of -19.19 seconds? More, it does not make sense to estimate the average bracketed duration for an earthquake which had a magnitude of 0 (not even an earthquake). This can often be the case when trying to interpret the intercept term due to what we call <strong>extrapolation</strong>. We do not have any data on the bracketed duration for locations which experienced an earthquake with a magnitude less than 4.5. Therefore, we are using a model to predict for a region over which the model was not constructed to operate. This is a lot like using a screw driver to hammer a nail — we are using a tool to accomplish a task for which it was not designed. We should not be surprised when it fails. The primary reason extrapolation is dangerous is that without data in a particular region, we have nothing supporting that the model will continue to hold in that region. We have illustrated this when discussing the intercept, but extrapolation can occur in any region for which there is no data. For this reason, unless you have strong scientific justification for why a model will hold over all values of the predictor, extrapolation should be avoided</p>

<div class="definition">
<span id="def:defn-extrapolation" class="definition"><strong>Definition 18.1  (Extrapolation)  </strong></span>Using a model to predict outside of a region for which data is available.
</div>

<p>We have seen that the intercept is the average value of the response when the predictor has the value of 0. How then do we interpret the coefficient associated with the predictor (the slope). We again use an example. Notice that based on our estimates, the average bracketed duration is 4.48 seconds longer for those locations which experience a 6.0 magnitude earthquake compared to those which experience a 5.0 magnitude earthquake, and this difference is the value of the estimated slope. This leads us to observing that 4.48 seconds is the change in the average bracketed duration that is associated with a 1-unit increase in the magnitude of an earthquake.</p>

<div class="rmdtip">
The coefficient (or slope) <span class="math inline">\(\beta_1\)</span> in a regression model represents the <em>average</em> change in the response associated with a 1 unit <em>increase</em> in the predictor.
</div>

</div>
<div id="embedding-our-question-in-a-statistical-framework" class="section level3">
<h3><span class="header-section-number">18.1.2</span> Embedding our Question in a Statistical Framework</h3>
<p>Our first fundamental idea centers on the idea that the majority of research questions can be framed in terms of a parameter within the population. This seemed somewhat intuitive when the parameter was simply the mean response. With parameters which are the slope and intercept of a line, this seems less clear. However, this condition that the errors have mean 0 for all values of the predictor (because of its implications on the interpretation of the parameters) ensures that our questions of interest can be framed in terms of the parameters. Consider the following question:</p>
<blockquote>
<p>On average, is the bracketed duration related to the magnitude of an earthquake?</p>
</blockquote>
<p>Let’s consider how we might write this in terms of a null and alternative hypotheses.</p>
<blockquote>
<p><span class="math inline">\(H_0:\)</span> the bracketed duration does not change, on average, as the magnitude changes.<br />
<span class="math inline">\(H_1:\)</span> the bracketed duration is linearly related, on average, with the magnitude; that is, as the magnitude increases, the bracketed duration changes, on average.</p>
</blockquote>
<p>In order to address this question, we considered the following model for the data generating process:</p>
<p><span class="math display">\[(\text{Bracketed Duration})_i = \beta_0 + \beta_1(\text{Magnitude})_i + \epsilon_i\]</span></p>
<p>If the null hypothesis above is true, then that suggests that the bracketed duration is flat on average, regardless of the value of the magnitude. What would be true about the parameters if that were true? Remember that <span class="math inline">\(\beta_1\)</span> captures the change in the average bracketed duration as the magnitude increases by 1 unit; and, the null hypothesis says that there is no change in the average bracketed duration as the magnitude changes. That is, if the null hypothesis is true, <span class="math inline">\(\beta_1 = 0\)</span>. Said another way, we need a model for which changing the value of the magnitude does not affect the resulting bracketed duration — a flat line. Therefore, our null and alternative hypotheses can be written as</p>
<blockquote>
<p><span class="math inline">\(H_0: \beta_1 = 0\)</span><br />
<span class="math inline">\(H_1: \beta_1 \neq 0\)</span></p>
</blockquote>
<p>where <span class="math inline">\(\beta_1\)</span> is the parameter linearly relating the bracketed duration to the magnitude. That is, if the paramter associated with magnitude is 0, then it is plays no role in the data generating process; if it is anything other than 0, then magnitude has a role within the data generating process.</p>

<div class="rmdkeyidea">
Setting a slope parameter to 0 in the model for a data generating process is associated with saying that the corresponding predictor is not associated with the response in a linear fashion — that it does not belong in the model.
</div>

<p>The interpretation of our parameters allows us to see that our research questions are characterizing the relationship between the response and the predictor, <em>on average</em>. As in the previous unit, our questions are about the average response; instead of looking at the overall average, however, we are allowing it to depend upon a predictor.</p>
<p>This first condition on the error term — holding the average error to be 0 for all values of the predictor — is only one condition typically placed on the stochastic portion. We now desribe others.</p>
</div>
</div>
<div id="additional-conditions" class="section level2">
<h2><span class="header-section-number">18.2</span> Additional Conditions</h2>
<p>The second condition we consider is that the noise attributed to one observed individual is independent of the noise attributed to any other individual observed. That is, the amount of error in any one individual’s response is unrelated to the error in any other response observed. This is the same condition we introduced in Chapter <a href="MeanModels.html#MeanModels">10</a>. We still want each observation in our data to be independent of one another.</p>
<p>With just these first two conditions (that the average error is 0 for all values of the predictors and the errors are independent of one another), we can use a bootstrap algorithm in order to model the sampling distribution of the least squares estimate for the slope (as well as the intercept). However, additional conditions are often placed on the error term.</p>
<p>The third condition that is typically placed on the distribution of the errors is that the errors are identically distributed. Again, we introduced this condition in Chapter <a href="MeanModels.html#MeanModels">10</a>. However, in the context of regression, this is often described a bit differently. In particular, if the errors are not identically distributed, it is typically because the variability of the error differs for one value of the predictor compared to another. Practically, this reveals itself as our response being more precise in one region than in another. As a result of focusing on the variability of the response for each predictor, this condition is often referred to as <em>homoskedasticity</em> instead of the errors being identically distributed.</p>
<p>With this additional condition imposed, we are able to modify our bootstrap algorithm when constructing a model for the sampling distribution of the least squares estimates. Because we are relying on a bootstrap procedure, our model for the sampling distribution or null distribution, depending on whether we are interested in computing a confidence interval or p-value) is empirical. As a result, our model for the sampling distribution can be unstable in small sample sizes; this can be avoided by building an analytical model for the sampling distribution. This requires us to impose a fourth condition (common in the engineering and science disciplines) on the distribution of the errors and then rely on some probability theory.</p>
<div id="modeling-the-population" class="section level3">
<h3><span class="header-section-number">18.2.1</span> Modeling the Population</h3>
<p>Before we delve into more detail, let’s set the stage for the bigger story being told. Recall that our goal is to say something about the population using a sample. We have developed a process to address this goal:</p>
<ol style="list-style-type: decimal">
<li>Frame our question through a parameter of interest.</li>
<li>Collect data that allows us to estimate the parameter using the analogous statistic within the sample.</li>
<li>Summarize the variability in the data graphically.</li>
<li>Quantify the variability in the statistic through modeling the sampling distribution (or null distribution, whichever is appropriate).</li>
<li>Using the sampling distribution , quantify the evidence in the sample.</li>
</ol>
<p>This process is presented through our <em>Five Fundamental Ideas of Inference</em> and the <em>Distributional Quartet</em>. The key step in this process is quantifying the variability by modeling the <em>sampling distribution</em> (or <em>null distribution</em>, whichever is appropriate for our research goal). We have described the construction of these models empirically, through repeating the study by appropriately resampling the data available and performing the analysis on each resample.</p>
<p>Our goal is still to model the sampling distribution; that is the key inferential step. Instead of building an empirical model, we can construct an exact analytical model through an additional step: modeling the population directly.</p>

<div class="rmdkeyidea">
A model for the sampling distribution of a statistic can often be obtained by placing a model on the distribution of the population.
</div>

<p>So, we have two distributional models; the model for the distribution of the population is simply a stepping stone to a model for the sampling distribution of the statistic, which is what we really need. It is important to separate these steps. We are not interested in directly modeling the population; we do it in order to construct a model for the sampling distribution.</p>
<p>There is one other distinction to make: a model for the population is <em>always</em> an assumption. We hope that the data is consistent with this assumption in order to apply the resulting model for the sampling distribution. In later chapters, we will discuss how we assess whether our data is consistent with these conditions; for now, we simiply want to understand we are making an assumption when we place such a condition on the stochastic portion of data generating process.</p>
</div>
</div>
<div id="adding-the-assumption-of-normality" class="section level2">
<h2><span class="header-section-number">18.3</span> Adding the Assumption of Normality</h2>
<p>Probability, a sub-field of mathematics which is used heavily in statistics, is the discipline of modeling randomness. In particular, we make use of probability to model a distribution. In order to get a feel for probability models, consider the following example.</p>

<div class="example">
<span id="exm:ex-iris" class="example"><strong>Example 18.1  (Iris Characteristics)  </strong></span>The discipline of statistics began in the early 1900’s primarily within the context of agricultural research. Edgar Anderson was a researcher investigating the characteristics of the iris. He had collected measurements on over one hundred iris flowers, including their petal length and width and their sepal length and width. The sepal is the area (typically green) beneath the petal of a flower. It offers protection while the flower is budding and then support for the petals after the flower blooms.
</div>

<p>Figure <a href="Regconditions.html#fig:regconditions-iris-histogram">18.1</a> is a histogram of the sepal width for the iris plants observed by Edgar Anderson; overlayed is the density plot for the same dataset, which we have described as a smoothed histogram. Both the histogram and the density plot are empirical models of the distribution of the sepal width.</p>
<div class="figure" style="text-align: center"><span id="fig:regconditions-iris-histogram"></span>
<img src="images/regconditions-iris-histogram-1.png" alt="Summary of the distribution of sepal widths for a sample of irises." width="80%" />
<p class="caption">
Figure 18.1: Summary of the distribution of sepal widths for a sample of irises.
</p>
</div>
<p>Probability models are analytical models for the distribution of a variable. Instead of constructing a density using data, probability theory posits a functional form for the density. For example, Figure <a href="Regconditions.html#fig:regconditions-iris-normal">18.2</a> overlays the following function on top of the the iris data:</p>
<p><span class="math display">\[f(x) = \frac{1}{\sqrt{0.380\pi}} e^{-\frac{1}{0.380}(x - 3.057)^2}\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:regconditions-iris-normal"></span>
<img src="images/regconditions-iris-normal-1.png" alt="Summary of the distribution of the sepal widths for a sample of irises with a probability model overlayed." width="80%" />
<p class="caption">
Figure 18.2: Summary of the distribution of the sepal widths for a sample of irises with a probability model overlayed.
</p>
</div>
<p>A density (whether constructed empirically or posited analytically) is just a model for the distribution of a variable. Further, all density functions share a few basic properties:</p>
<ol style="list-style-type: decimal">
<li>The density is non-negative for all values of the variable.</li>
<li>The area under the density function must equal 1.</li>
</ol>
<p>While the value on the y-axis is not directly meaningful, density functions provide a link between the value of the variable and the likelihood of it occuring. Specifically, the probability that a variable falls in a specific range corresponds to the area under the curve in that region. For example, based on the analytical model described above (the blue curve in the figure), the probability that an iris has a sepal width between 3.5 and 4 centimeters is 0.14, illustrated in Figure <a href="Regconditions.html#fig:regconditions-iris-prob">18.3</a>. That is, there is a 14% chance we find an irish with a sepal width between 3.5 and 4 centimeters.</p>
<div class="figure" style="text-align: center"><span id="fig:regconditions-iris-prob"></span>
<img src="images/regconditions-iris-prob-1.png" alt="Using the model for a density function to compute a probability." width="80%" />
<p class="caption">
Figure 18.3: Using the model for a density function to compute a probability.
</p>
</div>
<p>While the above model for the density is not perfect, it does capture many of the characteristics present in the data. Similar to empirical models, analytical models for distributions are just that — <em>models</em>. This particular model, characterized by the bell-shape density, is known as the <strong>Normal Distribution</strong>.</p>

<div class="definition">
<p><span id="def:defn-normal-distribution" class="definition"><strong>Definition 18.2  (Normal Distribution)  </strong></span>Also called the Gaussian Distribution, this probability model is popular for modeling noise within a data-generating process. It has the following characteristics:</p>
<ul>
<li>It is bell-shaped.</li>
<li>It is symmetric, meaning the mean is directly at its center, and the lower half of the distribution looks like a mirror image of the upper half of the distribution.</li>
<li>Often useful for modeling natural phenomena or sums of measurements.</li>
</ul>
<p>The functional form of the Normal distribution is <span class="math display">\[f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{1}{2\sigma^2}(x - \mu)^2}\]</span></p>
where <span class="math inline">\(\mu\)</span> is the mean of the distribution and <span class="math inline">\(\sigma^2\)</span> is the variance of the distribution.
</div>

<p>While there are several nice properties of the Normal Distribution, we are primarily interested in the fact that if the error in a data generating process follows a Normal Distribution (in addition to the other three conditions described above placed on the error term), then the form of the sampling distribution for the least squares estimates of the slope and intercept is known. That is, with all four conditions in place, we have an analytical model for the sampling distribution. This means we avoid simulating in order to build a model for the sampling distribution; so, computationally it is faster. If the errors really are from a Normal Distribution, then we also gain power in our study by imposing this condition. Finally, such a model does not rely on sufficient data to construct; it is valid for any sample size (of course, large samples will always decrease variability in the estimates, which is a plus).</p>
<p>Let’s think about what this condition means for the responses. Given the shape of the Normal distribution, imposing this condition (in addition to the other conditions) implies that some errors are positive and some are negative. This in turn implies that some responses will tend to fall above the line (we will underpredict for these observations), and some response will tend to fall below the line (we will overpredict for these observations).</p>
</div>
<div id="classical-regression-model" class="section level2">
<h2><span class="header-section-number">18.4</span> Classical Regression Model</h2>
<p>We have discussed four conditions we could place on the stochastic portion of the data generating process. Placing all four conditions on the error term is what we refer to as the “Classical Regression Model.”</p>

<div class="definition">
<p><span id="def:defn-classical-regression" class="definition"><strong>Definition 18.3  (Classical Regression Model)  </strong></span>For a quantitative response and single predictor, the classical regression model assumes the following data-generating process:</p>
<p><span class="math display">\[(\text{Response})_i = \beta_0 + \beta_1 (\text{Predictor})_{i} + \epsilon_i\]</span></p>
<p>where</p>
<ol style="list-style-type: decimal">
<li>The error in the response has a mean of 0 for all values of the predictor.</li>
<li>The error in the response for one subject is independent of the error in the response for all other subjects.</li>
<li>The errors are identically distributed for all values of the predictor. This is often stated as the variability in the error of the response is the same for all values of the predictor.</li>
<li>The errors follow a Normal Distribution.</li>
</ol>
This is the default “regression” analysis implemented in the majority of statistical packages.
</div>

<p>We note that regression need not require these conditions. Placing all four conditions on the error term results in a specific analytical model for the sampling distribution of the least squares estimates. Changing the conditions changes the way we model the sampling distribution.</p>

<div class="rmdkeyidea">
The model for the sampling distribution of a statistic is determined by the conditions you place on the data generating process.
</div>

<p>We have stressed the implications of each condition individually. Figure <a href="Regconditions.html#fig:regconditions-assumptions">18.4</a> illustrates these conditions working together. The condition that the errors have mean 0 implies that for a given value of the predictor, the average response is given by the line (shown as the green dot in the figure). The condition of Normality implies that for a given value of the predictor, the response is distributed evenly about the regression line, with some above and some below. Further, the shape of the Normal distribution implies that these responses will cluster about the line. The identically distributed condition (specifically homoskedasticity) implies that while the responses vary around the line, they do so the same degree, regardless of the value of the predictor. Therefore, the model is just as precise for all values of the predictor. Finally, any two responses must be unrelated.</p>
<div class="figure" style="text-align: center"><span id="fig:regconditions-assumptions"></span>
<img src="images/RegConditions-Assumptions.jpg" alt="Illustration of the conditions on the error term for the classical regression model." width="80%" />
<p class="caption">
Figure 18.4: Illustration of the conditions on the error term for the classical regression model.
</p>
</div>
</div>
<div id="imposing-the-conditions" class="section level2">
<h2><span class="header-section-number">18.5</span> Imposing the Conditions</h2>
<p>Let’s return to our model for the bracketed duration as a function of the magnitude of the corresponding earthquake:</p>
<p><span class="math display">\[(\text{Bracketed Duration})_i = \beta_0 + \beta_1(\text{Magnitude})_i + \epsilon_i\]</span></p>
<p>Our hypotheses of interest which captures the question of interest was</p>
<blockquote>
<p><span class="math inline">\(H_0: \beta_1 = 0\)</span><br />
<span class="math inline">\(H_0: \beta_1 \neq 0\)</span></p>
</blockquote>
<p>which corresponds to testing whether there is evidence of a linear relationship between the two variables. Using the method of least squares, we estimated the parameters in the model; this leads to the following equation for estimating the average bracketed duration given the magnitude:</p>
<p><span class="math display">\[(\text{Brackted Duration}) = -19.19 + 4.48(\text{Magnitude})\]</span></p>
<p>If we are willing to assume the data is consistent with the conditions for the classical regression model, we are able to model the sampling distribution of these estimates and therefore construct confidence intervals. Table <a href="Regconditions.html#tab:regconditions-slr-summary">18.1</a> summarizes the fit for the above model. In addition to the least squares estimates, it also contains the <strong>standard error</strong> of the statistic, quantifying the variability in the estimates.</p>

<div class="definition">
<span id="def:defn-standard-error" class="definition"><strong>Definition 18.4  (Standard Error)  </strong></span>The estimated standard deviation of a statistic, computed from the model for the statistic’s sampling distribution. It quantifies the variability in the sampling distribution of the statistic.
</div>

<p>Finally, there is a 95% confidence interval estimating each parameter. Notice that based on the confidence interval for the slope, 0 is not a reasonable value for this parameter. Therefore, we have evidence that the slope coefficient associated with the magnitude differs from 0; that is, we have evidence of a linear relationship between the bracketed duration and the magnitude of the earthquake.</p>
<table>
<caption><span id="tab:regconditions-slr-summary">Table 18.1: </span>Summary of the linear model fit relating the bracketed duration at locations in Greece following an earthquake with the magnitude of the event.</caption>
<thead>
<tr class="header">
<th align="left">Term</th>
<th align="right">Estimate</th>
<th align="right">Standard Error</th>
<th align="right">Lower 95% CI</th>
<th align="right">Upper 95% CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">-19.194</td>
<td align="right">3.975</td>
<td align="right">-27.066</td>
<td align="right">-11.323</td>
</tr>
<tr class="even">
<td align="left">Magnitude</td>
<td align="right">4.484</td>
<td align="right">0.724</td>
<td align="right">3.050</td>
<td align="right">5.917</td>
</tr>
</tbody>
</table>
<p>We have described in general how confidence intervals are constructed. Under the classical regression model, there is an analytical model for the sampling distribution, and it is known. As a result, the confidence interval can be computed from a formula.</p>

<div class="rmdtip">
<p>If the classical regression model is assumed, the 95% confidence interval for the parameter <span class="math inline">\(\beta_j\)</span> can be approximated by</p>
<p><span class="math display">\[\widehat{\beta}_j \pm (1.96)\left(\text{standard error of } \widehat{\beta}_j\right)\]</span></p>
</div>

<p>The confidence interval for the change in the average bracketed duration for each 1-unit increase in the magnitude of an earthquake (the slope <span class="math inline">\(\beta_1\)</span>) was constructed assuming the classical regression model. Suppose, however, that we are only willing to impose the following conditions:</p>
<ul>
<li>The error in the bracketed duration is 0 on average for earthquakes of any magnitude.</li>
<li>The error in the bracketed duration for one earthquake is independent of the error in the bracketed duration for any other earthquake.</li>
</ul>
<p>Since the conditions have been altered, the model for the sampling distribution of the estimates will change and therefore the corresponding confidence intervals. Under these conditions, we can appeal to a bootstrapping algorithm. Specifically, we could resample (with replacement) 119 earthquakes from the original data; for each resample, we compute the least squares fit (see Figure <a href="Regconditions.html#fig:regconditions-bootstrap">18.5</a>). Since the observations selected change with each resample, the least squares estimates will also change. By repeating this process over and over again, we can obtain a model for how the estimates would change in repeated sampling.</p>
<div class="figure" style="text-align: center"><span id="fig:regconditions-bootstrap"></span>
<img src="images/RegConditions-Bootstrap.jpg" alt="Illustration of a single iteration of a bootstrap procedure to construct an empirical estimate of the sampling distribution for the estimates of the coefficients in a regression model." width="80%" />
<p class="caption">
Figure 18.5: Illustration of a single iteration of a bootstrap procedure to construct an empirical estimate of the sampling distribution for the estimates of the coefficients in a regression model.
</p>
</div>
<p>Using the empirical model of the sampling distribution for each estimate, we can construct confidence intervals. These updated confidence intervals are shown in Table <a href="Regconditions.html#tab:regconditions-slr-summary-alt">18.2</a></p>
<table>
<caption><span id="tab:regconditions-slr-summary-alt">Table 18.2: </span>Summary of the linear model fit relating the bracketed duration at locations in Greece following an earthquake with the magnitude of the event. Only assumes errors are independent and have mean 0.</caption>
<thead>
<tr class="header">
<th align="left">Term</th>
<th align="right">Estimate</th>
<th align="right">Standard Error</th>
<th align="right">Lower 95% CI</th>
<th align="right">Upper 95% CI</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(Intercept)</td>
<td align="right">-19.194</td>
<td align="right">4.960</td>
<td align="right">-29.258</td>
<td align="right">-9.545</td>
</tr>
<tr class="even">
<td align="left">Magnitude</td>
<td align="right">4.484</td>
<td align="right">0.965</td>
<td align="right">2.593</td>
<td align="right">6.437</td>
</tr>
</tbody>
</table>
<p>While the exact interval differs from what we computed previously, our overall conclusion remains the same (there is evidence of a relationship). It is reasonable to ask, which confidence interval should we use? That depends on the conditions you are willing to assume, which is an issue we will tackle soon.</p>
</div>
<div id="recap" class="section level2">
<h2><span class="header-section-number">18.6</span> Recap</h2>
<p>We have covered a lot of ground in this chapter, and it is worth taking a moment to summarize the big ideas. In order to construct a model for the sampling distribution for the estimates of the parameters in the regression model, we took a step back and modeled the data generating process. Such a model consists of two components: a deterministic component explaining the differences in the response as a function of the predictor and a stochastic component capturing the noise in the system.</p>
<p>Certain conditions are placed on the distribution of the noise in our model. With a full set of conditions (classical regression model), we are able to model the sampling distribution analytically. We can also construct an empirical model for the sampling distribution assuming the data is consistent with fewer conditions.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="Regmodel.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="Regquality.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"download": ["MA223CourseNotes.pdf"],
"toc": {
"collapse": "section"
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
